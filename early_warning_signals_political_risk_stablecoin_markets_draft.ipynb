{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8krLMSH3M7a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# On-Chain Behavioral Pre-Emption System (OBPS): Early-Warning Signals of Political Risk in Stablecoin Markets\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2512.00893-b31b1b.svg)](https://arxiv.org/abs/2512.00893)\n",
        "[![Journal](https://img.shields.io/badge/Journal-Quantitative%20Finance%20%28q--fin.ST%29-003366)](https://arxiv.org/abs/2512.00893)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/early_warning_signals_political_risk_stablecoin_markets)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Econophysics%20%7C%20Blockchain%20Analytics-00529B)](https://github.com/chirindaopensource/early_warning_signals_political_risk_stablecoin_markets)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Ethereum%20Blockchain%20%28ERC--20%29-lightgrey)](https://etherscan.io/)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Google%20BigQuery%20Crypto-lightgrey)](https://cloud.google.com/blog/products/data-analytics/introducing-six-new-cryptocurrencies-in-bigquery-public-datasets-and-how-to-analyze-them)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-CoinGecko%20%2F%20CEX%20Data-lightgrey)](https://www.coingecko.com/)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Bai--Perron%20Structural%20Breaks%20%7C%20Hilbert--Huang%20Transform-orange)](https://github.com/chirindaopensource/early_warning_signals_political_risk_stablecoin_markets)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Structural%20VAR%20%7C%20AAFT%20Surrogates-red)](https://github.com/chirindaopensource/early_warning_signals_political_risk_stablecoin_markets)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/statsmodels-blue.svg)](https://www.statsmodels.org/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-gray?logo=yaml&logoColor=white)](https://pyyaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/early_warning_signals_political_risk_stablecoin_markets`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election\"** by:\n",
        "\n",
        "*   Kundan Mukhia\n",
        "*   Buddha Nath Sharma\n",
        "*   Salam Rabindrajit Luwang\n",
        "*   Md. Nurujjaman\n",
        "*   Chittaranjan Hens\n",
        "*   Suman Saha\n",
        "*   Tanujit Chakraborty\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous on-chain data validation and behavioral topology classification to advanced structural break detection, non-linear signal processing via Hilbert-Huang Transform, and regime-dependent Structural Vector Autoregression (SVAR) analysis.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_obps_pipeline`](#key-callable-run_obps_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Mukhia et al. (2025). The core of this repository is the iPython Notebook `early_warning_signals_political_risk_stablecoin_markets_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed to be a generalizable toolkit for detecting and quantifying the transmission of political uncertainty into cryptocurrency markets.\n",
        "\n",
        "The paper addresses the challenge of identifying early-warning signals of market stress by distinguishing between human-driven peer-to-peer stablecoin transactions and automated algorithmic activity. This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Process raw ERC-20 transaction logs to isolate **Human (EOA-EOA)** and **Algorithmic (SC-SC)** flows.\n",
        "-   Implement **Bai-Perron Structural Break Detection** using dynamic programming to identify endogenous regime shifts in transaction volumes.\n",
        "-   Apply **Empirical Mode Decomposition (EMD)** and **Hilbert Spectral Analysis** to detect non-linear extreme volatility events in BTC and ETH prices.\n",
        "-   Execute **Structural Vector Autoregression (SVAR)** with Cholesky identification to quantify volatility spillovers between stablecoins (USDT/USDC).\n",
        "-   Validate findings using **Amplitude-Adjusted Fourier Transform (AAFT)** surrogates and **Wald statistics** for regime comparison.\n",
        "-   Automatically generate a comprehensive synthesis report confirming the \"early warning\" property of human on-chain flows.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in econophysics, time-series econometrics, and blockchain analytics.\n",
        "\n",
        "**1. Behavioral Topology Classification:**\n",
        "The core differentiation strategy relies on the Ethereum Account Model.\n",
        "-   **Human Signal (EOA-EOA):** Transactions where both sender and receiver are Externally Owned Accounts (controlled by private keys). This captures direct human sentiment and precautionary hedging.\n",
        "-   **Algorithmic Signal (SC-SC):** Transactions involving Smart Contracts, reflecting automated arbitrage, DeFi protocols, and bot activity.\n",
        "\n",
        "**2. Structural Break Detection:**\n",
        "The pipeline uses the Bai-Perron methodology to estimate unknown break dates $T_1, \\dots, T_m$ in a mean-shift model $y_t = \\mu_j + u_t$. The optimal partition is found by minimizing the global Sum of Squared Residuals (SSR) via dynamic programming:\n",
        "$$ \\{\\hat{T}_1, \\dots, \\hat{T}_m\\} = \\arg \\min_{(T_1, \\dots, T_m)} SSR(T_1, \\dots, T_m) $$\n",
        "\n",
        "**3. Non-Linear Signal Processing:**\n",
        "To analyze non-stationary price dynamics, the Hilbert-Huang Transform is used:\n",
        "-   **EMD:** Decomposes the signal into Intrinsic Mode Functions (IMFs).\n",
        "-   **Hilbert Spectrum:** Computes instantaneous energy $IE(t) = \\int H^2(t, \\omega) d\\omega$.\n",
        "-   **Extreme Events:** Identified when $IE(t) > E_\\mu + 4\\sigma$.\n",
        "\n",
        "**4. Structural VAR:**\n",
        "Volatility spillovers are modeled using a regime-dependent SVAR:\n",
        "$$ A_0 u_t = \\varepsilon_t \\implies \\Sigma_u = A_0^{-1} (A_0^{-1})' $$\n",
        "Identification is achieved via Cholesky decomposition, and regime shifts are tested using the Wald statistic.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`early_warning_signals_political_risk_stablecoin_markets_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 32 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks the schema, content integrity, and temporal consistency of blockchain and market data.\n",
        "-   **Advanced Econometrics:** Integrates `statsmodels` for ADF tests and VAR estimation, and custom `numpy`/`scipy` implementations for Bai-Perron DP and EMD.\n",
        "-   **Robustness Verification:** Includes automated AAFT surrogate generation (1000 iterations) to validate the statistical significance of all detected breaks and events.\n",
        "-   **Reproducible Artifacts:** Generates structured dataclasses for every intermediate result, ensuring full auditability.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Preprocessing (Tasks 1-9):** Ingests raw chain logs, validates schemas, filters by time/status, normalizes values to USD, deduplicates, and classifies topology.\n",
        "2.  **Market Data Processing (Tasks 10-11):** Cleanses exchange data and extracts aligned price/volume series.\n",
        "3.  **Panel Construction (Tasks 12-15):** Merges datasets, applies log-transformations, tests stationarity (ADF), and differences I(1) series.\n",
        "4.  **Structural Break Analysis (Tasks 16-21):** Executes Bai-Perron detection on EOA, SC, and Exchange series, validated by AAFT surrogates.\n",
        "5.  **HHT Analysis (Tasks 22-26):** Performs EMD and Hilbert Spectral Analysis on BTC/ETH prices to detect extreme volatility events.\n",
        "6.  **SVAR Analysis (Tasks 27-31):** Estimates regime-dependent VAR models, identifies structural shocks via Cholesky, and performs Wald tests for parameter stability.\n",
        "7.  **Synthesis (Task 32):** Aggregates all findings and validates them against the expected empirical results (e.g., Nov 3 human signal vs. Nov 5 election).\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `early_warning_signals_political_risk_stablecoin_markets_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 32 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_obps_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_obps_pipeline`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between all 32 sub-tasks.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `pyyaml`, `scipy`, `statsmodels`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/early_warning_signals_political_risk_stablecoin_markets.git\n",
        "    cd early_warning_signals_political_risk_stablecoin_markets\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy pyyaml scipy statsmodels\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two primary DataFrames:\n",
        "1.  **`df_chain_raw`**: ERC-20 transfer logs with columns: `timeStamp`, `tokenAddress`, `from`, `to`, `value`, `fromIsContract`, `toIsContract`, `transactionHash`, `blockNumber`, `logIndex`, `txStatus`.\n",
        "2.  **`df_market_raw`**: Exchange OHLCV data with columns: `Date`, `Symbol`, `Close`, `Volume`.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `early_warning_signals_political_risk_stablecoin_markets_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `run_obps_pipeline` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        study_config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Load raw datasets (Example using synthetic generator provided in the notebook)\n",
        "    # In production, load from CSV/Parquet: pd.read_csv(...)\n",
        "    df_chain_raw = ...\n",
        "    df_market_raw = ...\n",
        "    \n",
        "    # 3. Execute the entire replication study.\n",
        "    results = run_obps_pipeline(\n",
        "        df_chain_raw=df_chain_raw,\n",
        "        df_market_raw=df_market_raw,\n",
        "        study_config=study_config\n",
        "    )\n",
        "    \n",
        "    # 4. Access results\n",
        "    print(f\"Conclusion: {results.final_report.overall_conclusion}\")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns an `OBPSPipelineResult` object containing all analytical artifacts:\n",
        "-   **`config`**: Validated configuration object.\n",
        "-   **`final_panel`**: The processed econometric panel DataFrame.\n",
        "-   **`structural_breaks`**: Dictionary of detected break dates and SupF statistics.\n",
        "-   **`break_robustness`**: AAFT p-values for structural breaks.\n",
        "-   **`hht_analysis`**: Hilbert Spectra and detected extreme event dates.\n",
        "-   **`svar_model`**: Full-sample VAR estimation results.\n",
        "-   **`svar_regimes`**: Pre- and Post-election VAR models.\n",
        "-   **`svar_identification`**: Structural impact matrices (Cholesky factors).\n",
        "-   **`wald_test`**: Wald statistic for regime shift significance.\n",
        "-   **`final_report`**: A synthesis object comparing observed results to expected findings.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "early_warning_signals_political_risk_stablecoin_markets/\n",
        "│\n",
        "├── early_warning_signals_political_risk_stablecoin_markets_draft.ipynb  # Main implementation notebook\n",
        "├── config.yaml                                                          # Master configuration file\n",
        "├── requirements.txt                                                     # Python package dependencies\n",
        "│\n",
        "├── LICENSE                                                              # MIT Project License File\n",
        "└── README.md                                                            # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as:\n",
        "-   **Observation Window:** `start_date`, `end_date`.\n",
        "-   **Bai-Perron Settings:** `max_breaks`, `trimming_epsilon`.\n",
        "-   **HHT Settings:** `max_imfs`, `B` (threshold multiplier).\n",
        "-   **SVAR Settings:** `max_lags`, `break_date`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Real-Time Monitoring:** Adapting the pipeline for streaming blockchain data.\n",
        "-   **Multi-Chain Support:** Extending the analysis to other EVM-compatible chains (Polygon, Arbitrum).\n",
        "-   **Machine Learning Integration:** Incorporating LSTM or Transformer models for predictive signaling.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{mukhia2025earlywarning,\n",
        "  title={Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election},\n",
        "  author={Mukhia, Kundan and Sharma, Buddha Nath and Luwang, Salam Rabindrajit and Nurujjaman, Md. and Hens, Chittaranjan and Saha, Suman and Chakraborty, Tanujit},\n",
        "  journal={arXiv preprint arXiv:2512.00893},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). On-Chain Behavioral Pre-Emption System (OBPS): An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/early_warning_signals_political_risk_stablecoin_markets\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Kundan Mukhia et al.** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, and Statsmodels**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `early_warning_signals_political_risk_stablecoin_markets_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "euQHbmJaFyZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election*\"\n",
        "\n",
        "Authors: Kundan Mukhia, Buddha Nath Sharma, Salam Rabindrajit Luwang, Md. Nurujjaman, Chittaranjan Hens, Suman Saha, Tanujit Chakraborty\n",
        "\n",
        "E-Journal Submission Date: 30 November 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2512.00893\n",
        "\n",
        "Abstract:\n",
        "\n",
        "We study how the 2024 U.S. presidential election, viewed as a major political risk event, affected cryptocurrency markets by distinguishing human-driven peer-to-peer stablecoin transactions from automated algorithmic activity. Using structural break analysis, we find that human-driven Ethereum Request for Comment 20 (ERC-20) transactions shifted on November 3, two days before the election, while exchange trading volumes reacted only on Election Day. Automated smart-contract activity adjusted much later, with structural breaks appearing in January 2025. We validate these shifts using surrogate-based robustness tests. Complementary energy-spectrum analysis of Bitcoin and Ethereum identifies pronounced post-election turbulence, and a structural vector autoregression confirms a regime shift in stablecoin dynamics. Overall, human-driven stablecoin flows act as early-warning indicators of political stress, preceding both exchange behavior and algorithmic responses."
      ],
      "metadata": {
        "id": "BKXgoI2d3VPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "\n",
        "### **Executive Abstract**\n",
        "This paper presents a novel econometric analysis of the 2024 U.S. Presidential Election's impact on cryptocurrency markets. By decomposing Ethereum (ERC-20) blockchain data, the authors isolate **human-driven** (Externally Owned Accounts or EOA) transactions from **algorithmic** (Smart Contract or SC) activity. The central finding is a distinct temporal divergence: human actors exhibit anticipatory hedging behavior *prior* to the election, serving as a leading indicator, while algorithmic agents demonstrate a significant lag, recalibrating only after market volatility stabilizes.\n",
        "\n",
        "--\n",
        "\n",
        "### **Problem Definition and Data Architecture**\n",
        "The authors address a limitation in standard financial literature: the reliance on aggregated exchange-based price and volume data, which obscures the heterogeneity of market participants.\n",
        "\n",
        "*   **The Event:** The 2024 U.S. Presidential Election is treated as a major exogenous political shock.\n",
        "*   **The Assets:** The study focuses on the two dominant stablecoins, **Tether (USDT)** and **USD Coin (USDC)**, alongside Bitcoin (BTC) and Ethereum (ETH) as market proxies.\n",
        "*   **Data Granularity:** The dataset covers March 2024 to February 2025. The authors utilize raw ERC-20 token transfer data to classify transactions into two distinct vectors:\n",
        "    1.  **EOA-to-EOA:** Peer-to-peer transfers initiated by private keys (Human-driven).\n",
        "    2.  **SC-to-SC:** Interactions between automated code blocks (Algorithmic/Bot-driven).\n",
        "\n",
        "### **Methodological Framework**\n",
        "The paper employs a sophisticated suite of statistical and signal processing techniques to detect structural breaks and non-linear dynamics.\n",
        "\n",
        "1.  **Stationarity Testing:** The Augmented Dickey-Fuller (ADF) test is applied. Log-transformed series were found to be non-stationary ($I(1)$), necessitating first-differencing for the Vector Autoregression models.\n",
        "2.  **Structural Break Detection:** The **Bai-Perron (BP)** test is utilized to endogenously identify breakpoints in the time series without prior specification. The **SupF** statistic validates the significance of these regime shifts.\n",
        "3.  **Robustness via Surrogate Data:** To ensure breaks were not artifacts of noise, the authors employed **Amplitude-Adjusted Fourier Transform (AAFT)** surrogates. This generates synthetic data preserving the original linear autocorrelation and amplitude distribution but randomizing the phase, providing a rigorous null hypothesis.\n",
        "4.  **Signal Processing (HHT):** The **Hilbert-Huang Transform**, utilizing Empirical Mode Decomposition (EMD), was used on non-stationary BTC/ETH price data to derive instantaneous energy spectra and detect \"Extreme Events\" (defined as energy exceeding mean + 4 standard deviations).\n",
        "5.  **Structural Vector Autoregression (SVAR):** An SVAR model with Cholesky decomposition was estimated to quantify volatility spillovers and causal flows between stablecoins across pre- and post-election regimes.\n",
        "\n",
        "### **Empirical Evidence – The Temporal Divergence**\n",
        "The core contribution of the paper is the identification of a lead-lag relationship between human and machine actors.\n",
        "\n",
        "*   **The Human Signal (Leading Indicator):**\n",
        "    *   The Bai-Perron test detected a statistically significant structural break in **EOA-to-EOA** transactions on **November 3, 2024**—two days *before* the election.\n",
        "    *   **Interpretation:** This represents precautionary capital reallocation and hedging by human agents anticipating political uncertainty. It serves as an **Early Warning System (EWS)**.\n",
        "\n",
        "*   **The Market Reaction (Coincident Indicator):**\n",
        "    *   Centralized Exchange (CEX) trading volumes for USDT and USDC exhibited structural breaks on **November 5, 2024** (Election Day).\n",
        "    *   **Interpretation:** The broader market reacted synchronously with the event, lagging the on-chain human signal.\n",
        "\n",
        "*   **The Algorithmic Lag (Lagging Indicator):**\n",
        "    *   Automated **SC-to-SC** activity did not break until **January 2025** (Jan 2 for USDC, Jan 16 for USDT).\n",
        "    *   **Interpretation:** Algorithmic trading systems and DeFi protocols are reactive. They require a period of observation to recalibrate parameters to the new post-election equilibrium before altering their execution logic.\n",
        "\n",
        "### **Volatility Dynamics and Regime Shifts**\n",
        "The paper quantifies the magnitude of the market stress using HHT and SVAR analysis.\n",
        "\n",
        "*   **Extreme Energy Events:** The Hilbert Spectrum analysis revealed concentrated regions of high instantaneous energy in BTC and ETH prices immediately following the election (Nov 6–10), confirming the transmission of stress from stablecoin flows to volatile assets.\n",
        "*   **SVAR Results:**\n",
        "    *   **Wald Tests:** Confirmed a massive structural shift in the dynamic relationship between USDT and USDC ($p < 0.0001$).\n",
        "    *   **Impulse Response & Spillovers:** Post-election, volatility spillovers increased by approximately **2848%**.\n",
        "    *   **Transmission Channel:** USDT was identified as the dominant channel for shock transmission, with its own-shock effects and cross-market spillovers rising significantly more than USDC's.\n",
        "\n",
        "### **Conclusion and Implications**\n",
        "The study establishes that on-chain data possesses predictive power superior to traditional exchange metrics during political risk events.\n",
        "\n",
        "1.  **Alpha Generation/Risk Management:** Investors can utilize EOA-specific stablecoin velocity as a leading indicator for volatility, providing a roughly 48-hour advantage over exchange-based signals.\n",
        "2.  **Algorithmic Rigidity:** The significant delay in smart contract adjustments suggests that current algorithmic trading strategies lack the semantic understanding to process exogenous political shocks in real-time, creating a window of inefficiency.\n",
        "3.  **Systemic Stability:** The massive increase in volatility spillovers post-election highlights the fragility of the crypto-ecosystem to political narratives, with stablecoins acting as the primary conduit for this systemic stress."
      ],
      "metadata": {
        "id": "vd--vNi1Ac4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules\n"
      ],
      "metadata": {
        "id": "sPs-sHdeqXH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  On-Chain Behavioral Pre-Emption System (OBPS) for Political Risk Analysis\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Early-Warning Signals of Political Risk in\n",
        "#  Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election\"\n",
        "#  by Mukhia et al. (2025). It delivers a rigorous system for detecting and\n",
        "#  quantifying the transmission of political uncertainty into cryptocurrency markets\n",
        "#  by isolating human-driven on-chain signals from algorithmic activity.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Behavioral Topology Classification: Segregation of EOA-EOA (Human) vs. SC-SC (Algo) flows\n",
        "#  • Endogenous Structural Break Detection: Bai-Perron mean-shift models with dynamic programming\n",
        "#  • Non-Linear Signal Processing: Hilbert-Huang Transform (HHT) for instantaneous energy analysis\n",
        "#  • Robustness Verification: Amplitude-Adjusted Fourier Transform (AAFT) surrogate testing\n",
        "#  • Structural Vector Autoregression (SVAR): Regime-dependent volatility spillover analysis\n",
        "#  • Hypothesis Testing: Wald statistics for structural parameter stability across regimes\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Vectorized processing of high-frequency blockchain transaction logs\n",
        "#  • Rigorous time-series stationarity enforcement via ADF testing\n",
        "#  • Efficient dynamic programming algorithms for global SSR minimization\n",
        "#  • FFT-based phase randomization for non-linear surrogate generation\n",
        "#  • Cholesky identification for structural shock transmission\n",
        "#  • Comprehensive validation framework for replication fidelity\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Mukhia, K., Sharma, B. N., Luwang, S. R., Nurujjaman, M., Hens, C., Saha, S., & Chakraborty, T. (2025).\n",
        "#  Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior\n",
        "#  Around the 2024 U.S. Election. arXiv preprint arXiv:2512.00893.\n",
        "#  https://arxiv.org/abs/2512.00893\n",
        "#\n",
        "#  License: MIT\n",
        "#  Author:  CS Chirinda\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "import re\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime, timezone\n",
        "from typing import List, Dict, Any, Set, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy.signal import argrelextrema, hilbert\n",
        "from scipy.stats import chi2\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.vector_ar.var_model import VARResultsWrapper\n"
      ],
      "metadata": {
        "id": "HE7nnLGVqb79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "m4Sc6_KIqdY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Discussion of the Inputs, Processes and Outputs of Key Callables**\n",
        "\n",
        "### 1. `validate_study_config` (Task 1 Orchestrator)\n",
        "\n",
        "*   **Inputs:** A raw Python dictionary (`config`) containing metadata, schema definitions, and analysis parameters.\n",
        "*   **Processes:** This callable orchestrates a tripartite validation sequence. First, it invokes `validate_meta_config` to enforce temporal logic (e.g., $T_{start} < T_{end}$) and UTC standardization. Second, it calls `validate_schemas` to verify the presence of required columns (e.g., `timeStamp`, `value`) and asset definitions (USDT/USDC addresses). Third, it executes `validate_analysis_config` to enforce mathematical constraints on model parameters (e.g., Bai-Perron trimming parameter $\\epsilon \\in (0, 0.5)$).\n",
        "*   **Outputs:** A `ValidatedStudyConfig` immutable dataclass containing typed and verified configuration objects.\n",
        "*   **Research Context:** This implements the **Data Governance** layer of the pipeline. It ensures that all subsequent econometric models (Bai-Perron, HHT, SVAR) receive parameters that are mathematically valid and consistent with the study's observation window (March 2024 – February 2025).\n",
        "\n",
        "### 2. `validate_df_chain_raw` (Task 2 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The raw on-chain DataFrame (`df_raw`) and the validated schema configuration (`schemas`).\n",
        "*   **Processes:** It executes a sequence of integrity checks: `validate_chain_columns` enforces data types (casting `int64`, `int8`), `validate_chain_domains` applies regex filters (e.g., `^0x[0-9a-fA-F]{40}$` for addresses) and value range checks ($t > 0$), and `validate_chain_uniqueness` computes the cardinality of the composite primary key $(TxHash, Block, LogIndex)$.\n",
        "*   **Outputs:** A `ChainValidationResult` dataclass containing the validated DataFrame and summary statistics (row counts, duplicates).\n",
        "*   **Research Context:** This implements the **Data Integrity Verification** step. It ensures the raw ledger data conforms to the ERC-20 standard interface requirements described in Section 2, preventing garbage-in-garbage-out errors in the behavioral topology classification.\n",
        "\n",
        "### 3. `validate_df_market_raw` (Task 3 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The raw market DataFrame (`df_raw`), schema config (`schemas`), and metadata (`meta_config`).\n",
        "*   **Processes:** It validates the market microstructure log by calling `validate_market_columns` (type enforcement), `validate_symbol_coverage` (verifying presence of BTC, ETH, USDT, USDC), and `validate_temporal_coverage` (identifying missing dates via set difference against the expected daily range).\n",
        "*   **Outputs:** A `MarketValidationResult` dataclass containing the validated DataFrame and coverage metrics.\n",
        "*   **Research Context:** This implements the **Market Data Quality Control** step. It ensures that the control variables (BTC/ETH prices) and exchange volume series cover the critical election period, a prerequisite for the comparative analysis in Section 4.3.\n",
        "\n",
        "### 4. `filter_chain_data` (Task 4 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The validated on-chain DataFrame (`df_validated`), metadata (`meta_config`), and preprocessing config.\n",
        "*   **Processes:** It transforms the data by converting Unix epoch timestamps to UTC dates via `convert_timestamp_to_utc_date`. It then applies `apply_temporal_filter` to restrict records to the interval $[T_{start}, T_{end}]$ and `filter_transaction_status` to remove failed transactions (where `txStatus \\neq 0`).\n",
        "*   **Outputs:** A `FilteredChainData` dataclass containing the temporally and status-filtered DataFrame.\n",
        "*   **Research Context:** This implements the **Temporal and Status Filtering** described in Section 2. It isolates the relevant \"successful\" economic activity within the study window, ensuring that failed transactions do not distort the volume aggregation.\n",
        "\n",
        "### 5. `normalize_chain_data` (Task 5 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The filtered chain data (`filtered_data`), schemas, and preprocessing config.\n",
        "*   **Processes:** It filters the dataset to retain only USDT and USDC transfers via `filter_by_token_address`. Crucially, it applies `normalize_token_values` to transform raw `uint256` values into USD equivalents using Equation (1) from the paper: $\\text{USD Value} = \\frac{\\texttt{value}}{10^6}$. Finally, it removes economically insignificant zero-value transfers via `remove_zero_value_transfers`.\n",
        "*   **Outputs:** A `NormalizedChainData` dataclass with the normalized `usd_value` column.\n",
        "*   **Research Context:** This implements the **Unit Normalization** step (Equation 1). It transforms raw blockchain integers into economically meaningful USD figures, enabling the aggregation of volume series for the structural break analysis.\n",
        "\n",
        "### 6. `deduplicate_chain_data` (Task 6 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The normalized chain data (`normalized_data`).\n",
        "*   **Processes:** It identifies duplicate log entries based on the tuple $(TxHash, BlockNumber, LogIndex)$ via `check_uniqueness_stats`. It then executes `remove_duplicates`, sorting by block height to deterministically retain the canonical record, and analyzes internal transaction flags via `analyze_internal_transactions`.\n",
        "*   **Outputs:** A `DeduplicatedChainData` dataclass containing the unique set of transfer logs.\n",
        "*   **Research Context:** This implements the **Data Cleansing** requirement mentioned in Section 2 (\"Duplicate... transactions were removed to reduce noise\"). It ensures that the volume series reflects unique economic transfers, preventing double-counting.\n",
        "\n",
        "### 7. `classify_chain_topology` (Task 7 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The deduplicated chain data (`deduplicated_data`).\n",
        "*   **Processes:** It applies `apply_topology_classification` to categorize transactions based on the `fromIsContract` and `toIsContract` flags. It implements the logic:\n",
        "    *   **Human (EOA-EOA):** `from=0` AND `to=0`\n",
        "    *   **Algo (SC-SC):** `from=1` AND `to=1`\n",
        "    It then validates the classification integrity via `validate_topology_counts`.\n",
        "*   **Outputs:** A `ClassifiedChainData` dataclass with a categorical `topology` column.\n",
        "*   **Research Context:** This implements the **Behavioral Topology Classification** described in Section 2 and Table 2. This is the core differentiation strategy of the paper, isolating human sentiment (EOA) from algorithmic response (SC).\n",
        "\n",
        "### 8. `aggregate_chain_volumes` (Task 8 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The classified chain data (`classified_data`) and metadata (`meta_config`).\n",
        "*   **Processes:** It executes `group_and_sum_volumes` to compute daily sums: $V_{k,t,d} = \\sum_{i \\in \\text{group}} \\text{usd\\_value}_i$. It then reshapes the data via `pivot_to_wide_format` to create distinct columns for each topology-token pair (e.g., `V_EOA_EOA_USDT`) and ensures a complete time series via `reindex_to_full_window`, filling missing days with zero.\n",
        "*   **Outputs:** An `AggregatedChainData` dataclass containing the daily volume time series.\n",
        "*   **Research Context:** This implements the **Time Series Aggregation** step. It transforms discrete transaction logs into continuous daily volume signals, which serve as the primary input for the Bai-Perron and SVAR models.\n",
        "\n",
        "### 9. `validate_daily_chain_series` (Task 9 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The aggregated chain data (`aggregated_data`) and metadata.\n",
        "*   **Processes:** It performs statistical validation via `check_series_integrity` (non-negativity), computes descriptive statistics (moments) via `compute_series_statistics`, and identifies outliers using the $4\\sigma$ rule via `detect_outliers`. It also generates visualization specifications.\n",
        "*   **Outputs:** A `ChainSeriesValidationResult` dataclass containing statistical summaries and validation status.\n",
        "*   **Research Context:** This implements **Descriptive Statistical Analysis**. It verifies the distributional properties of the constructed time series before they are subjected to rigorous econometric testing.\n",
        "\n",
        "### 10. `cleanse_market_data` (Task 10 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The validated market results (`validation_result`) and metadata.\n",
        "*   **Processes:** It normalizes market dates via `normalize_market_dates`, filters to the study window via `filter_market_window`, and removes invalid numeric entries (e.g., negative prices) via `clean_numeric_columns`.\n",
        "*   **Outputs:** A `CleanedMarketData` dataclass containing the sanitized market DataFrame.\n",
        "*   **Research Context:** This implements **Market Data Preprocessing**. It ensures that the exchange-based control variables are temporally aligned and numerically valid for the subsequent correlation and causality analysis.\n",
        "\n",
        "### 11. `extract_market_series` (Task 11 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The cleaned market data (`cleaned_data`).\n",
        "*   **Processes:** It separates the dataset into individual time series for BTC price, ETH price, USDT volume, and USDC volume using `extract_price_series` and `extract_volume_series`. It then combines them into a single aligned DataFrame via `combine_market_series` using an inner join on the date index.\n",
        "*   **Outputs:** A `MarketSeriesData` dataclass containing the aligned market time series.\n",
        "*   **Research Context:** This implements the **Variable Selection** step. It isolates the specific financial variables (BTC/ETH Close, Stablecoin Volumes) required for the Hilbert-Huang Transform and Structural Break tests.\n",
        "\n",
        "### 12. `merge_data_sources` (Task 12 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The aggregated chain data (`chain_data`) and market series (`market_data`).\n",
        "*   **Processes:** It executes `join_datasets` to perform an inner join on the `Date` column, ensuring temporal alignment between on-chain and off-chain data. It verifies completeness via `verify_panel_completeness` and finalizes the structure via `finalize_panel`.\n",
        "*   **Outputs:** A `MergedPanelData` dataclass containing the unified econometric panel.\n",
        "*   **Research Context:** This implements the **Dataset Integration** step. It creates the master dataset $S_{merged} = S_{chain} \\cap S_{market}$ required for the multivariate SVAR analysis in Section 4.5.\n",
        "\n",
        "### 13. `construct_log_series` (Task 13 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The merged panel data (`merged_data`).\n",
        "*   **Processes:** It identifies target series via `identify_target_series` and applies logarithmic transformations via `apply_log_transformations`. It uses $y_t = \\ln(x_t)$ for prices and $y_t = \\ln(x_t + 1)$ for volumes to handle potential zeros.\n",
        "*   **Outputs:** A `LogTransformedData` dataclass containing the log-level series.\n",
        "*   **Research Context:** This implements the **Logarithmic Transformation** described in Section 3.1.2 (Equation 9). This transformation stabilizes variance and interprets changes as percentage shifts, which is standard for financial time series analysis.\n",
        "\n",
        "### 14. `perform_stationarity_tests` (Task 14 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The log-transformed data (`log_data`) and preprocessing config.\n",
        "*   **Processes:** It iterates through all series, configuring the Augmented Dickey-Fuller test via `get_adf_config` (Regression='ct', Autolag='AIC'). It executes `run_adf_test` to test the null hypothesis of a unit root ($H_0: \\alpha=1$) and summarizes the results via `summarize_stationarity`.\n",
        "*   **Outputs:** A `StationarityTestResults` dataclass identifying I(1) and I(0) series.\n",
        "*   **Research Context:** This implements the **Augmented Dickey-Fuller (ADF) Test** described in Section 3.1.1 (Equations 2-5). It determines the integration order of the variables, a critical prerequisite for selecting the appropriate form (levels vs. differences) for the SVAR model.\n",
        "\n",
        "### 15. `finalize_integration_order` (Task 15 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Log data (`log_data`), stationarity results (`stationarity_results`), and config.\n",
        "*   **Processes:** It computes the first difference $\\Delta y_t = y_t - y_{t-1}$ for all I(1) series via `compute_differences`. It verifies the stationarity of these differences via `verify_diff_stationarity` and maps each series to its appropriate analysis method (Bai-Perron uses levels, SVAR uses differences) via `map_series_to_methods`.\n",
        "*   **Outputs:** A `FinalizedSeriesData` dataclass containing the fully prepared dataset and method mappings.\n",
        "*   **Research Context:** This implements the **Data Transformation for Stationarity** step. It ensures that the input vectors for the SVAR model satisfy the stationarity condition required for stable estimation, while preserving level data for structural break detection.\n",
        "\n",
        "### 16. `run_bai_perron_eoa` (Task 16 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The finalized data (`finalized_data`) and analysis config.\n",
        "*   **Processes:** It extracts the EOA-EOA log-volume series and executes the **Bai-Perron Dynamic Programming Algorithm** via `analyze_series_breaks_robust`. This involves computing the global SSR matrix, finding optimal partitions for $k$ breaks, selecting $k$ via BIC, and computing the SupF statistic.\n",
        "*   **Outputs:** A `BaiPerronResults` dataclass containing break dates and statistics for USDT and USDC human flows.\n",
        "*   **Research Context:** This implements the **Structural Break Analysis (SBA)** for human behavior described in Section 4.2. It solves Equation (8): $\\{\\hat{T}_1, \\dots, \\hat{T}_m\\} = \\arg \\min SSR(T_1, \\dots, T_m)$ to detect the anticipatory shift in human sentiment on Nov 3, 2024.\n",
        "\n",
        "### 17. `run_bai_perron_sc` (Task 17 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The finalized data (`finalized_data`) and analysis config.\n",
        "*   **Processes:** It applies the same robust Bai-Perron analysis (`analyze_series_breaks_robust`) to the SC-SC (algorithmic) log-volume series.\n",
        "*   **Outputs:** An `SCBaiPerronResults` dataclass containing break dates for algorithmic flows.\n",
        "*   **Research Context:** This implements the **SBA for Algorithmic Behavior** (Section 4.4). It detects the delayed structural breaks in automated activity (Jan 2025), contrasting them with the early human response.\n",
        "\n",
        "### 18. `run_bai_perron_exchange` (Task 18 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The finalized data (`finalized_data`) and analysis config.\n",
        "*   **Processes:** It applies the Bai-Perron analysis to the exchange volume series (`log_Volume_USDT_USD`, etc.).\n",
        "*   **Outputs:** An `ExchangeBaiPerronResults` dataclass containing break dates for centralized exchange activity.\n",
        "*   **Research Context:** This implements the **SBA for Market Microstructure** (Section 4.3.1). It identifies the coincident break on Election Day (Nov 5), serving as a benchmark for the \"early warning\" property of the on-chain signal.\n",
        "\n",
        "### 19. `run_bai_perron_prices` (Task 19 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The finalized data (`finalized_data`) and analysis config.\n",
        "*   **Processes:** It applies the Bai-Perron analysis to the BTC and ETH log-price series.\n",
        "*   **Outputs:** A `PriceBaiPerronResults` dataclass containing break dates for asset prices.\n",
        "*   **Research Context:** This implements the **SBA for Asset Prices**. It confirms that price adjustments (Nov 6-9) lagged behind the on-chain behavioral shifts, validating the predictive utility of the EOA signal.\n",
        "\n",
        "### 20. `orchestrate_structural_breaks` (Task 20 Orchestrator)\n",
        "\n",
        "*   **Inputs:** The finalized data (`finalized_data`) and analysis config.\n",
        "*   **Processes:** This is a meta-orchestrator that iterates through all series flagged for structural break analysis. It calls the internal `callable_bai_perron` (wrapping the robust DP logic) for each series and aggregates the results.\n",
        "*   **Outputs:** A `StructuralBreakOrchestratorResult` dataclass containing a dictionary of all break detection results.\n",
        "*   **Research Context:** This unifies the **Structural Break Detection** phase, ensuring a consistent methodological application across all variable types (Human, Algo, Exchange, Price).\n",
        "\n",
        "### 21. `execute_aaft_robustness` (Task 21 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Finalized data, break results, and analysis config.\n",
        "*   **Processes:** It executes `compute_surrogate_stats` for each series with a detected break. This involves generating 1000 AAFT surrogates via `generate_aaft_surrogates` (Gaussianization $\\to$ Phase Randomization $\\to$ Remapping), running the Bai-Perron algorithm on each surrogate, and computing the empirical p-value of the observed SupF statistic.\n",
        "*   **Outputs:** An `AAFTRobustnessResults` dataclass containing p-values and surrogate statistics.\n",
        "*   **Research Context:** This implements the **Robustness Verification** described in Section 3.2.2 and 4.2. It tests the null hypothesis that the observed breaks are artifacts of linear autocorrelation, ensuring the statistical significance of the findings.\n",
        "\n",
        "### 22. `decompose_prices_emd` (Task 22 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Finalized data and analysis config.\n",
        "*   **Processes:** It executes `decompose_series` for BTC and ETH log-prices. This calls `empirical_mode_decomposition`, which iteratively sifts the signal using cubic spline envelopes (`get_envelopes`) until the residue is monotonic, extracting Intrinsic Mode Functions (IMFs).\n",
        "*   **Outputs:** An `EMDDecompositionResults` dataclass containing the IMFs and residues.\n",
        "*   **Research Context:** This implements the **Empirical Mode Decomposition (EMD)** described in Section 3.2.1 (Equations 14-17). It decomposes the non-stationary price series into intrinsic oscillatory modes, a prerequisite for the Hilbert Spectral Analysis.\n",
        "\n",
        "### 23. `compute_hilbert_spectrum` (Task 23 Orchestrator)\n",
        "\n",
        "*   **Inputs:** EMD results (`emd_results`).\n",
        "*   **Processes:** It executes `compute_analytic_properties` for each IMF to derive the analytic signal $z(t) = x(t) + iH(t)$ via the Hilbert Transform (Equation 18). It computes instantaneous phase $\\phi(t)$, frequency $\\omega(t) = d\\phi/dt$, and amplitude $A(t)$. It then aggregates these into a time-frequency distribution via `construct_spectrum`.\n",
        "*   **Outputs:** A `HilbertTransformResults` dataclass containing the Hilbert Spectra.\n",
        "*   **Research Context:** This implements the **Hilbert Spectral Analysis** described in Section 3.2.1 (Equations 18-21). It provides the high-resolution time-frequency representation necessary to detect transient volatility events.\n",
        "\n",
        "### 24. `detect_extreme_events` (Task 24 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Hilbert results, analysis config, and date index.\n",
        "*   **Processes:** It computes the instantaneous energy $IE(t) = \\int H^2(t, \\omega) d\\omega$ via `compute_instantaneous_energy`. It calculates the adaptive threshold $E_{th} = E_\\mu + 4\\sigma$ via `compute_threshold` and identifies dates where $IE(t) > E_{th}$ via `find_extreme_events`.\n",
        "*   **Outputs:** An `ExtremeEventResults` dataclass containing detected event dates.\n",
        "*   **Research Context:** This implements the **Extreme Event Detection** described in Section 3.2.1 (Equations 22-24). It identifies the specific dates of market turbulence (Nov 7, Nov 10) associated with the election shock.\n",
        "\n",
        "### 25. `orchestrate_hht` (Task 25 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Finalized data and analysis config.\n",
        "*   **Processes:** This is a meta-orchestrator for the HHT pipeline. It sequentially calls `callable_emd`, `callable_hilbert_spectrum`, and `callable_extreme_events` for the target price series.\n",
        "*   **Outputs:** An `HHTOrchestratorResult` dataclass aggregating all HHT artifacts.\n",
        "*   **Research Context:** This unifies the **Non-Linear Signal Processing** phase, providing a streamlined execution flow for detecting extreme market events.\n",
        "\n",
        "### 26. `execute_hht_robustness` (Task 26 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Finalized data, HHT results, and analysis config.\n",
        "*   **Processes:** It executes `compute_surrogate_maxima` to generate 1000 AAFT surrogates and compute their maximum instantaneous energy. It then calculates the empirical p-value of the observed maximum energy via `calculate_hht_p_value`.\n",
        "*   **Outputs:** An `HHTRobustnessResults` dataclass containing significance tests for extreme events.\n",
        "*   **Research Context:** This implements the **Robustness Verification for HHT**. It ensures that the detected extreme events are statistically significant deviations from the underlying process, not random noise.\n",
        "\n",
        "### 27. `prepare_svar_inputs` (Task 27 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Finalized data and analysis config.\n",
        "*   **Processes:** It selects the differenced EOA-EOA volume series via `select_svar_series`, cleans missing values, and splits the dataset into pre-election ($t < T_{break}$) and post-election ($t \\ge T_{break}$) regimes via `split_regimes`.\n",
        "*   **Outputs:** A `SVARInputData` dataclass containing the stationary input vectors for the VAR model.\n",
        "*   **Research Context:** This implements the **Data Preparation for SVAR**. It constructs the vector $Y_t = [\\Delta \\log V_{USDC}, \\Delta \\log V_{USDT}]'$ and defines the structural break point (Nov 5) for the regime-dependent analysis.\n",
        "\n",
        "### 28. `estimate_reduced_var` (Task 28 Orchestrator)\n",
        "\n",
        "*   **Inputs:** SVAR inputs and analysis config.\n",
        "*   **Processes:** It initializes the VAR model via `initialize_var_model`, selects the optimal lag order $p^*$ using the Akaike Information Criterion (AIC) via `select_optimal_lag`, and estimates the reduced-form model on the full sample via `fit_var_model`.\n",
        "*   **Outputs:** A `VARModelResults` dataclass containing the fitted model and optimal lag.\n",
        "*   **Research Context:** This implements the **Reduced-Form VAR Estimation** (Equation 29). It determines the lag structure required to capture the dynamic interdependencies between stablecoin flows.\n",
        "\n",
        "### 29. `estimate_regime_vars` (Task 29 Orchestrator)\n",
        "\n",
        "*   **Inputs:** SVAR inputs and full model results.\n",
        "*   **Processes:** It estimates separate VAR models for the pre-election and post-election periods using the optimal lag $p^*$ via `estimate_regime_var`. It extracts and vectorizes the coefficients ($\\hat{\\theta}_{pre}, \\hat{\\theta}_{post}$) and their covariance matrices.\n",
        "*   **Outputs:** A `RegimeComparisonData` dataclass containing the regime-specific model estimates.\n",
        "*   **Research Context:** This implements the **Regime-Dependent Estimation**. It provides the parameter estimates required to test for structural changes in the transmission mechanism of volatility.\n",
        "\n",
        "### 30. `identify_structural_shocks` (Task 30 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Regime data, SVAR inputs, and analysis config.\n",
        "*   **Processes:** It executes `compute_cholesky_impact` for each regime and ordering. This involves permuting the residual covariance matrix $\\Sigma_u$ to match the identification ordering (e.g., USDC $\\to$ USDT) and performing Cholesky decomposition $\\Sigma_u = PP'$ to obtain the structural impact matrix $P$ (Equation 32).\n",
        "*   **Outputs:** A `SVARIdentificationResults` dataclass containing the structural impact matrices.\n",
        "*   **Research Context:** This implements the **Structural Identification** described in Section 3.3.1 (Equations 30-32). It recovers the orthogonal structural shocks $\\varepsilon_t$ and quantifies the contemporaneous spillover effects between stablecoins.\n",
        "\n",
        "### 31. `perform_wald_test` (Task 31 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Regime comparison data.\n",
        "*   **Processes:** It executes `calculate_wald_stat` to compute the Wald statistic $W = (\\hat{\\theta}_{post} - \\hat{\\theta}_{pre})' (\\hat{\\Sigma}_{pre} + \\hat{\\Sigma}_{post})^{-1} (\\hat{\\theta}_{post} - \\hat{\\theta}_{pre})$. It then computes the p-value from the $\\chi^2$ distribution via `compute_chi2_p_value`.\n",
        "*   **Outputs:** A `WaldTestResult` dataclass containing the test statistic and significance.\n",
        "*   **Research Context:** This implements the **Wald Test for Structural Change** described in Section 3.3.2 (Equations 33-34). It formally tests whether the election induced a statistically significant shift in the dynamic relationships between stablecoins.\n",
        "\n",
        "### 32. `synthesize_study_results` (Task 32 Orchestrator)\n",
        "\n",
        "*   **Inputs:** Results from all analysis phases (Breaks, AAFT, HHT, SVAR).\n",
        "*   **Processes:** It validates the findings against the paper's claims via `verify_breaks` (checking dates like Nov 3), `verify_hht` (checking dates like Nov 7/10), and `verify_svar` (checking Wald significance and spillover increase). It aggregates these checks into a final report.\n",
        "*   **Outputs:** A `FinalStudyReport` dataclass summarizing the replication success.\n",
        "*   **Research Context:** This implements the **Synthesis and Validation** phase. It confirms that the computational pipeline reproduces the empirical findings of the study: the early warning signal in human flows, the delayed algorithmic response, and the post-election regime shift in volatility.\n",
        "\n",
        "### 33. `run_obps_pipeline` (Top-Level Orchestrator)\n",
        "\n",
        "*   **Inputs:** Raw chain DataFrame, raw market DataFrame, and configuration dictionary.\n",
        "*   **Processes:** This master function sequentially invokes every orchestrator from Task 1 to Task 32. It manages the data flow, passing validated and transformed data objects between stages (Validation $\\to$ Processing $\\to$ Panel Construction $\\to$ Analysis $\\to$ Synthesis).\n",
        "*   **Outputs:** An `OBPSPipelineResult` dataclass containing every intermediate and final artifact produced by the system.\n",
        "*   **Research Context:** This is the **End-to-End System Implementation**. It represents the complete, automated realization of the OBPS framework, capable of ingesting raw data and producing the full set of econometric insights and validation metrics required for political risk monitoring.\n",
        "\n",
        "<br><br>\n",
        "### **Usage Example**\n",
        "\n",
        "The following code snippet uses synthetically generated data to illustrate, in a step by step fashion, how to run the \"*Early-Warning Signals of Political Risk in Stablecoin Markets: Human and Algorithmic Behavior Around the 2024 U.S. Election*\" End-to-End research pipeline accurately:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ==============================================================================\n",
        "# USAGE EXAMPLE: On-Chain Behavioral Pre-Emption System (OBPS)\n",
        "# ==============================================================================\n",
        "\n",
        "# This script demonstrates the end-to-end execution of the OBPS pipeline using\n",
        "# synthetically generated data that mirrors the structure and statistical\n",
        "# properties of the 2024 U.S. Election study.\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Synthetic Data Generation\n",
        "# ------------------------------------------------------------------------------\n",
        "# We generate two DataFrames: 'df_chain_raw' (blockchain logs) and 'df_market_raw'\n",
        "# (exchange data). The data is seeded to contain the structural breaks and\n",
        "# volatility events hypothesized in the study (e.g., Nov 3 human signal).\n",
        "\n",
        "def generate_synthetic_data():\n",
        "    \"\"\"\n",
        "    Generates implementation-grade synthetic datasets for the OBPS pipeline.\n",
        "    \"\"\"\n",
        "    print(\"Generating synthetic research data...\")\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # --- 1. Define Scope ---\n",
        "    start_date = pd.Timestamp(\"2024-03-01\", tz=\"UTC\")\n",
        "    end_date = pd.Timestamp(\"2025-02-28\", tz=\"UTC\")\n",
        "    days = pd.date_range(start_date, end_date, freq=\"D\")\n",
        "    n_days = len(days)\n",
        "    \n",
        "    # --- 2. Generate Market Data (df_market_raw) ---\n",
        "    # We simulate BTC/ETH prices with a shock around Nov 5\n",
        "    market_rows = []\n",
        "    symbols = [\"BTC/USD\", \"ETH/USD\", \"USDT/USD\", \"USDC/USD\"]\n",
        "    \n",
        "    for sym in symbols:\n",
        "        # Base price and volatility\n",
        "        if \"BTC\" in sym: price, vol = 60000, 0.02\n",
        "        elif \"ETH\" in sym: price, vol = 3000, 0.03\n",
        "        else: price, vol = 1.0, 0.001 # Stablecoins\n",
        "        \n",
        "        # Random walk\n",
        "        returns = np.random.normal(0, vol, n_days)\n",
        "        \n",
        "        # Inject Election Shock (Nov 5 - Nov 10)\n",
        "        election_idx = days.get_loc(\"2024-11-05\")\n",
        "        returns[election_idx:election_idx+5] += np.random.normal(0.05, 0.02, 5) # 5% shock\n",
        "        \n",
        "        price_series = price * np.cumprod(1 + returns)\n",
        "        volume_series = np.random.lognormal(16, 1, n_days) # ~$10M daily volume base\n",
        "        \n",
        "        # Inject Volume Shock for Stablecoins on Election Day\n",
        "        if \"USD\" in sym and \"BTC\" not in sym and \"ETH\" not in sym:\n",
        "            volume_series[election_idx] *= 5.0\n",
        "            \n",
        "        for i, d in enumerate(days):\n",
        "            market_rows.append({\n",
        "                \"Date\": d,\n",
        "                \"Symbol\": sym,\n",
        "                \"Close\": price_series[i],\n",
        "                \"Volume\": volume_series[i]\n",
        "            })\n",
        "            \n",
        "    df_market_raw = pd.DataFrame(market_rows)\n",
        "    \n",
        "    # --- 3. Generate On-Chain Data (df_chain_raw) ---\n",
        "    # We simulate ~500 transactions per day for efficiency in this example\n",
        "    # (Real data would be millions)\n",
        "    chain_rows = []\n",
        "    \n",
        "    usdt_addr = \"0xdac17f958d2ee523a2206206994597c13d831ec7\"\n",
        "    usdc_addr = \"0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48\"\n",
        "    \n",
        "    for i, d in enumerate(days):\n",
        "        # Base transaction count\n",
        "        n_tx = 500\n",
        "        \n",
        "        # Inject Human Signal (EOA-EOA) Surge on Nov 3\n",
        "        is_human_signal = (d.date() == pd.Timestamp(\"2024-11-03\").date())\n",
        "        if is_human_signal:\n",
        "            n_tx = 2000 # Surge\n",
        "            \n",
        "        for _ in range(n_tx):\n",
        "            # Randomize Token\n",
        "            token = usdt_addr if np.random.random() > 0.5 else usdc_addr\n",
        "            \n",
        "            # Randomize Topology\n",
        "            # Normal: 70% Human, 30% Algo\n",
        "            # Signal Day: 95% Human\n",
        "            if is_human_signal:\n",
        "                is_human = np.random.random() < 0.95\n",
        "            else:\n",
        "                is_human = np.random.random() < 0.70\n",
        "                \n",
        "            if is_human:\n",
        "                f_contract, t_contract = 0, 0\n",
        "            else:\n",
        "                f_contract, t_contract = 1, 1\n",
        "                \n",
        "            # Value (6 decimals)\n",
        "            val_usd = np.random.exponential(1000) # Avg $1000\n",
        "            val_int = int(val_usd * 1e6)\n",
        "            \n",
        "            # Timestamp (random time within the day)\n",
        "            ts_offset = np.random.randint(0, 86400)\n",
        "            ts = int(d.timestamp()) + ts_offset\n",
        "            \n",
        "            chain_rows.append({\n",
        "                \"timeStamp\": ts,\n",
        "                \"tokenAddress\": token,\n",
        "                \"from\": f\"0xsender{i}\",\n",
        "                \"to\": f\"0xreceiver{i}\",\n",
        "                \"value\": str(val_int), # String as per schema\n",
        "                \"fromIsContract\": f_contract,\n",
        "                \"toIsContract\": t_contract,\n",
        "                \"transactionHash\": f\"0xhash{len(chain_rows)}\",\n",
        "                \"blockNumber\": 1000000 + i,\n",
        "                \"logIndex\": len(chain_rows) % 100,\n",
        "                \"txStatus\": 0, # Success\n",
        "                \"isInternal\": 0\n",
        "            })\n",
        "            \n",
        "    df_chain_raw = pd.DataFrame(chain_rows)\n",
        "    \n",
        "    print(f\"Generated {len(df_market_raw)} market rows and {len(df_chain_raw)} chain rows.\")\n",
        "    return df_chain_raw, df_market_raw\n",
        "\n",
        "# Generate the data\n",
        "df_chain_raw, df_market_raw = generate_synthetic_data()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 2: Load Configuration\n",
        "# ------------------------------------------------------------------------------\n",
        "# We define the YAML content directly here for the example, mimicking file read.\n",
        "yaml_content = \"\"\"\n",
        "meta:\n",
        "  study_title: \"Early-Warning Signals of Political Risk in Stablecoin Markets\"\n",
        "  authors: [\"Mukhia\", \"Sharma\", \"Luwang\", \"Nurujjaman\", \"Hens\", \"Saha\", \"Chakraborty\"]\n",
        "  observation_window:\n",
        "    start_date: \"2024-03-01\"\n",
        "    end_date: \"2025-02-28\"\n",
        "    timezone: \"UTC\"\n",
        "  critical_events:\n",
        "    election_day: \"2024-11-05\"\n",
        "    human_signal_date: \"2024-11-03\"\n",
        "  frequency: \"1D\"\n",
        "\n",
        "schemas:\n",
        "  chain_raw:\n",
        "    columns:\n",
        "      timeStamp: \"int64\"\n",
        "      tokenAddress: \"string\"\n",
        "      from: \"string\"\n",
        "      to: \"string\"\n",
        "      value: \"object\"\n",
        "      fromIsContract: \"int8\"\n",
        "      toIsContract: \"int8\"\n",
        "      transactionHash: \"string\"\n",
        "      blockNumber: \"int64\"\n",
        "      logIndex: \"int64\"\n",
        "      txStatus: \"int8\"\n",
        "      isInternal: \"int8\"\n",
        "    assets:\n",
        "      USDT: {address: \"0xdac17f958d2ee523a2206206994597c13d831ec7\", decimals: 6}\n",
        "      USDC: {address: \"0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48\", decimals: 6}\n",
        "    topology_filters:\n",
        "      human_signal: {fromIsContract: 0, toIsContract: 0}\n",
        "      algo_signal: {fromIsContract: 1, toIsContract: 1}\n",
        "\n",
        "  market_raw:\n",
        "    columns:\n",
        "      Date: \"datetime64[ns]\"\n",
        "      Symbol: \"string\"\n",
        "      Close: \"float64\"\n",
        "      Volume: \"float64\"\n",
        "    symbols: [\"BTC/USD\", \"ETH/USD\", \"USDT/USD\", \"USDC/USD\"]\n",
        "\n",
        "preprocessing:\n",
        "  normalization_equation: \"value / (10 ** decimals)\"\n",
        "  handling:\n",
        "    drop_zero_value: true\n",
        "    drop_failed_tx: true\n",
        "    tx_status_column: \"txStatus\"\n",
        "    accepted_status: [0]\n",
        "  transformations:\n",
        "    logarithm: true\n",
        "    differencing: \"conditional\"\n",
        "  stationarity_test:\n",
        "    method: \"Augmented Dickey-Fuller\"\n",
        "    regression: \"ct\"\n",
        "    autolag: \"AIC\"\n",
        "    significance_level: 0.05\n",
        "    critical_values_source: \"MacKinnon (1996)\"\n",
        "\n",
        "bai_perron:\n",
        "  model_type: \"mean_shift\"\n",
        "  algorithm: \"dynamic_programming\"\n",
        "  parameters:\n",
        "    max_breaks: 5\n",
        "    trimming_epsilon: 0.15\n",
        "    significance_test: \"SupF\"\n",
        "    alpha: 0.05\n",
        "  robustness:\n",
        "    method: \"AAFT Surrogates\"\n",
        "    iterations: 100 # Reduced from 1000 for example speed\n",
        "    null_hypothesis: \"Linear Gaussian Process\"\n",
        "\n",
        "hht:\n",
        "  target_variable: \"Close\"\n",
        "  emd_parameters:\n",
        "    spline_type: \"cubic\"\n",
        "    sifting_stop_sd: 0.2\n",
        "    max_imfs: 10\n",
        "  spectrum_parameters:\n",
        "    type: \"Hilbert\"\n",
        "    energy_norm: \"max_energy\"\n",
        "  extreme_event_detection:\n",
        "    formula: \"mean + B * sigma\"\n",
        "    B: 4\n",
        "\n",
        "svar:\n",
        "  target_variables: [\"USDC_log_diff\", \"USDT_log_diff\"]\n",
        "  model_selection:\n",
        "    criterion: \"AIC\"\n",
        "    max_lags: 12\n",
        "  identification:\n",
        "    method: \"Cholesky\"\n",
        "    orderings:\n",
        "      - [\"USDC\", \"USDT\"]\n",
        "      - [\"USDT\", \"USDC\"]\n",
        "  regime_analysis:\n",
        "    break_date: \"2024-11-05\"\n",
        "    test: \"Wald Statistic\"\n",
        "    hypothesis: \"one_sided\"\n",
        "\n",
        "analysis_targets:\n",
        "  structural_break_series:\n",
        "    blockchain:\n",
        "      - \"log V_EOA_EOA_USDT\"\n",
        "      - \"log V_EOA_EOA_USDC\"\n",
        "      - \"log V_SC_SC_USDT\"\n",
        "      - \"log V_SC_SC_USDC\"\n",
        "    exchange:\n",
        "      - \"log Volume_USDT_USD\"\n",
        "      - \"log Volume_USDC_USD\"\n",
        "    prices:\n",
        "      - \"log Close_BTC_USD\"\n",
        "      - \"log Close_ETH_USD\"\n",
        "  hht_series:\n",
        "    - \"log Close_BTC_USD\"\n",
        "    - \"log Close_ETH_USD\"\n",
        "  svar_series:\n",
        "    blockchain_EOA:\n",
        "      - \"Δ log V_EOA_EOA_USDC\"\n",
        "      - \"Δ log V_EOA_EOA_USDT\"\n",
        "    exchange:\n",
        "      - \"Δ log Volume_USDC_USD\"\n",
        "      - \"Δ log Volume_USDT_USD\"\n",
        "\"\"\"\n",
        "\n",
        "# Parse the YAML string into a Python dictionary\n",
        "study_config = yaml.safe_load(yaml_content)\n",
        "print(\"Configuration loaded successfully.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 3: Execute Pipeline\n",
        "# ------------------------------------------------------------------------------\n",
        "# We invoke the top-level orchestrator with our prepared data and config.\n",
        "# This will trigger the full sequence of 32 tasks.\n",
        "\n",
        "try:\n",
        "    pipeline_results = run_obps_pipeline(\n",
        "        df_chain_raw=df_chain_raw,\n",
        "        df_market_raw=df_market_raw,\n",
        "        study_config=study_config\n",
        "    )\n",
        "    print(\"\\nPipeline execution successful!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nPipeline execution failed: {e}\")\n",
        "    # In a real notebook, we would raise e here to see the traceback\n",
        "    # raise e\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 4: Inspect Results\n",
        "# ------------------------------------------------------------------------------\n",
        "# We can now access the rich structured data returned by the pipeline.\n",
        "\n",
        "if 'pipeline_results' in locals():\n",
        "    res = pipeline_results\n",
        "    \n",
        "    print(\"\\n--- Final Report Summary ---\")\n",
        "    print(f\"Conclusion: {res.final_report.overall_conclusion}\")\n",
        "    \n",
        "    print(\"\\n--- Structural Break Detection (Human Signal) ---\")\n",
        "    # Accessing the result for USDT EOA-EOA\n",
        "    usdt_breaks = res.structural_breaks.results.get(\"log_V_EOA_EOA_USDT\")\n",
        "    if usdt_breaks:\n",
        "        dates = [d.date() for d in usdt_breaks.break_dates]\n",
        "        sup_f = usdt_breaks.sup_f_stat\n",
        "        print(f\"USDT EOA-EOA Breaks: {dates}\")\n",
        "        print(f\"SupF Statistic: {sup_f:.2f}\")\n",
        "        \n",
        "    print(\"\\n--- SVAR Regime Shift Analysis ---\")\n",
        "    wald = res.wald_test\n",
        "    print(f\"Wald Statistic: {wald.wald_statistic:.2f}\")\n",
        "    print(f\"p-value: {wald.p_value:.4e}\")\n",
        "    print(f\"Significant Shift: {wald.is_significant}\")\n",
        "    \n",
        "    print(\"\\n--- HHT Extreme Events ---\")\n",
        "    btc_events = res.hht_analysis.results.get(\"log_Close_BTC_USD\", {}).get(\"events\")\n",
        "    if btc_events:\n",
        "        dates = [d.date() for d in btc_events.extreme_event_dates]\n",
        "        print(f\"BTC Extreme Events: {dates}\")\n",
        "```\n",
        "\n",
        "Note: The Usage Example assumes that the user has imported all the relevant Python modules and callables into his/her/their Python enviroment.\n",
        "<br>"
      ],
      "metadata": {
        "id": "bB6Wie4O1ArV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate `STUDY_CONFIG` Parameter Integrity\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate and parse the study configuration dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1:  Load the `study_parameters` dictionary and verify structural completeness.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ValidatedMetaConfig:\n",
        "    \"\"\"\n",
        "    Immutable container for validated metadata and temporal parameters.\n",
        "\n",
        "    Attributes:\n",
        "        start_date (datetime): The start of the observation window (UTC).\n",
        "        end_date (datetime): The end of the observation window (UTC).\n",
        "        election_day (datetime): The date of the U.S. election (UTC).\n",
        "        human_signal_date (datetime): The date of the structural break in human activity (UTC).\n",
        "        timezone_str (str): The timezone string, strictly 'UTC'.\n",
        "    \"\"\"\n",
        "    start_date: datetime\n",
        "    end_date: datetime\n",
        "    election_day: datetime\n",
        "    human_signal_date: datetime\n",
        "    timezone_str: str\n",
        "\n",
        "def validate_meta_config(meta_config: Dict[str, Any]) -> ValidatedMetaConfig:\n",
        "    \"\"\"\n",
        "    Validates the 'meta' section of the study configuration.\n",
        "\n",
        "    This function parses date strings, enforces UTC timezone, and verifies\n",
        "    temporal ordering constraints (start < end, events within window).\n",
        "\n",
        "    Args:\n",
        "        meta_config (Dict[str, Any]): The 'meta' dictionary from STUDY_CONFIG.\n",
        "\n",
        "    Returns:\n",
        "        ValidatedMetaConfig: A structured object containing parsed dates.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If dates are malformed, logically invalid, or timezone is not UTC.\n",
        "        KeyError: If required keys are missing.\n",
        "    \"\"\"\n",
        "    # Define required keys for the observation window\n",
        "    # Check for existence of 'observation_window' key\n",
        "    if \"observation_window\" not in meta_config:\n",
        "        raise KeyError(\"Missing 'observation_window' in meta config.\")\n",
        "\n",
        "    obs_window = meta_config[\"observation_window\"]\n",
        "\n",
        "    # Validate Timezone - Strict adherence to UTC is required for replication\n",
        "    if obs_window.get(\"timezone\") != \"UTC\":\n",
        "        raise ValueError(f\"Timezone must be strictly 'UTC', found: {obs_window.get('timezone')}\")\n",
        "\n",
        "    # Helper to parse dates strictly as YYYY-MM-DD\n",
        "    def parse_date(date_str: str, field_name: str) -> datetime:\n",
        "        try:\n",
        "            # Parse string to datetime\n",
        "            dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "            # Set timezone to UTC explicitly\n",
        "            return dt.replace(tzinfo=timezone.utc)\n",
        "        except ValueError:\n",
        "            raise ValueError(f\"Invalid date format for '{field_name}': {date_str}. Expected YYYY-MM-DD.\")\n",
        "\n",
        "    # Parse start and end dates\n",
        "    start_date = parse_date(obs_window[\"start_date\"], \"start_date\")\n",
        "    end_date = parse_date(obs_window[\"end_date\"], \"end_date\")\n",
        "\n",
        "    # Enforce temporal ordering: start must be strictly before end\n",
        "    if start_date >= end_date:\n",
        "        raise ValueError(f\"start_date ({start_date.date()}) must be strictly before end_date ({end_date.date()}).\")\n",
        "\n",
        "    # Validate Critical Events\n",
        "    if \"critical_events\" not in meta_config:\n",
        "        raise KeyError(\"Missing 'critical_events' in meta config.\")\n",
        "\n",
        "    events = meta_config[\"critical_events\"]\n",
        "    election_day = parse_date(events[\"election_day\"], \"election_day\")\n",
        "    human_signal_date = parse_date(events[\"human_signal_date\"], \"human_signal_date\")\n",
        "\n",
        "    # Enforce inclusion: events must be within [start_date, end_date]\n",
        "    # Equation: start_date <= event_date <= end_date\n",
        "    if not (start_date <= election_day <= end_date):\n",
        "        raise ValueError(f\"election_day ({election_day.date()}) is outside the observation window.\")\n",
        "\n",
        "    if not (start_date <= human_signal_date <= end_date):\n",
        "        raise ValueError(f\"human_signal_date ({human_signal_date.date()}) is outside the observation window.\")\n",
        "\n",
        "    # Return validated dataclass\n",
        "    return ValidatedMetaConfig(\n",
        "        start_date=start_date,\n",
        "        end_date=end_date,\n",
        "        election_day=election_day,\n",
        "        human_signal_date=human_signal_date,\n",
        "        timezone_str=\"UTC\"\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate numerical parameter ranges and types.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ValidatedSchemas:\n",
        "    \"\"\"\n",
        "    Immutable container for validated schema definitions.\n",
        "\n",
        "    Attributes:\n",
        "        chain_columns (Set[str]): Set of required columns for on-chain data.\n",
        "        market_columns (Set[str]): Set of required columns for market data.\n",
        "        market_symbols (List[str]): List of required market symbols.\n",
        "        assets (Dict[str, Any]): Validated asset definitions (USDT/USDC).\n",
        "    \"\"\"\n",
        "    chain_columns: Set[str]\n",
        "    market_columns: Set[str]\n",
        "    market_symbols: List[str]\n",
        "    assets: Dict[str, Any]\n",
        "\n",
        "def validate_schemas(schemas_config: Dict[str, Any]) -> ValidatedSchemas:\n",
        "    \"\"\"\n",
        "    Validates the 'schemas' section of the study configuration.\n",
        "\n",
        "    Verifies that all required columns, assets, and symbols are present and\n",
        "    conform to the specified formats (e.g., Ethereum address regex).\n",
        "\n",
        "    Args:\n",
        "        schemas_config (Dict[str, Any]): The 'schemas' dictionary from STUDY_CONFIG.\n",
        "\n",
        "    Returns:\n",
        "        ValidatedSchemas: A structured object containing validated schema sets.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required columns/assets are missing or malformed.\n",
        "        KeyError: If major schema sections are missing.\n",
        "    \"\"\"\n",
        "    # 1. Validate Chain Raw Schema\n",
        "    if \"chain_raw\" not in schemas_config:\n",
        "        raise KeyError(\"Missing 'chain_raw' in schemas config.\")\n",
        "\n",
        "    chain_config = schemas_config[\"chain_raw\"]\n",
        "\n",
        "    # Required columns for on-chain ledger\n",
        "    required_chain_cols = {\n",
        "        \"timeStamp\", \"tokenAddress\", \"from\", \"to\", \"value\",\n",
        "        \"fromIsContract\", \"toIsContract\", \"transactionHash\",\n",
        "        \"blockNumber\", \"logIndex\", \"txStatus\"\n",
        "    }\n",
        "\n",
        "    # Check if configured columns are a superset of required columns\n",
        "    configured_chain_cols = set(chain_config.get(\"columns\", {}).keys())\n",
        "    missing_chain_cols = required_chain_cols - configured_chain_cols\n",
        "    if missing_chain_cols:\n",
        "        raise ValueError(f\"Missing required chain columns: {missing_chain_cols}\")\n",
        "\n",
        "    # Validate Assets (USDT, USDC)\n",
        "    assets = chain_config.get(\"assets\", {})\n",
        "    required_assets = {\"USDT\", \"USDC\"}\n",
        "    if not required_assets.issubset(assets.keys()):\n",
        "        raise ValueError(f\"Missing required assets. Found: {list(assets.keys())}, Required: {required_assets}\")\n",
        "\n",
        "    # Regex for Ethereum Address: 0x followed by 40 hex chars\n",
        "    eth_address_pattern = re.compile(r\"^0x[0-9a-fA-F]{40}$\")\n",
        "\n",
        "    for asset_name in required_assets:\n",
        "        asset_def = assets[asset_name]\n",
        "        # Check address format\n",
        "        addr = asset_def.get(\"address\", \"\")\n",
        "        if not eth_address_pattern.match(addr):\n",
        "            raise ValueError(f\"Invalid Ethereum address for {asset_name}: {addr}\")\n",
        "\n",
        "        # Check decimals (Must be 6 for USDT/USDC per ERC-20 spec)\n",
        "        decimals = asset_def.get(\"decimals\")\n",
        "        if decimals != 6:\n",
        "            raise ValueError(f\"Invalid decimals for {asset_name}: {decimals}. Expected 6.\")\n",
        "\n",
        "    # 2. Validate Market Raw Schema\n",
        "    if \"market_raw\" not in schemas_config:\n",
        "        raise KeyError(\"Missing 'market_raw' in schemas config.\")\n",
        "\n",
        "    market_config = schemas_config[\"market_raw\"]\n",
        "\n",
        "    # Required columns for market data\n",
        "    required_market_cols = {\"Date\", \"Symbol\", \"Close\", \"Volume\"}\n",
        "    configured_market_cols = set(market_config.get(\"columns\", {}).keys())\n",
        "    missing_market_cols = required_market_cols - configured_market_cols\n",
        "    if missing_market_cols:\n",
        "        raise ValueError(f\"Missing required market columns: {missing_market_cols}\")\n",
        "\n",
        "    # Required symbols\n",
        "    required_symbols = {\"BTC/USD\", \"ETH/USD\", \"USDT/USD\", \"USDC/USD\"}\n",
        "    configured_symbols = set(market_config.get(\"symbols\", []))\n",
        "    missing_symbols = required_symbols - configured_symbols\n",
        "    if missing_symbols:\n",
        "        raise ValueError(f\"Missing required market symbols: {missing_symbols}\")\n",
        "\n",
        "    return ValidatedSchemas(\n",
        "        chain_columns=configured_chain_cols,\n",
        "        market_columns=configured_market_cols,\n",
        "        market_symbols=market_config[\"symbols\"],\n",
        "        assets=assets\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate string-based model identifiers and create a configuration snapshot.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ValidatedAnalysisConfig:\n",
        "    \"\"\"\n",
        "    Immutable container for validated analysis parameters (Bai-Perron, HHT, SVAR).\n",
        "\n",
        "    Attributes:\n",
        "        bp_max_breaks (int): Maximum number of structural breaks.\n",
        "        bp_trimming (float): Trimming parameter epsilon.\n",
        "        hht_threshold_b (float): Extreme event threshold multiplier.\n",
        "        hht_max_imfs (int): Maximum number of IMFs.\n",
        "        svar_max_lags (int): Maximum lag order for VAR.\n",
        "        svar_break_date (datetime): Date of regime shift for SVAR.\n",
        "    \"\"\"\n",
        "    bp_max_breaks: int\n",
        "    bp_trimming: float\n",
        "    hht_threshold_b: float\n",
        "    hht_max_imfs: int\n",
        "    svar_max_lags: int\n",
        "    svar_break_date: datetime\n",
        "\n",
        "def validate_analysis_config(config: Dict[str, Any], obs_window: ValidatedMetaConfig) -> ValidatedAnalysisConfig:\n",
        "    \"\"\"\n",
        "    Validates the analysis-specific configurations (Bai-Perron, HHT, SVAR).\n",
        "\n",
        "    Enforces mathematical constraints on model parameters (e.g., trimming epsilon\n",
        "    must be < 0.5) and verifies that analysis dates align with the observation window.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The full STUDY_CONFIG dictionary.\n",
        "        obs_window (ValidatedMetaConfig): The validated metadata (for date checking).\n",
        "\n",
        "    Returns:\n",
        "        ValidatedAnalysisConfig: A structured object with validated parameters.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If parameters violate mathematical or logical constraints.\n",
        "    \"\"\"\n",
        "    # 1. Validate Bai-Perron\n",
        "    bp_config = config.get(\"bai_perron\", {})\n",
        "\n",
        "    # Model type check\n",
        "    if bp_config.get(\"model_type\") != \"mean_shift\":\n",
        "        raise ValueError(f\"Bai-Perron model_type must be 'mean_shift', found: {bp_config.get('model_type')}\")\n",
        "\n",
        "    bp_params = bp_config.get(\"parameters\", {})\n",
        "    max_breaks = bp_params.get(\"max_breaks\")\n",
        "    trimming = bp_params.get(\"trimming_epsilon\")\n",
        "\n",
        "    # Constraint: max_breaks >= 1\n",
        "    if not isinstance(max_breaks, int) or max_breaks < 1:\n",
        "        raise ValueError(f\"Bai-Perron max_breaks must be a positive integer. Found: {max_breaks}\")\n",
        "\n",
        "    # Constraint: 0 < trimming < 0.5\n",
        "    if not isinstance(trimming, float) or not (0 < trimming < 0.5):\n",
        "        raise ValueError(f\"Bai-Perron trimming_epsilon must be in (0, 0.5). Found: {trimming}\")\n",
        "\n",
        "    # 2. Validate HHT\n",
        "    hht_config = config.get(\"hht\", {})\n",
        "    ee_config = hht_config.get(\"extreme_event_detection\", {})\n",
        "    emd_config = hht_config.get(\"emd_parameters\", {})\n",
        "\n",
        "    # Constraint: B = 4 (per study)\n",
        "    b_val = ee_config.get(\"B\")\n",
        "    if b_val != 4:\n",
        "        raise ValueError(f\"HHT extreme event threshold B must be 4. Found: {b_val}\")\n",
        "\n",
        "    # Constraint: max_imfs > 0\n",
        "    max_imfs = emd_config.get(\"max_imfs\")\n",
        "    if not isinstance(max_imfs, int) or max_imfs < 1:\n",
        "        raise ValueError(f\"HHT max_imfs must be a positive integer. Found: {max_imfs}\")\n",
        "\n",
        "    # 3. Validate SVAR\n",
        "    svar_config = config.get(\"svar\", {})\n",
        "    model_sel = svar_config.get(\"model_selection\", {})\n",
        "    regime_an = svar_config.get(\"regime_analysis\", {})\n",
        "\n",
        "    # Constraint: max_lags > 0\n",
        "    max_lags = model_sel.get(\"max_lags\")\n",
        "    if not isinstance(max_lags, int) or max_lags < 1:\n",
        "        raise ValueError(f\"SVAR max_lags must be a positive integer. Found: {max_lags}\")\n",
        "\n",
        "    # Validate SVAR break date\n",
        "    break_date_str = regime_an.get(\"break_date\")\n",
        "    try:\n",
        "        svar_break_date = datetime.strptime(break_date_str, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Invalid SVAR break_date format: {break_date_str}\")\n",
        "\n",
        "    # Constraint: SVAR break date must be within observation window\n",
        "    if not (obs_window.start_date <= svar_break_date <= obs_window.end_date):\n",
        "        raise ValueError(f\"SVAR break_date ({svar_break_date.date()}) is outside observation window.\")\n",
        "\n",
        "    return ValidatedAnalysisConfig(\n",
        "        bp_max_breaks=max_breaks,\n",
        "        bp_trimming=trimming,\n",
        "        hht_threshold_b=float(b_val),\n",
        "        hht_max_imfs=max_imfs,\n",
        "        svar_max_lags=max_lags,\n",
        "        svar_break_date=svar_break_date\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ValidatedStudyConfig:\n",
        "    \"\"\"\n",
        "    Master container for the fully validated study configuration.\n",
        "\n",
        "    Attributes:\n",
        "        meta (ValidatedMetaConfig): Metadata and temporal scope.\n",
        "        schemas (ValidatedSchemas): Data structure definitions.\n",
        "        analysis (ValidatedAnalysisConfig): Model parameters.\n",
        "    \"\"\"\n",
        "    meta: ValidatedMetaConfig\n",
        "    schemas: ValidatedSchemas\n",
        "    analysis: ValidatedAnalysisConfig\n",
        "\n",
        "def validate_study_config(config: Dict[str, Any]) -> ValidatedStudyConfig:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the entire study configuration dictionary.\n",
        "\n",
        "    This function invokes specific validators for metadata, schemas, and analysis\n",
        "    parameters, aggregating the results into a single immutable configuration object.\n",
        "    It ensures that all parameters required for the OBPS pipeline are present,\n",
        "    correctly typed, and mathematically consistent.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The raw STUDY_CONFIG dictionary.\n",
        "\n",
        "    Returns:\n",
        "        ValidatedStudyConfig: The fully validated configuration object.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any parameter violates constraints.\n",
        "        KeyError: If required sections are missing.\n",
        "    \"\"\"\n",
        "    # Step 1: Validate Metadata (Dates, Timezone)\n",
        "    # This establishes the temporal ground truth for the study.\n",
        "    validated_meta = validate_meta_config(config.get(\"meta\", {}))\n",
        "\n",
        "    # Step 2: Validate Schemas (Columns, Assets)\n",
        "    # This ensures input dataframes will match expected structure.\n",
        "    validated_schemas = validate_schemas(config.get(\"schemas\", {}))\n",
        "\n",
        "    # Step 3: Validate Analysis Parameters (Models)\n",
        "    # This ensures mathematical models are configured correctly and consistent with metadata.\n",
        "    validated_analysis = validate_analysis_config(config, validated_meta)\n",
        "\n",
        "    return ValidatedStudyConfig(\n",
        "        meta=validated_meta,\n",
        "        schemas=validated_schemas,\n",
        "        analysis=validated_analysis\n",
        "    )\n"
      ],
      "metadata": {
        "id": "-OhPvVuhqqN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate df_chain_raw Structure and Integrity\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate df_chain_raw Structure and Integrity\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ChainValidationResult:\n",
        "    \"\"\"\n",
        "    Container for the results of the on-chain data validation process.\n",
        "\n",
        "    Attributes:\n",
        "        validated_df (pd.DataFrame): The DataFrame after type coercion and validation.\n",
        "        row_count (int): Total number of rows.\n",
        "        unique_count (int): Number of unique records based on the composite key.\n",
        "        duplicate_count (int): Number of duplicate records found.\n",
        "        distinct_tokens (int): Number of unique token addresses found.\n",
        "        date_range_start (pd.Timestamp): Earliest timestamp in the data.\n",
        "        date_range_end (pd.Timestamp): Latest timestamp in the data.\n",
        "    \"\"\"\n",
        "    validated_df: pd.DataFrame\n",
        "    row_count: int\n",
        "    unique_count: int\n",
        "    duplicate_count: int\n",
        "    distinct_tokens: int\n",
        "    date_range_start: pd.Timestamp\n",
        "    date_range_end: pd.Timestamp\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Validate Column Presence and Data Types\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_chain_columns(df: pd.DataFrame, required_columns: Set[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates that the DataFrame contains all required columns and enforces data types.\n",
        "\n",
        "    This function checks for the existence of columns defined in the schema.\n",
        "    It attempts to cast columns to their expected types (e.g., int64, int8, string).\n",
        "    It raises an error if columns are missing or if type coercion fails for critical fields.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw on-chain DataFrame.\n",
        "        required_columns (Set[str]): A set of column names required by the schema.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with validated and coerced data types.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required columns are missing or type coercion fails.\n",
        "    \"\"\"\n",
        "    # 1. Check for missing columns\n",
        "    # Set difference: required - existing\n",
        "    missing_cols = required_columns - set(df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"df_chain_raw is missing required columns: {missing_cols}\")\n",
        "\n",
        "    # 2. Define expected types mapping\n",
        "    # We map schema types to pandas/numpy dtypes for strict enforcement.\n",
        "    # Note: 'value' is kept as object (string) to preserve uint256 precision.\n",
        "    dtype_map = {\n",
        "        \"timeStamp\": \"int64\",\n",
        "        \"blockNumber\": \"int64\",\n",
        "        \"logIndex\": \"int64\",\n",
        "        \"fromIsContract\": \"int8\",\n",
        "        \"toIsContract\": \"int8\",\n",
        "        \"txStatus\": \"int8\",\n",
        "        # String/Object columns\n",
        "        \"tokenAddress\": \"string\",\n",
        "        \"from\": \"string\",\n",
        "        \"to\": \"string\",\n",
        "        \"transactionHash\": \"string\",\n",
        "        \"value\": \"object\"\n",
        "    }\n",
        "\n",
        "    # 3. Enforce Data Types\n",
        "    # We iterate through the map. If the column exists (it should), we cast it.\n",
        "    # We work on a copy to avoid SettingWithCopy warnings on the original input if it's a slice.\n",
        "    df_out = df.copy()\n",
        "\n",
        "    for col, dtype in dtype_map.items():\n",
        "        if col in df_out.columns:\n",
        "            try:\n",
        "                # Special handling for boolean-like integers or actual booleans\n",
        "                if dtype == \"int8\":\n",
        "                    # If it's boolean, convert to int first (True->1, False->0)\n",
        "                    if pd.api.types.is_bool_dtype(df_out[col]):\n",
        "                        df_out[col] = df_out[col].astype(int)\n",
        "\n",
        "                    # Check for nulls before casting to integer, as int8 doesn't support NaN\n",
        "                    if df_out[col].isnull().any():\n",
        "                        # If nulls exist in flags, we cannot proceed safely for this strict schema\n",
        "                        raise ValueError(f\"Column '{col}' contains null values, which are not allowed for int8 flags.\")\n",
        "\n",
        "                    df_out[col] = df_out[col].astype(\"int8\")\n",
        "\n",
        "                elif dtype == \"int64\":\n",
        "                    if df_out[col].isnull().any():\n",
        "                        raise ValueError(f\"Column '{col}' contains null values, which are not allowed for int64 fields.\")\n",
        "                    df_out[col] = df_out[col].astype(\"int64\")\n",
        "\n",
        "                elif dtype == \"string\":\n",
        "                    df_out[col] = df_out[col].astype(\"string\")\n",
        "\n",
        "                # 'value' is left as object/string, but we verify it's not all null\n",
        "                elif col == \"value\":\n",
        "                     if df_out[col].isnull().all():\n",
        "                         raise ValueError(\"Column 'value' is entirely null.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Failed to cast column '{col}' to {dtype}: {str(e)}\")\n",
        "\n",
        "    # 4. Check for entirely null columns (redundant check but good for safety)\n",
        "    for col in required_columns:\n",
        "        if df_out[col].isnull().all():\n",
        "            raise ValueError(f\"Column '{col}' is entirely null. Data quality insufficient.\")\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate Value Domains\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_chain_domains(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the value domains of specific columns in the on-chain DataFrame.\n",
        "\n",
        "    Checks:\n",
        "    - timeStamp: Must be positive integers.\n",
        "    - tokenAddress: Must match Ethereum address regex (0x + 40 hex chars).\n",
        "    - fromIsContract/toIsContract: Must be {0, 1}.\n",
        "    - txStatus: Must be {0, 1}.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame validated in Step 1.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any value domain constraint is violated.\n",
        "    \"\"\"\n",
        "    # 1. Validate timeStamp\n",
        "    # Must be strictly positive\n",
        "    if (df[\"timeStamp\"] <= 0).any():\n",
        "        invalid_count = (df[\"timeStamp\"] <= 0).sum()\n",
        "        raise ValueError(f\"Found {invalid_count} rows with timeStamp <= 0.\")\n",
        "\n",
        "    # 2. Validate tokenAddress format\n",
        "    # Regex: Starts with 0x, followed by exactly 40 hex characters (case-insensitive)\n",
        "    eth_address_pattern = r\"^0x[0-9a-fA-F]{40}$\"\n",
        "\n",
        "    # We use vectorized string matching. 'na=False' treats NaNs as non-matches (though we checked for NaNs earlier).\n",
        "    valid_addresses = df[\"tokenAddress\"].str.match(eth_address_pattern, na=False)\n",
        "    if not valid_addresses.all():\n",
        "        invalid_count = (~valid_addresses).sum()\n",
        "        # Show a few examples\n",
        "        examples = df.loc[~valid_addresses, \"tokenAddress\"].head(3).tolist()\n",
        "        raise ValueError(f\"Found {invalid_count} invalid tokenAddress values. Examples: {examples}\")\n",
        "\n",
        "    # 3. Validate Binary Flags (fromIsContract, toIsContract)\n",
        "    # Allowed values: {0, 1}\n",
        "    for col in [\"fromIsContract\", \"toIsContract\"]:\n",
        "        # We use isin. Since we cast to int8, we check against integers.\n",
        "        if not df[col].isin([0, 1]).all():\n",
        "            unique_vals = df[col].unique()\n",
        "            raise ValueError(f\"Column '{col}' contains invalid values: {unique_vals}. Expected {{0, 1}}.\")\n",
        "\n",
        "    # 4. Validate txStatus\n",
        "    # Allowed values: {0, 1}\n",
        "    if not df[\"txStatus\"].isin([0, 1]).all():\n",
        "        unique_vals = df[\"txStatus\"].unique()\n",
        "        raise ValueError(f\"Column 'txStatus' contains invalid values: {unique_vals}. Expected {{0, 1}}.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate Uniqueness and Record Count\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_chain_uniqueness(df: pd.DataFrame) -> Tuple[int, int, int]:\n",
        "    \"\"\"\n",
        "    Checks the uniqueness of the composite key (transactionHash, blockNumber, logIndex).\n",
        "\n",
        "    Calculates total rows, unique keys, and duplicate counts.\n",
        "    Logs warnings if row counts are suspiciously low.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame validated in Step 2.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[int, int, int]: (row_count, unique_count, duplicate_count)\n",
        "    \"\"\"\n",
        "    # 1. Define Composite Key\n",
        "    key_cols = [\"transactionHash\", \"blockNumber\", \"logIndex\"]\n",
        "\n",
        "    # 2. Calculate Counts\n",
        "    row_count = len(df)\n",
        "\n",
        "    # Count unique tuples.\n",
        "    # drop_duplicates() returns a dataframe with unique rows based on subset.\n",
        "    unique_count = len(df.drop_duplicates(subset=key_cols))\n",
        "\n",
        "    duplicate_count = row_count - unique_count\n",
        "\n",
        "    # 3. Heuristic Checks (Warnings)\n",
        "    # If row count is very low (< 1000), it might indicate a test sample, not full data.\n",
        "    # We won't raise an error, but we note it.\n",
        "    if row_count < 1000:\n",
        "        print(f\"WARNING: Row count is low ({row_count}). Ensure this is the full dataset.\")\n",
        "\n",
        "    return row_count, unique_count, duplicate_count\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_df_chain_raw(df_raw: pd.DataFrame, schemas: Any) -> ChainValidationResult:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the raw on-chain DataFrame.\n",
        "\n",
        "    Executes:\n",
        "    1. Column presence and type validation.\n",
        "    2. Value domain validation (regex, ranges, binary flags).\n",
        "    3. Uniqueness checks on the composite primary key.\n",
        "    4. Summary statistics calculation.\n",
        "\n",
        "    Args:\n",
        "        df_raw (pd.DataFrame): The raw input DataFrame.\n",
        "        schemas (ValidatedSchemas): The schema configuration object from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        ChainValidationResult: A dataclass containing the validated DataFrame and metadata.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any validation step fails.\n",
        "    \"\"\"\n",
        "    # Step 1: Column and Type Validation\n",
        "    # We use the set of columns defined in the ValidatedSchemas object\n",
        "    print(\"Task 2: Validating columns and types...\")\n",
        "    df_validated = validate_chain_columns(df_raw, schemas.chain_columns)\n",
        "\n",
        "    # Step 2: Domain Validation\n",
        "    print(\"Task 2: Validating value domains...\")\n",
        "    validate_chain_domains(df_validated)\n",
        "\n",
        "    # Step 3: Uniqueness and Stats\n",
        "    print(\"Task 2: Checking uniqueness...\")\n",
        "    row_count, unique_count, duplicate_count = validate_chain_uniqueness(df_validated)\n",
        "\n",
        "    # Calculate additional stats for the result object\n",
        "    distinct_tokens = df_validated[\"tokenAddress\"].nunique()\n",
        "\n",
        "    # Convert min/max timestamps to datetime for reporting\n",
        "    min_ts = df_validated[\"timeStamp\"].min()\n",
        "    max_ts = df_validated[\"timeStamp\"].max()\n",
        "    date_start = pd.to_datetime(min_ts, unit='s', utc=True)\n",
        "    date_end = pd.to_datetime(max_ts, unit='s', utc=True)\n",
        "\n",
        "    print(f\"Task 2 Complete. Rows: {row_count}, Unique: {unique_count}, Duplicates: {duplicate_count}\")\n",
        "    print(f\"Date Range: {date_start.date()} to {date_end.date()}\")\n",
        "\n",
        "    return ChainValidationResult(\n",
        "        validated_df=df_validated,\n",
        "        row_count=row_count,\n",
        "        unique_count=unique_count,\n",
        "        duplicate_count=duplicate_count,\n",
        "        distinct_tokens=distinct_tokens,\n",
        "        date_range_start=date_start,\n",
        "        date_range_end=date_end\n",
        "    )\n"
      ],
      "metadata": {
        "id": "2Iodo4p0vKjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Validate df_market_raw Structure and Coverage\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Validate df_market_raw Structure and Coverage\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class MarketValidationResult:\n",
        "    \"\"\"\n",
        "    Container for the results of the market data validation process.\n",
        "\n",
        "    Attributes:\n",
        "        validated_df (pd.DataFrame): The DataFrame after type coercion and validation.\n",
        "        symbol_counts (Dict[str, int]): Number of rows per symbol.\n",
        "        missing_dates (Dict[str, List[pd.Timestamp]]): List of missing dates per symbol within the window.\n",
        "        date_ranges (Dict[str, Tuple[pd.Timestamp, pd.Timestamp]]): Min and max dates per symbol.\n",
        "        total_rows (int): Total number of rows in the dataset.\n",
        "    \"\"\"\n",
        "    validated_df: pd.DataFrame\n",
        "    symbol_counts: Dict[str, int]\n",
        "    missing_dates: Dict[str, List[pd.Timestamp]]\n",
        "    date_ranges: Dict[str, Tuple[pd.Timestamp, pd.Timestamp]]\n",
        "    total_rows: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Validate Column Presence and Data Types\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_market_columns(df: pd.DataFrame, required_columns: Set[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates that the market DataFrame contains required columns and enforces strict data types.\n",
        "\n",
        "    Enforces:\n",
        "    - Date: datetime64[ns, UTC]\n",
        "    - Symbol: string\n",
        "    - Close: float64\n",
        "    - Volume: float64\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw market DataFrame.\n",
        "        required_columns (Set[str]): Set of required column names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The validated DataFrame with coerced types.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If columns are missing or type coercion fails.\n",
        "    \"\"\"\n",
        "    # 1. Check for missing columns\n",
        "    missing_cols = required_columns - set(df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"df_market_raw is missing required columns: {missing_cols}\")\n",
        "\n",
        "    # Work on a copy\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # 2. Enforce Data Types\n",
        "    try:\n",
        "        # Date: Force UTC\n",
        "        # errors='raise' ensures we don't silently ignore bad dates\n",
        "        df_out[\"Date\"] = pd.to_datetime(df_out[\"Date\"], utc=True, errors='raise')\n",
        "\n",
        "        # Symbol: String\n",
        "        df_out[\"Symbol\"] = df_out[\"Symbol\"].astype(\"string\")\n",
        "\n",
        "        # Close and Volume: Float64\n",
        "        # pd.to_numeric handles strings like \"100.50\" correctly, raises on \"abc\"\n",
        "        df_out[\"Close\"] = pd.to_numeric(df_out[\"Close\"], errors='raise').astype(\"float64\")\n",
        "        df_out[\"Volume\"] = pd.to_numeric(df_out[\"Volume\"], errors='raise').astype(\"float64\")\n",
        "\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Type coercion failed for market data: {str(e)}\")\n",
        "\n",
        "    # 3. Check for Nulls in Critical Columns\n",
        "    # We allow nulls in Close/Volume *only if* we plan to drop them later,\n",
        "    # but for strict validation, we should flag them.\n",
        "    # However, Date and Symbol must never be null.\n",
        "    if df_out[\"Date\"].isnull().any():\n",
        "        raise ValueError(\"Column 'Date' contains null values.\")\n",
        "    if df_out[\"Symbol\"].isnull().any():\n",
        "        raise ValueError(\"Column 'Symbol' contains null values.\")\n",
        "\n",
        "    # Log warning for nulls in numeric columns (handled in cleansing task)\n",
        "    null_close = df_out[\"Close\"].isnull().sum()\n",
        "    null_vol = df_out[\"Volume\"].isnull().sum()\n",
        "    if null_close > 0 or null_vol > 0:\n",
        "        print(f\"WARNING: Found nulls in market data - Close: {null_close}, Volume: {null_vol}\")\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Validate Symbol Coverage\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_symbol_coverage(df: pd.DataFrame, required_symbols: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Verifies that all required symbols are present and have sufficient data points.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The validated DataFrame from Step 1.\n",
        "        required_symbols (List[str]): List of expected symbols (e.g., BTC/USD).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: A dictionary mapping symbols to their row counts.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any required symbol is completely missing.\n",
        "    \"\"\"\n",
        "    # Get unique symbols present in data\n",
        "    present_symbols = set(df[\"Symbol\"].unique())\n",
        "    required_set = set(required_symbols)\n",
        "\n",
        "    # Check for missing symbols\n",
        "    missing_symbols = required_set - present_symbols\n",
        "    if missing_symbols:\n",
        "        raise ValueError(f\"Missing required symbols in market data: {missing_symbols}\")\n",
        "\n",
        "    # Count rows per symbol\n",
        "    symbol_counts = df[\"Symbol\"].value_counts().to_dict()\n",
        "\n",
        "    # Check for sufficiency (heuristic: ~300 rows for a year)\n",
        "    for sym in required_symbols:\n",
        "        count = symbol_counts.get(sym, 0)\n",
        "        if count < 300:\n",
        "            print(f\"WARNING: Symbol '{sym}' has low row count ({count}). Expected ~365 for daily data.\")\n",
        "\n",
        "    return symbol_counts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Validate Temporal Coverage\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_temporal_coverage(\n",
        "    df: pd.DataFrame,\n",
        "    required_symbols: List[str],\n",
        "    start_date: pd.Timestamp,\n",
        "    end_date: pd.Timestamp\n",
        ") -> Tuple[Dict[str, List[pd.Timestamp]], Dict[str, Tuple[pd.Timestamp, pd.Timestamp]]]:\n",
        "    \"\"\"\n",
        "    Checks temporal coverage for each symbol against the observation window.\n",
        "\n",
        "    Identifies missing dates within the [start_date, end_date] range.\n",
        "    Computes the actual min/max dates for each symbol.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The validated DataFrame.\n",
        "        required_symbols (List[str]): List of symbols to check.\n",
        "        start_date (pd.Timestamp): Start of observation window (UTC).\n",
        "        end_date (pd.Timestamp): End of observation window (UTC).\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "        - missing_dates: Dict mapping symbol to list of missing timestamps.\n",
        "        - date_ranges: Dict mapping symbol to (min_date, max_date) tuple.\n",
        "    \"\"\"\n",
        "    # Generate expected daily date range (inclusive)\n",
        "    # Normalize start/end to midnight UTC for comparison if they aren't already\n",
        "    expected_range = pd.date_range(start=start_date, end=end_date, freq='D', tz='UTC').normalize()\n",
        "    expected_set = set(expected_range)\n",
        "\n",
        "    missing_dates = {}\n",
        "    date_ranges = {}\n",
        "\n",
        "    for sym in required_symbols:\n",
        "        # Filter data for this symbol\n",
        "        sym_df = df[df[\"Symbol\"] == sym]\n",
        "\n",
        "        # Get actual dates present (normalized to midnight for set comparison)\n",
        "        actual_dates = pd.to_datetime(sym_df[\"Date\"]).dt.normalize()\n",
        "        actual_set = set(actual_dates)\n",
        "\n",
        "        # Find missing dates\n",
        "        missing = sorted(list(expected_set - actual_set))\n",
        "        missing_dates[sym] = missing\n",
        "\n",
        "        # Record actual range\n",
        "        if not actual_dates.empty:\n",
        "            min_date = actual_dates.min()\n",
        "            max_date = actual_dates.max()\n",
        "            date_ranges[sym] = (min_date, max_date)\n",
        "\n",
        "            # Check bounds\n",
        "            if min_date > start_date:\n",
        "                print(f\"WARNING: {sym} starts late ({min_date.date()} > {start_date.date()})\")\n",
        "            if max_date < end_date:\n",
        "                print(f\"WARNING: {sym} ends early ({max_date.date()} < {end_date.date()})\")\n",
        "        else:\n",
        "            date_ranges[sym] = (pd.NaT, pd.NaT)\n",
        "\n",
        "    return missing_dates, date_ranges\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_df_market_raw(df_raw: pd.DataFrame, schemas: Any, meta_config: Any) -> MarketValidationResult:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the raw market DataFrame.\n",
        "\n",
        "    Executes:\n",
        "    1. Column presence and type enforcement.\n",
        "    2. Symbol coverage verification.\n",
        "    3. Temporal coverage and missing date identification.\n",
        "\n",
        "    Args:\n",
        "        df_raw (pd.DataFrame): The raw market DataFrame.\n",
        "        schemas (ValidatedSchemas): Schema configuration from Task 1.\n",
        "        meta_config (ValidatedMetaConfig): Metadata configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        MarketValidationResult: Validated data and coverage statistics.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If validation fails.\n",
        "    \"\"\"\n",
        "    print(\"Task 3: Validating market columns and types...\")\n",
        "    # Step 1: Column/Type Validation\n",
        "    df_validated = validate_market_columns(df_raw, schemas.market_columns)\n",
        "\n",
        "    print(\"Task 3: Validating symbol coverage...\")\n",
        "    # Step 2: Symbol Coverage\n",
        "    symbol_counts = validate_symbol_coverage(df_validated, schemas.market_symbols)\n",
        "\n",
        "    print(\"Task 3: Validating temporal coverage...\")\n",
        "    # Step 3: Temporal Coverage\n",
        "    # Ensure start/end dates are Timestamp objects (Task 1 returns datetime, pandas handles conversion)\n",
        "    start_ts = pd.Timestamp(meta_config.start_date)\n",
        "    end_ts = pd.Timestamp(meta_config.end_date)\n",
        "\n",
        "    missing_dates, date_ranges = validate_temporal_coverage(\n",
        "        df_validated,\n",
        "        schemas.market_symbols,\n",
        "        start_ts,\n",
        "        end_ts\n",
        "    )\n",
        "\n",
        "    total_rows = len(df_validated)\n",
        "    print(f\"Task 3 Complete. Total Rows: {total_rows}\")\n",
        "    for sym, count in symbol_counts.items():\n",
        "        missing_count = len(missing_dates[sym])\n",
        "        print(f\"  - {sym}: {count} rows, {missing_count} missing dates in window.\")\n",
        "\n",
        "    return MarketValidationResult(\n",
        "        validated_df=df_validated,\n",
        "        symbol_counts=symbol_counts,\n",
        "        missing_dates=missing_dates,\n",
        "        date_ranges=date_ranges,\n",
        "        total_rows=total_rows\n",
        "    )\n"
      ],
      "metadata": {
        "id": "94RoatX_wRLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Filter df_chain_raw by Time Window and Transaction Status\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Filter df_chain_raw by Time Window and Transaction Status\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class FilteredChainData:\n",
        "    \"\"\"\n",
        "    Container for the results of the on-chain data filtering process.\n",
        "\n",
        "    Attributes:\n",
        "        filtered_df (pd.DataFrame): The DataFrame after temporal and status filtering.\n",
        "        initial_rows (int): Row count before any filtering.\n",
        "        rows_after_time_filter (int): Row count after applying the observation window.\n",
        "        rows_after_status_filter (int): Row count after removing failed transactions.\n",
        "        dropped_time_count (int): Number of rows dropped due to time window.\n",
        "        dropped_status_count (int): Number of rows dropped due to transaction status.\n",
        "    \"\"\"\n",
        "    filtered_df: pd.DataFrame\n",
        "    initial_rows: int\n",
        "    rows_after_time_filter: int\n",
        "    rows_after_status_filter: int\n",
        "    dropped_time_count: int\n",
        "    dropped_status_count: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Convert Timestamps to UTC Dates\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def convert_timestamp_to_utc_date(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converts the Unix epoch 'timeStamp' column to a normalized UTC datetime column 'txDate'.\n",
        "\n",
        "    The resulting 'txDate' column will have a dtype of datetime64[ns, UTC] with\n",
        "    the time component normalized to midnight (00:00:00).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The validated on-chain DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the DataFrame with the new 'txDate' column.\n",
        "    \"\"\"\n",
        "    # Work on a copy to avoid side effects\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Convert int64 timestamp to UTC datetime\n",
        "    # unit='s' assumes standard Unix epoch seconds\n",
        "    df_out[\"txDate\"] = pd.to_datetime(df_out[\"timeStamp\"], unit='s', utc=True)\n",
        "\n",
        "    # Normalize to midnight to facilitate daily grouping later\n",
        "    # This keeps the dtype as datetime64[ns, UTC]\n",
        "    df_out[\"txDate\"] = df_out[\"txDate\"].dt.normalize()\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Apply Temporal Filter\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_temporal_filter(\n",
        "    df: pd.DataFrame,\n",
        "    start_date: pd.Timestamp,\n",
        "    end_date: pd.Timestamp\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the DataFrame to retain only rows within the observation window [start_date, end_date].\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the 'txDate' column.\n",
        "        start_date (pd.Timestamp): The start of the observation window (UTC).\n",
        "        end_date (pd.Timestamp): The end of the observation window (UTC).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A subset of the input DataFrame.\n",
        "    \"\"\"\n",
        "    # Ensure start/end are normalized to midnight for accurate comparison\n",
        "    start_norm = start_date.normalize()\n",
        "    end_norm = end_date.normalize()\n",
        "\n",
        "    # Create boolean mask\n",
        "    # Inclusive boundaries: start_date <= txDate <= end_date\n",
        "    mask = (df[\"txDate\"] >= start_norm) & (df[\"txDate\"] <= end_norm)\n",
        "\n",
        "    # Return filtered copy\n",
        "    return df.loc[mask].copy()\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Remove Failed Transactions\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def filter_transaction_status(\n",
        "    df: pd.DataFrame,\n",
        "    status_col: str,\n",
        "    accepted_codes: List[int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the DataFrame to retain only successful transactions.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to filter.\n",
        "        status_col (str): The name of the status column (e.g., 'txStatus').\n",
        "        accepted_codes (List[int]): List of status codes indicating success (e.g., [0]).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A subset of the input DataFrame containing only accepted transactions.\n",
        "    \"\"\"\n",
        "    # Check if column exists\n",
        "    if status_col not in df.columns:\n",
        "        print(f\"WARNING: Status column '{status_col}' not found. Skipping status filter.\")\n",
        "        return df\n",
        "\n",
        "    # Create boolean mask\n",
        "    # We use isin() to support multiple accepted codes if necessary\n",
        "    mask = df[status_col].isin(accepted_codes)\n",
        "\n",
        "    # Return filtered copy\n",
        "    return df.loc[mask].copy()\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def filter_chain_data(\n",
        "    df_validated: pd.DataFrame,\n",
        "    meta_config: Any,\n",
        "    preprocessing_config: Any\n",
        ") -> FilteredChainData:\n",
        "    \"\"\"\n",
        "    Orchestrates the temporal and status filtering of the on-chain data.\n",
        "\n",
        "    Executes:\n",
        "    1. Conversion of timestamps to UTC dates.\n",
        "    2. Filtering rows outside the observation window.\n",
        "    3. Filtering rows with failed transaction status.\n",
        "\n",
        "    Args:\n",
        "        df_validated (pd.DataFrame): The validated on-chain DataFrame from Task 2.\n",
        "        meta_config (ValidatedMetaConfig): Metadata configuration from Task 1.\n",
        "        preprocessing_config (Dict): The 'preprocessing' section of the raw config.\n",
        "\n",
        "    Returns:\n",
        "        FilteredChainData: Result object containing the filtered DataFrame and counts.\n",
        "    \"\"\"\n",
        "    initial_rows = len(df_validated)\n",
        "    print(f\"Task 4: Starting filtering on {initial_rows} rows...\")\n",
        "\n",
        "    # Step 1: Date Conversion\n",
        "    print(\"Task 4: Converting timestamps to UTC dates...\")\n",
        "    df_dated = convert_timestamp_to_utc_date(df_validated)\n",
        "\n",
        "    # Step 2: Temporal Filter\n",
        "    print(f\"Task 4: Applying temporal filter ({meta_config.start_date.date()} to {meta_config.end_date.date()})...\")\n",
        "    # Ensure we use the Timestamp objects from the validated config\n",
        "    df_time_filtered = apply_temporal_filter(\n",
        "        df_dated,\n",
        "        meta_config.start_date,\n",
        "        meta_config.end_date\n",
        "    )\n",
        "\n",
        "    rows_after_time = len(df_time_filtered)\n",
        "    dropped_time = initial_rows - rows_after_time\n",
        "    print(f\"  - Dropped {dropped_time} rows outside observation window.\")\n",
        "\n",
        "    # Step 3: Status Filter\n",
        "    print(\"Task 4: Applying transaction status filter...\")\n",
        "    handling_config = preprocessing_config.get(\"handling\", {})\n",
        "\n",
        "    if handling_config.get(\"drop_failed_tx\", True):\n",
        "        status_col = handling_config.get(\"tx_status_column\", \"txStatus\")\n",
        "        accepted_codes = handling_config.get(\"accepted_status\", [0])\n",
        "\n",
        "        df_final = filter_transaction_status(\n",
        "            df_time_filtered,\n",
        "            status_col,\n",
        "            accepted_codes\n",
        "        )\n",
        "\n",
        "        rows_after_status = len(df_final)\n",
        "        dropped_status = rows_after_time - rows_after_status\n",
        "        print(f\"  - Dropped {dropped_status} failed transactions.\")\n",
        "    else:\n",
        "        print(\"  - Skipping status filter (drop_failed_tx=False).\")\n",
        "        df_final = df_time_filtered\n",
        "        rows_after_status = rows_after_time\n",
        "        dropped_status = 0\n",
        "\n",
        "    print(f\"Task 4 Complete. Final Rows: {rows_after_status}\")\n",
        "\n",
        "    return FilteredChainData(\n",
        "        filtered_df=df_final,\n",
        "        initial_rows=initial_rows,\n",
        "        rows_after_time_filter=rows_after_time,\n",
        "        rows_after_status_filter=rows_after_status,\n",
        "        dropped_time_count=dropped_time,\n",
        "        dropped_status_count=dropped_status\n",
        "    )\n"
      ],
      "metadata": {
        "id": "SWWguHtZxPAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Filter df_chain_raw by Token and Normalize Values\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Filter df_chain_raw by Token and Normalize Values\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class NormalizedChainData:\n",
        "    \"\"\"\n",
        "    Container for the results of the token filtering and value normalization process.\n",
        "\n",
        "    Attributes:\n",
        "        normalized_df (pd.DataFrame): The DataFrame containing only USDT/USDC transfers with normalized USD values.\n",
        "        rows_after_token_filter (int): Row count after keeping only target tokens.\n",
        "        rows_after_zero_filter (int): Row count after removing zero-value transfers.\n",
        "        dropped_token_count (int): Number of rows dropped because they were not USDT/USDC.\n",
        "        dropped_zero_value_count (int): Number of rows dropped due to zero or negative value.\n",
        "    \"\"\"\n",
        "    normalized_df: pd.DataFrame\n",
        "    rows_after_token_filter: int\n",
        "    rows_after_zero_filter: int\n",
        "    dropped_token_count: int\n",
        "    dropped_zero_value_count: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Apply Token Address Filter\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def filter_by_token_address(df: pd.DataFrame, assets_config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the DataFrame to retain only USDT and USDC transfers and assigns a 'token' label.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The filtered on-chain DataFrame from Task 4.\n",
        "        assets_config (Dict[str, Any]): The 'assets' dictionary from the schema config.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A subset of the input DataFrame with a new 'token' column.\n",
        "    \"\"\"\n",
        "    # Extract canonical addresses and normalize to lowercase\n",
        "    usdt_addr = assets_config[\"USDT\"][\"address\"].lower()\n",
        "    usdc_addr = assets_config[\"USDC\"][\"address\"].lower()\n",
        "\n",
        "    # Normalize DataFrame addresses to lowercase for comparison\n",
        "    # We use a temporary series to avoid modifying the original if not desired,\n",
        "    # but here we are creating a new filtered DF anyway.\n",
        "    addr_series = df[\"tokenAddress\"].str.lower()\n",
        "\n",
        "    # Create masks\n",
        "    is_usdt = addr_series == usdt_addr\n",
        "    is_usdc = addr_series == usdc_addr\n",
        "\n",
        "    # Combine masks\n",
        "    mask = is_usdt | is_usdc\n",
        "\n",
        "    # Filter DataFrame\n",
        "    df_out = df.loc[mask].copy()\n",
        "\n",
        "    # Assign 'token' label\n",
        "    # Initialize with empty string or NaN, then fill\n",
        "    df_out[\"token\"] = np.nan\n",
        "    df_out.loc[is_usdt[mask].index, \"token\"] = \"USDT\"\n",
        "    df_out.loc[is_usdc[mask].index, \"token\"] = \"USDC\"\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Cast and Normalize the `value` Field\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_token_values(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converts the raw 'value' string to a normalized float 'usd_value'.\n",
        "\n",
        "    Implements Equation (1): USD Value = value / 10^6.\n",
        "    Assumes both USDT and USDC have 6 decimals (verified in Task 1).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with 'token' and 'value' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with a new 'usd_value' column.\n",
        "    \"\"\"\n",
        "    # Work on a copy\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # 1. Convert object/string 'value' to Python integer (arbitrary precision)\n",
        "    # We use a lambda to handle potential non-numeric strings gracefully if any slipped through,\n",
        "    # though Task 2 validation should have caught them.\n",
        "    # Using pd.to_numeric with downcast='integer' might overflow for uint256,\n",
        "    # so we stick to python int for the intermediate step or direct float conversion if precision allows.\n",
        "    # However, direct float conversion of uint256 string loses precision for very large numbers immediately.\n",
        "    # Best approach: Convert to python int, then divide.\n",
        "\n",
        "    try:\n",
        "        # Vectorized conversion to numeric (float64) directly is risky for precision if values > 2^53.\n",
        "        # But we are dividing by 10^6 immediately.\n",
        "        # Let's use a safe apply approach for correctness over raw speed here,\n",
        "        # as 'value' is an object column.\n",
        "        int_values = df_out[\"value\"].apply(int)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(f\"Failed to convert 'value' column to integers: {e}\")\n",
        "\n",
        "    # 2. Normalize\n",
        "    # Both tokens have 6 decimals.\n",
        "    # Division by 10^6 converts to float64.\n",
        "    df_out[\"usd_value\"] = int_values / 1_000_000.0\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Remove Zero-Value Transfers\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def remove_zero_value_transfers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes rows where the normalized USD value is zero or negative.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with 'usd_value'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A subset of the input DataFrame.\n",
        "    \"\"\"\n",
        "    # Filter condition: usd_value > 0\n",
        "    # This implicitly removes 0 and any negative values (data errors)\n",
        "    mask = df[\"usd_value\"] > 0\n",
        "\n",
        "    # Check for negative values to log a warning\n",
        "    neg_count = (df[\"usd_value\"] < 0).sum()\n",
        "    if neg_count > 0:\n",
        "        print(f\"WARNING: Found {neg_count} negative value transactions. Removing them.\")\n",
        "\n",
        "    return df.loc[mask].copy()\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_chain_data(\n",
        "    filtered_data: Any,\n",
        "    schemas: Any,\n",
        "    preprocessing_config: Any\n",
        ") -> NormalizedChainData:\n",
        "    \"\"\"\n",
        "    Orchestrates the token filtering and value normalization.\n",
        "\n",
        "    Executes:\n",
        "    1. Filtering for USDT/USDC addresses.\n",
        "    2. Normalization of raw values to USD units.\n",
        "    3. Removal of zero-value transfers.\n",
        "\n",
        "    Args:\n",
        "        filtered_data (FilteredChainData): Result object from Task 4.\n",
        "        schemas (ValidatedSchemas): Schema configuration from Task 1.\n",
        "        preprocessing_config (Dict): Preprocessing configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        NormalizedChainData: Result object containing the normalized DataFrame and counts.\n",
        "    \"\"\"\n",
        "    df_input = filtered_data.filtered_df\n",
        "    initial_rows = len(df_input)\n",
        "    print(f\"Task 5: Starting normalization on {initial_rows} rows...\")\n",
        "\n",
        "    # Step 1: Token Filter\n",
        "    print(\"Task 5: Filtering for USDT and USDC...\")\n",
        "    df_token_filtered = filter_by_token_address(df_input, schemas.assets)\n",
        "\n",
        "    rows_after_token = len(df_token_filtered)\n",
        "    dropped_token = initial_rows - rows_after_token\n",
        "    print(f\"  - Dropped {dropped_token} rows (non-target tokens).\")\n",
        "\n",
        "    # Step 2: Value Normalization\n",
        "    print(\"Task 5: Normalizing raw values (dividing by 10^6)...\")\n",
        "    df_normalized = normalize_token_values(df_token_filtered)\n",
        "\n",
        "    # Step 3: Zero Value Filter\n",
        "    print(\"Task 5: Removing zero-value transfers...\")\n",
        "    handling_config = preprocessing_config.get(\"handling\", {})\n",
        "\n",
        "    if handling_config.get(\"drop_zero_value\", True):\n",
        "        df_final = remove_zero_value_transfers(df_normalized)\n",
        "        rows_after_zero = len(df_final)\n",
        "        dropped_zero = rows_after_token - rows_after_zero\n",
        "        print(f\"  - Dropped {dropped_zero} zero-value rows.\")\n",
        "    else:\n",
        "        print(\"  - Skipping zero-value filter (drop_zero_value=False).\")\n",
        "        df_final = df_normalized\n",
        "        rows_after_zero = rows_after_token\n",
        "        dropped_zero = 0\n",
        "\n",
        "    print(f\"Task 5 Complete. Final Rows: {rows_after_zero}\")\n",
        "\n",
        "    return NormalizedChainData(\n",
        "        normalized_df=df_final,\n",
        "        rows_after_token_filter=rows_after_token,\n",
        "        rows_after_zero_filter=rows_after_zero,\n",
        "        dropped_token_count=dropped_token,\n",
        "        dropped_zero_value_count=dropped_zero\n",
        "    )\n"
      ],
      "metadata": {
        "id": "3oQ7EqWix_YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — Deduplicate df_chain_raw\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Deduplicate df_chain_raw\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class DeduplicatedChainData:\n",
        "    \"\"\"\n",
        "    Container for the results of the deduplication process.\n",
        "\n",
        "    Attributes:\n",
        "        deduplicated_df (pd.DataFrame): The DataFrame with duplicate logs removed.\n",
        "        rows_before (int): Row count before deduplication.\n",
        "        rows_after (int): Row count after deduplication.\n",
        "        duplicates_removed (int): Number of duplicate rows removed.\n",
        "        internal_tx_count (Optional[int]): Number of internal transactions (if flag present).\n",
        "        top_level_tx_count (Optional[int]): Number of top-level transactions (if flag present).\n",
        "    \"\"\"\n",
        "    deduplicated_df: pd.DataFrame\n",
        "    rows_before: int\n",
        "    rows_after: int\n",
        "    duplicates_removed: int\n",
        "    internal_tx_count: Optional[int]\n",
        "    top_level_tx_count: Optional[int]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Define Uniqueness Key\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def check_uniqueness_stats(df: pd.DataFrame) -> Tuple[int, int, int]:\n",
        "    \"\"\"\n",
        "    Calculates uniqueness statistics based on the composite primary key.\n",
        "\n",
        "    Key: (transactionHash, blockNumber, logIndex)\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The normalized DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[int, int, int]: (total_rows, unique_rows, duplicate_count)\n",
        "    \"\"\"\n",
        "    key_cols = [\"transactionHash\", \"blockNumber\", \"logIndex\"]\n",
        "\n",
        "    total_rows = len(df)\n",
        "\n",
        "    # Count unique combinations\n",
        "    # drop_duplicates returns the unique rows, so len() gives unique count\n",
        "    unique_rows = len(df.drop_duplicates(subset=key_cols))\n",
        "\n",
        "    duplicate_count = total_rows - unique_rows\n",
        "\n",
        "    return total_rows, unique_rows, duplicate_count\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Remove Duplicate Rows\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes duplicate rows based on the composite primary key.\n",
        "\n",
        "    Sorts by blockNumber and logIndex to ensure deterministic retention of the 'first' record.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing duplicates.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A deduplicated DataFrame with a reset index.\n",
        "    \"\"\"\n",
        "    key_cols = [\"transactionHash\", \"blockNumber\", \"logIndex\"]\n",
        "\n",
        "    # Sort for deterministic behavior\n",
        "    # We assume that if duplicates exist, they are identical.\n",
        "    # If they differ in other fields, sorting ensures we pick a consistent one.\n",
        "    df_sorted = df.sort_values(by=[\"blockNumber\", \"logIndex\"], ascending=True)\n",
        "\n",
        "    # Drop duplicates, keeping the first occurrence\n",
        "    df_deduped = df_sorted.drop_duplicates(subset=key_cols, keep=\"first\")\n",
        "\n",
        "    # Reset index for cleanliness\n",
        "    df_final = df_deduped.reset_index(drop=True)\n",
        "\n",
        "    return df_final\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Handle `isInternal` Flag (If Present)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_internal_transactions(df: pd.DataFrame) -> Tuple[Optional[int], Optional[int]]:\n",
        "    \"\"\"\n",
        "    Analyzes the distribution of internal vs. top-level transactions if the flag exists.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The deduplicated DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[int], Optional[int]]: (internal_count, top_level_count).\n",
        "        Returns (None, None) if 'isInternal' column is missing.\n",
        "    \"\"\"\n",
        "    if \"isInternal\" in df.columns:\n",
        "        # Assuming 1 = Internal, 0 = Top-level (or boolean)\n",
        "        # We cast to int to be safe if it's boolean\n",
        "        try:\n",
        "            is_internal = df[\"isInternal\"].astype(int)\n",
        "            internal_count = (is_internal == 1).sum()\n",
        "            top_level_count = (is_internal == 0).sum()\n",
        "            return internal_count, top_level_count\n",
        "        except Exception:\n",
        "            print(\"WARNING: Could not cast 'isInternal' to integer for analysis.\")\n",
        "            return None, None\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def deduplicate_chain_data(normalized_data: Any) -> DeduplicatedChainData:\n",
        "    \"\"\"\n",
        "    Orchestrates the deduplication of the on-chain data.\n",
        "\n",
        "    Executes:\n",
        "    1. Calculation of duplication statistics.\n",
        "    2. Removal of duplicate logs based on unique key.\n",
        "    3. Analysis of internal transaction flags (if available).\n",
        "\n",
        "    Args:\n",
        "        normalized_data (NormalizedChainData): Result object from Task 5.\n",
        "\n",
        "    Returns:\n",
        "        DeduplicatedChainData: Result object containing the clean DataFrame and stats.\n",
        "    \"\"\"\n",
        "    df_input = normalized_data.normalized_df\n",
        "    print(\"Task 6: Checking for duplicates...\")\n",
        "\n",
        "    # Step 1: Check Stats\n",
        "    total, unique, dups = check_uniqueness_stats(df_input)\n",
        "\n",
        "    if dups > 0:\n",
        "        print(f\"  - Found {dups} duplicate rows ({dups/total:.2%}). Removing...\")\n",
        "        # Step 2: Remove Duplicates\n",
        "        df_deduped = remove_duplicates(df_input)\n",
        "    else:\n",
        "        print(\"  - No duplicates found.\")\n",
        "        df_deduped = df_input.copy().reset_index(drop=True)\n",
        "\n",
        "    # Verify count\n",
        "    rows_after = len(df_deduped)\n",
        "    if rows_after != unique:\n",
        "        print(f\"WARNING: Post-deduplication count ({rows_after}) does not match expected unique count ({unique}).\")\n",
        "\n",
        "    # Step 3: Internal Tx Analysis\n",
        "    print(\"Task 6: Analyzing internal transaction flags...\")\n",
        "    internal_count, top_level_count = analyze_internal_transactions(df_deduped)\n",
        "\n",
        "    if internal_count is not None:\n",
        "        print(f\"  - Internal Tx: {internal_count}, Top-Level Tx: {top_level_count}\")\n",
        "    else:\n",
        "        print(\"  - 'isInternal' flag not present. Assuming finalized log data.\")\n",
        "\n",
        "    print(f\"Task 6 Complete. Final Rows: {rows_after}\")\n",
        "\n",
        "    return DeduplicatedChainData(\n",
        "        deduplicated_df=df_deduped,\n",
        "        rows_before=total,\n",
        "        rows_after=rows_after,\n",
        "        duplicates_removed=dups,\n",
        "        internal_tx_count=internal_count,\n",
        "        top_level_tx_count=top_level_count\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Ctt6DMWwzCRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Classify Transactions by Behavioral Topology\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Classify Transactions by Behavioral Topology\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ClassifiedChainData:\n",
        "    \"\"\"\n",
        "    Container for the results of the behavioral topology classification.\n",
        "\n",
        "    Attributes:\n",
        "        classified_df (pd.DataFrame): The DataFrame with the new 'topology' column.\n",
        "        count_eoa_eoa (int): Number of human-driven transactions.\n",
        "        count_sc_sc (int): Number of algorithmic transactions.\n",
        "        count_other (int): Number of other transactions (mixed topology).\n",
        "        total_rows (int): Total number of rows processed.\n",
        "    \"\"\"\n",
        "    classified_df: pd.DataFrame\n",
        "    count_eoa_eoa: int\n",
        "    count_sc_sc: int\n",
        "    count_other: int\n",
        "    total_rows: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1 & 2: Define Rules and Apply Classification\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_topology_classification(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Classifies transactions into behavioral topologies based on sender/receiver contract status.\n",
        "\n",
        "    Rules:\n",
        "    - EOA-EOA (Human): fromIsContract == 0 AND toIsContract == 0\n",
        "    - SC-SC (Algo): fromIsContract == 1 AND toIsContract == 1\n",
        "    - OTHER: All other combinations\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The deduplicated on-chain DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the DataFrame with a new 'topology' column (categorical).\n",
        "    \"\"\"\n",
        "    # Work on a copy\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Define conditions\n",
        "    # We assume columns are int8/int64 as validated in Task 2\n",
        "    cond_human = (df_out[\"fromIsContract\"] == 0) & (df_out[\"toIsContract\"] == 0)\n",
        "    cond_algo = (df_out[\"fromIsContract\"] == 1) & (df_out[\"toIsContract\"] == 1)\n",
        "\n",
        "    # Define choices corresponding to conditions\n",
        "    choices = [\"EOA-EOA\", \"SC-SC\"]\n",
        "\n",
        "    # Apply vectorized selection\n",
        "    # default=\"OTHER\" covers (0,1) and (1,0) cases\n",
        "    df_out[\"topology\"] = np.select([cond_human, cond_algo], choices, default=\"OTHER\")\n",
        "\n",
        "    # Convert to categorical for memory efficiency and performance\n",
        "    df_out[\"topology\"] = df_out[\"topology\"].astype(\"category\")\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Validate Classification Integrity\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_topology_counts(df: pd.DataFrame) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Validates the integrity of the topology classification.\n",
        "\n",
        "    Checks that all rows are classified and counts sum to total.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The classified DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: Dictionary of counts per topology.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If classification resulted in nulls or count mismatch.\n",
        "    \"\"\"\n",
        "    # Check for nulls\n",
        "    if df[\"topology\"].isnull().any():\n",
        "        raise ValueError(\"Topology classification resulted in null values.\")\n",
        "\n",
        "    # Get value counts\n",
        "    counts = df[\"topology\"].value_counts().to_dict()\n",
        "\n",
        "    # Ensure all keys exist in dict even if 0\n",
        "    for key in [\"EOA-EOA\", \"SC-SC\", \"OTHER\"]:\n",
        "        if key not in counts:\n",
        "            counts[key] = 0\n",
        "\n",
        "    # Verify sum\n",
        "    total_classified = sum(counts.values())\n",
        "    total_rows = len(df)\n",
        "\n",
        "    if total_classified != total_rows:\n",
        "        raise ValueError(f\"Mismatch in classification counts: {total_classified} vs {total_rows}\")\n",
        "\n",
        "    return counts\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def classify_chain_topology(deduplicated_data: Any) -> ClassifiedChainData:\n",
        "    \"\"\"\n",
        "    Orchestrates the classification of transactions into behavioral topologies.\n",
        "\n",
        "    Executes:\n",
        "    1. Application of classification rules (EOA-EOA vs SC-SC).\n",
        "    2. Validation of classification integrity.\n",
        "\n",
        "    Args:\n",
        "        deduplicated_data (DeduplicatedChainData): Result object from Task 6.\n",
        "\n",
        "    Returns:\n",
        "        ClassifiedChainData: Result object containing the classified DataFrame and stats.\n",
        "    \"\"\"\n",
        "    df_input = deduplicated_data.deduplicated_df\n",
        "    print(\"Task 7: Classifying behavioral topology...\")\n",
        "\n",
        "    # Step 1 & 2: Apply Classification\n",
        "    df_classified = apply_topology_classification(df_input)\n",
        "\n",
        "    # Step 3: Validate\n",
        "    counts = validate_topology_counts(df_classified)\n",
        "\n",
        "    count_eoa = counts[\"EOA-EOA\"]\n",
        "    count_sc = counts[\"SC-SC\"]\n",
        "    count_other = counts[\"OTHER\"]\n",
        "    total = len(df_classified)\n",
        "\n",
        "    print(f\"Task 7 Complete. Total Rows: {total}\")\n",
        "    print(f\"  - EOA-EOA (Human): {count_eoa} ({count_eoa/total:.1%})\")\n",
        "    print(f\"  - SC-SC (Algo):    {count_sc} ({count_sc/total:.1%})\")\n",
        "    print(f\"  - OTHER:           {count_other} ({count_other/total:.1%})\")\n",
        "\n",
        "    return ClassifiedChainData(\n",
        "        classified_df=df_classified,\n",
        "        count_eoa_eoa=count_eoa,\n",
        "        count_sc_sc=count_sc,\n",
        "        count_other=count_other,\n",
        "        total_rows=total\n",
        "    )\n"
      ],
      "metadata": {
        "id": "MFN3W_Gxz4aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — Aggregate Daily Volumes by Token and Topology\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Aggregate Daily Volumes by Token and Topology\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class AggregatedChainData:\n",
        "    \"\"\"\n",
        "    Container for the aggregated daily on-chain volume series.\n",
        "\n",
        "    Attributes:\n",
        "        daily_df (pd.DataFrame): DataFrame indexed by Date with 4 volume columns.\n",
        "        start_date (pd.Timestamp): Start of the aggregation window.\n",
        "        end_date (pd.Timestamp): End of the aggregation window.\n",
        "        total_days (int): Number of days in the sequence.\n",
        "    \"\"\"\n",
        "    daily_df: pd.DataFrame\n",
        "    start_date: pd.Timestamp\n",
        "    end_date: pd.Timestamp\n",
        "    total_days: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Define Aggregation Groups\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def group_and_sum_volumes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Groups transactions by Date, Token, and Topology and sums the USD value.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The classified DataFrame with 'txDate', 'token', 'topology', 'usd_value'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a MultiIndex (txDate, token, topology) and summed 'usd_value'.\n",
        "    \"\"\"\n",
        "    # Group by the three dimensions\n",
        "    # observed=True ensures we only get combinations that exist in data initially\n",
        "    grouped = df.groupby([\"txDate\", \"token\", \"topology\"], observed=True)[\"usd_value\"].sum()\n",
        "\n",
        "    return grouped.to_frame(name=\"volume\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Pivot to Wide Format\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def pivot_to_wide_format(grouped_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Pivots the grouped data into a wide format with specific column names.\n",
        "\n",
        "    Target Columns:\n",
        "    - V_EOA_EOA_USDT\n",
        "    - V_EOA_EOA_USDC\n",
        "    - V_SC_SC_USDT\n",
        "    - V_SC_SC_USDC\n",
        "\n",
        "    Args:\n",
        "        grouped_df (pd.DataFrame): The grouped DataFrame from Step 1.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A wide DataFrame indexed by Date.\n",
        "    \"\"\"\n",
        "    # Reset index to make columns accessible for pivoting\n",
        "    df_flat = grouped_df.reset_index()\n",
        "\n",
        "    # Filter for only the topologies we care about (EOA-EOA, SC-SC)\n",
        "    # We ignore \"OTHER\" for the specific time series construction as per instructions\n",
        "    target_topologies = [\"EOA-EOA\", \"SC-SC\"]\n",
        "    df_filtered = df_flat[df_flat[\"topology\"].isin(target_topologies)].copy()\n",
        "\n",
        "    # Create a composite column name for pivoting\n",
        "    # Format: V_{TOPOLOGY}_{TOKEN}\n",
        "    # We replace hyphens in topology with underscores for variable-name friendliness if needed,\n",
        "    # but the paper uses EOA-EOA. Let's stick to the requested output format: V_EOA_EOA_USDT.\n",
        "    # So we replace '-' with '_' in topology.\n",
        "\n",
        "    # Ensure topology is string for manipulation\n",
        "    topo_str = df_filtered[\"topology\"].astype(str).str.replace(\"-\", \"_\")\n",
        "    token_str = df_filtered[\"token\"].astype(str)\n",
        "\n",
        "    df_filtered[\"col_name\"] = \"V_\" + topo_str + \"_\" + token_str\n",
        "\n",
        "    # Pivot\n",
        "    # Index: txDate\n",
        "    # Columns: col_name\n",
        "    # Values: volume\n",
        "    wide_df = df_filtered.pivot(index=\"txDate\", columns=\"col_name\", values=\"volume\")\n",
        "\n",
        "    # Fill NaNs with 0 (days where a specific category had no volume)\n",
        "    wide_df = wide_df.fillna(0.0)\n",
        "\n",
        "    return wide_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Fill Missing Dates\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def reindex_to_full_window(\n",
        "    df: pd.DataFrame,\n",
        "    start_date: pd.Timestamp,\n",
        "    end_date: pd.Timestamp\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reindexes the DataFrame to ensure a complete daily sequence from start_date to end_date.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The wide DataFrame.\n",
        "        start_date (pd.Timestamp): Start of window (UTC).\n",
        "        end_date (pd.Timestamp): End of window (UTC).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The reindexed DataFrame with 0-filling for missing days.\n",
        "    \"\"\"\n",
        "    # Generate full date range\n",
        "    full_idx = pd.date_range(start=start_date, end=end_date, freq='D', tz='UTC').normalize()\n",
        "\n",
        "    # Reindex\n",
        "    # fill_value=0.0 ensures missing days get 0 volume\n",
        "    df_reindexed = df.reindex(full_idx, fill_value=0.0)\n",
        "\n",
        "    # Rename index to 'Date' for consistency with market data\n",
        "    df_reindexed.index.name = \"Date\"\n",
        "\n",
        "    # Ensure all 4 expected columns exist, even if data was completely missing for one\n",
        "    expected_cols = [\n",
        "        \"V_EOA_EOA_USDT\", \"V_EOA_EOA_USDC\",\n",
        "        \"V_SC_SC_USDT\", \"V_SC_SC_USDC\"\n",
        "    ]\n",
        "    for col in expected_cols:\n",
        "        if col not in df_reindexed.columns:\n",
        "            df_reindexed[col] = 0.0\n",
        "\n",
        "    # Sort columns for consistent order\n",
        "    df_reindexed = df_reindexed[expected_cols]\n",
        "\n",
        "    return df_reindexed\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_chain_volumes(\n",
        "    classified_data: Any,\n",
        "    meta_config: Any\n",
        ") -> AggregatedChainData:\n",
        "    \"\"\"\n",
        "    Orchestrates the aggregation of daily on-chain volumes.\n",
        "\n",
        "    Executes:\n",
        "    1. Grouping and summing by date/token/topology.\n",
        "    2. Pivoting to wide format.\n",
        "    3. Reindexing to the full observation window.\n",
        "\n",
        "    Args:\n",
        "        classified_data (ClassifiedChainData): Result object from Task 7.\n",
        "        meta_config (ValidatedMetaConfig): Metadata configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        AggregatedChainData: Result object containing the daily time series.\n",
        "    \"\"\"\n",
        "    df_input = classified_data.classified_df\n",
        "    print(\"Task 8: Aggregating daily volumes...\")\n",
        "\n",
        "    # Step 1: Group and Sum\n",
        "    grouped_df = group_and_sum_volumes(df_input)\n",
        "\n",
        "    # Step 2: Pivot\n",
        "    wide_df = pivot_to_wide_format(grouped_df)\n",
        "\n",
        "    # Step 3: Reindex\n",
        "    # Ensure we use the validated timestamps\n",
        "    start_ts = pd.Timestamp(meta_config.start_date)\n",
        "    end_ts = pd.Timestamp(meta_config.end_date)\n",
        "\n",
        "    final_df = reindex_to_full_window(wide_df, start_ts, end_ts)\n",
        "\n",
        "    # Reset index to make Date a column, as requested in Task 8 Step 2 (\"Create a daily DataFrame with columns: Date...\")\n",
        "    # But keeping it as index is often easier for plotting/merging.\n",
        "    # The instructions say \"Create a daily DataFrame with columns: Date...\".\n",
        "    # Let's reset index to be strictly compliant with \"columns: Date\".\n",
        "    final_df_reset = final_df.reset_index()\n",
        "\n",
        "    total_days = len(final_df_reset)\n",
        "    print(f\"Task 8 Complete. Generated {total_days} daily observations.\")\n",
        "    print(f\"Columns: {list(final_df_reset.columns)}\")\n",
        "\n",
        "    return AggregatedChainData(\n",
        "        daily_df=final_df_reset,\n",
        "        start_date=start_ts,\n",
        "        end_date=end_ts,\n",
        "        total_days=total_days\n",
        "    )\n"
      ],
      "metadata": {
        "id": "mUFJtoPt0pkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Validate Daily On-Chain Series Quality\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Validate Daily On-Chain Series Quality\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ChainSeriesValidationResult:\n",
        "    \"\"\"\n",
        "    Container for the validation results of the daily on-chain time series.\n",
        "\n",
        "    Attributes:\n",
        "        is_valid (bool): True if all critical checks passed.\n",
        "        stats_df (pd.DataFrame): Descriptive statistics for each volume series.\n",
        "        outliers (Dict[str, List[pd.Timestamp]]): Dates where volume exceeded mean + 4 sigma.\n",
        "        zero_counts (Dict[str, int]): Number of days with zero volume per series.\n",
        "        plot_specifications (List[Dict[str, Any]]): Descriptions of required visualizations.\n",
        "    \"\"\"\n",
        "    is_valid: bool\n",
        "    stats_df: pd.DataFrame\n",
        "    outliers: Dict[str, List[pd.Timestamp]]\n",
        "    zero_counts: Dict[str, int]\n",
        "    plot_specifications: List[Dict[str, Any]]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Check for Non-Negativity and Missing Values\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def check_series_integrity(df: pd.DataFrame, columns: List[str]) -> bool:\n",
        "    \"\"\"\n",
        "    Validates that the specified columns contain no negative values and no NaNs.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The daily aggregated DataFrame.\n",
        "        columns (List[str]): List of volume column names to check.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid. Raises ValueError if invalid.\n",
        "    \"\"\"\n",
        "    for col in columns:\n",
        "        # Check for NaNs\n",
        "        if df[col].isnull().any():\n",
        "            raise ValueError(f\"Series '{col}' contains NaN values.\")\n",
        "\n",
        "        # Check for Negatives\n",
        "        if (df[col] < 0).any():\n",
        "            neg_count = (df[col] < 0).sum()\n",
        "            raise ValueError(f\"Series '{col}' contains {neg_count} negative values.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Compute Descriptive Statistics\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_series_statistics(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes descriptive statistics for the volume series.\n",
        "\n",
        "    Metrics: Mean, Median, Std, Min, Max, Skewness, Kurtosis.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The daily aggregated DataFrame.\n",
        "        columns (List[str]): List of volume column names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with statistics as rows and series as columns.\n",
        "    \"\"\"\n",
        "    stats_dict = {}\n",
        "\n",
        "    for col in columns:\n",
        "        series = df[col]\n",
        "        stats_dict[col] = {\n",
        "            \"mean\": series.mean(),\n",
        "            \"median\": series.median(),\n",
        "            \"std\": series.std(),\n",
        "            \"min\": series.min(),\n",
        "            \"max\": series.max(),\n",
        "            \"skew\": series.skew(),\n",
        "            \"kurtosis\": series.kurtosis()\n",
        "        }\n",
        "\n",
        "    return pd.DataFrame(stats_dict)\n",
        "\n",
        "def detect_outliers(df: pd.DataFrame, columns: List[str]) -> Dict[str, List[pd.Timestamp]]:\n",
        "    \"\"\"\n",
        "    Identifies dates where volume exceeds the mean + 4 sigma threshold.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The daily aggregated DataFrame.\n",
        "        columns (List[str]): List of volume column names.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, List[pd.Timestamp]]: Dictionary mapping series name to list of outlier dates.\n",
        "    \"\"\"\n",
        "    outliers = {}\n",
        "\n",
        "    for col in columns:\n",
        "        series = df[col]\n",
        "        mean = series.mean()\n",
        "        std = series.std()\n",
        "        threshold = mean + 4 * std\n",
        "\n",
        "        # Identify outliers\n",
        "        outlier_mask = series > threshold\n",
        "        outlier_dates = df.loc[outlier_mask, \"Date\"].tolist()\n",
        "        outliers[col] = outlier_dates\n",
        "\n",
        "    return outliers\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Visual Inspection Points (Specification Only)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_plot_specs(columns: List[str], meta_config: Any) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Generates specifications for required time-series visualizations.\n",
        "\n",
        "    Args:\n",
        "        columns (List[str]): List of volume column names.\n",
        "        meta_config (ValidatedMetaConfig): Metadata containing critical event dates.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: List of plot specification dictionaries.\n",
        "    \"\"\"\n",
        "    specs = []\n",
        "\n",
        "    # Extract event dates as strings for the spec\n",
        "    election_date = meta_config.election_day.date().isoformat()\n",
        "    human_signal_date = meta_config.human_signal_date.date().isoformat()\n",
        "\n",
        "    for col in columns:\n",
        "        spec = {\n",
        "            \"title\": f\"Daily Volume: {col}\",\n",
        "            \"x_axis\": \"Date\",\n",
        "            \"y_axis\": col,\n",
        "            \"annotations\": [\n",
        "                {\"date\": election_date, \"label\": \"Election Day\", \"color\": \"red\", \"style\": \"dashed\"},\n",
        "                {\"date\": human_signal_date, \"label\": \"Human Signal\", \"color\": \"blue\", \"style\": \"dotted\"}\n",
        "            ],\n",
        "            \"description\": f\"Time series plot of {col} with critical political events marked.\"\n",
        "        }\n",
        "        specs.append(spec)\n",
        "\n",
        "    return specs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_daily_chain_series(\n",
        "    aggregated_data: Any,\n",
        "    meta_config: Any\n",
        ") -> ChainSeriesValidationResult:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation and statistical analysis of the daily on-chain series.\n",
        "\n",
        "    Executes:\n",
        "    1. Integrity checks (non-negativity, no NaNs).\n",
        "    2. Computation of descriptive statistics and outlier detection.\n",
        "    3. Generation of visualization specifications.\n",
        "\n",
        "    Args:\n",
        "        aggregated_data (AggregatedChainData): Result object from Task 8.\n",
        "        meta_config (ValidatedMetaConfig): Metadata configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        ChainSeriesValidationResult: Result object containing stats and validation status.\n",
        "    \"\"\"\n",
        "    df = aggregated_data.daily_df\n",
        "\n",
        "    # Identify the 4 volume columns\n",
        "    # We look for columns starting with \"V_\"\n",
        "    vol_cols = [c for c in df.columns if c.startswith(\"V_\")]\n",
        "\n",
        "    print(f\"Task 9: Validating series: {vol_cols}\")\n",
        "\n",
        "    # Step 1: Integrity Check\n",
        "    check_series_integrity(df, vol_cols)\n",
        "    print(\"  - Integrity check passed (no NaNs, no negatives).\")\n",
        "\n",
        "    # Step 2: Statistics & Outliers\n",
        "    stats_df = compute_series_statistics(df, vol_cols)\n",
        "    outliers = detect_outliers(df, vol_cols)\n",
        "\n",
        "    # Check zero counts\n",
        "    zero_counts = {}\n",
        "    for col in vol_cols:\n",
        "        zeros = (df[col] == 0).sum()\n",
        "        zero_counts[col] = zeros\n",
        "        if zeros / len(df) > 0.05:\n",
        "            print(f\"WARNING: Series '{col}' has {zeros} zero-volume days (>5%).\")\n",
        "\n",
        "    # Step 3: Plot Specs\n",
        "    plot_specs = generate_plot_specs(vol_cols, meta_config)\n",
        "\n",
        "    print(\"Task 9 Complete. Statistics computed.\")\n",
        "\n",
        "    return ChainSeriesValidationResult(\n",
        "        is_valid=True,\n",
        "        stats_df=stats_df,\n",
        "        outliers=outliers,\n",
        "        zero_counts=zero_counts,\n",
        "        plot_specifications=plot_specs\n",
        "    )\n"
      ],
      "metadata": {
        "id": "tlpflwIp1fbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 — Cleanse and Normalize df_market_raw\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Cleanse and Normalize df_market_raw\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class CleanedMarketData:\n",
        "    \"\"\"\n",
        "    Container for the cleansed market data.\n",
        "\n",
        "    Attributes:\n",
        "        cleaned_df (pd.DataFrame): The DataFrame with normalized dates and valid numeric values.\n",
        "        initial_rows (int): Row count before cleansing.\n",
        "        rows_after_time_filter (int): Row count after applying observation window.\n",
        "        rows_after_numeric_clean (int): Row count after removing invalid/missing numeric data.\n",
        "        dropped_time_count (int): Number of rows dropped due to time window.\n",
        "        dropped_numeric_count (int): Number of rows dropped due to missing/invalid values.\n",
        "    \"\"\"\n",
        "    cleaned_df: pd.DataFrame\n",
        "    initial_rows: int\n",
        "    rows_after_time_filter: int\n",
        "    rows_after_numeric_clean: int\n",
        "    dropped_time_count: int\n",
        "    dropped_numeric_count: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Normalize Date Column to UTC\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_market_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes the 'Date' column to UTC midnight.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The validated market DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the DataFrame with normalized dates.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Ensure UTC and normalize to midnight\n",
        "    df_out[\"Date\"] = pd.to_datetime(df_out[\"Date\"], utc=True).dt.normalize()\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Apply Temporal Filter\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def filter_market_window(\n",
        "    df: pd.DataFrame,\n",
        "    start_date: pd.Timestamp,\n",
        "    end_date: pd.Timestamp\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the market data to the observation window.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with normalized dates.\n",
        "        start_date (pd.Timestamp): Start of window (UTC).\n",
        "        end_date (pd.Timestamp): End of window (UTC).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A subset of the input DataFrame.\n",
        "    \"\"\"\n",
        "    # Ensure start/end are normalized\n",
        "    s_norm = start_date.normalize()\n",
        "    e_norm = end_date.normalize()\n",
        "\n",
        "    mask = (df[\"Date\"] >= s_norm) & (df[\"Date\"] <= e_norm)\n",
        "\n",
        "    return df.loc[mask].copy()\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Validate Numeric Columns\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def clean_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates and cleans numeric columns (Close, Volume).\n",
        "\n",
        "    - Drops rows with NaN in Close or Volume.\n",
        "    - Drops rows with Close <= 0 or Volume < 0.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The temporally filtered DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    # Check for NaNs\n",
        "    # We drop rows with any missing critical data\n",
        "    df_clean = df.dropna(subset=[\"Close\", \"Volume\"]).copy()\n",
        "\n",
        "    # Check for invalid values\n",
        "    # Close must be > 0\n",
        "    # Volume must be >= 0\n",
        "    valid_mask = (df_clean[\"Close\"] > 0) & (df_clean[\"Volume\"] >= 0)\n",
        "\n",
        "    invalid_count = (~valid_mask).sum()\n",
        "    if invalid_count > 0:\n",
        "        print(f\"WARNING: Dropping {invalid_count} rows with invalid numeric values (Close<=0 or Volume<0).\")\n",
        "\n",
        "    return df_clean.loc[valid_mask].copy()\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_market_data(\n",
        "    validation_result: Any,\n",
        "    meta_config: Any\n",
        ") -> CleanedMarketData:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing of market data.\n",
        "\n",
        "    Executes:\n",
        "    1. Date normalization.\n",
        "    2. Temporal filtering.\n",
        "    3. Numeric validation and cleaning.\n",
        "\n",
        "    Args:\n",
        "        validation_result (MarketValidationResult): Result object from Task 3.\n",
        "        meta_config (ValidatedMetaConfig): Metadata configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        CleanedMarketData: Result object containing the clean DataFrame and counts.\n",
        "    \"\"\"\n",
        "    df_input = validation_result.validated_df\n",
        "    initial_rows = len(df_input)\n",
        "    print(f\"Task 10: Cleansing market data ({initial_rows} rows)...\")\n",
        "\n",
        "    # Step 1: Normalize Dates\n",
        "    df_dated = normalize_market_dates(df_input)\n",
        "\n",
        "    # Step 2: Temporal Filter\n",
        "    print(f\"Task 10: Applying temporal filter ({meta_config.start_date.date()} to {meta_config.end_date.date()})...\")\n",
        "    df_time_filtered = filter_market_window(\n",
        "        df_dated,\n",
        "        meta_config.start_date,\n",
        "        meta_config.end_date\n",
        "    )\n",
        "\n",
        "    rows_after_time = len(df_time_filtered)\n",
        "    dropped_time = initial_rows - rows_after_time\n",
        "    print(f\"  - Dropped {dropped_time} rows outside window.\")\n",
        "\n",
        "    # Step 3: Numeric Cleaning\n",
        "    print(\"Task 10: Cleaning numeric columns...\")\n",
        "    df_final = clean_numeric_columns(df_time_filtered)\n",
        "\n",
        "    rows_after_numeric = len(df_final)\n",
        "    dropped_numeric = rows_after_time - rows_after_numeric\n",
        "    print(f\"  - Dropped {dropped_numeric} rows with missing/invalid data.\")\n",
        "\n",
        "    print(f\"Task 10 Complete. Final Rows: {rows_after_numeric}\")\n",
        "\n",
        "    return CleanedMarketData(\n",
        "        cleaned_df=df_final,\n",
        "        initial_rows=initial_rows,\n",
        "        rows_after_time_filter=rows_after_time,\n",
        "        rows_after_numeric_clean=rows_after_numeric,\n",
        "        dropped_time_count=dropped_time,\n",
        "        dropped_numeric_count=dropped_numeric\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ibEDORSn2gbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Extract Asset-Specific Market Series\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Extract Asset-Specific Market Series\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class MarketSeriesData:\n",
        "    \"\"\"\n",
        "    Container for the extracted and combined market time series.\n",
        "\n",
        "    Attributes:\n",
        "        market_daily_df (pd.DataFrame): The combined DataFrame with Date and asset-specific columns.\n",
        "        btc_price_series (pd.Series): Daily BTC closing prices.\n",
        "        eth_price_series (pd.Series): Daily ETH closing prices.\n",
        "        usdt_volume_series (pd.Series): Daily USDT exchange volume.\n",
        "        usdc_volume_series (pd.Series): Daily USDC exchange volume.\n",
        "        common_dates_count (int): Number of days where all assets have data.\n",
        "    \"\"\"\n",
        "    market_daily_df: pd.DataFrame\n",
        "    btc_price_series: pd.Series\n",
        "    eth_price_series: pd.Series\n",
        "    usdt_volume_series: pd.Series\n",
        "    usdc_volume_series: pd.Series\n",
        "    common_dates_count: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Filter and Extract BTC and ETH Price Series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_price_series(df: pd.DataFrame, symbol: str, col_name: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Extracts the closing price series for a specific symbol.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The cleaned market DataFrame.\n",
        "        symbol (str): The trading symbol (e.g., \"BTC/USD\").\n",
        "        col_name (str): The name for the output series (e.g., \"Close_BTC_USD\").\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Time series of closing prices indexed by Date.\n",
        "    \"\"\"\n",
        "    # Filter by symbol\n",
        "    mask = df[\"Symbol\"] == symbol\n",
        "    subset = df.loc[mask].copy()\n",
        "\n",
        "    # Check for duplicate dates\n",
        "    if subset[\"Date\"].duplicated().any():\n",
        "        raise ValueError(f\"Duplicate dates found for symbol {symbol}.\")\n",
        "\n",
        "    # Set index and select Close\n",
        "    series = subset.set_index(\"Date\")[\"Close\"]\n",
        "    series.name = col_name\n",
        "\n",
        "    return series\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Filter and Extract Stablecoin Volume Series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_volume_series(df: pd.DataFrame, symbol: str, col_name: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Extracts the volume series for a specific symbol.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The cleaned market DataFrame.\n",
        "        symbol (str): The trading symbol (e.g., \"USDT/USD\").\n",
        "        col_name (str): The name for the output series (e.g., \"Volume_USDT_USD\").\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Time series of volume indexed by Date.\n",
        "    \"\"\"\n",
        "    # Filter by symbol\n",
        "    mask = df[\"Symbol\"] == symbol\n",
        "    subset = df.loc[mask].copy()\n",
        "\n",
        "    # Check for duplicate dates\n",
        "    if subset[\"Date\"].duplicated().any():\n",
        "        raise ValueError(f\"Duplicate dates found for symbol {symbol}.\")\n",
        "\n",
        "    # Set index and select Volume\n",
        "    series = subset.set_index(\"Date\")[\"Volume\"]\n",
        "    series.name = col_name\n",
        "\n",
        "    return series\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Combine into Market Daily DataFrame\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def combine_market_series(series_dict: Dict[str, pd.Series]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Combines individual time series into a single DataFrame aligned by Date.\n",
        "\n",
        "    Uses inner join to ensure only dates with complete data across all assets are retained.\n",
        "\n",
        "    Args:\n",
        "        series_dict (Dict[str, pd.Series]): Dictionary of named time series.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Combined DataFrame with 'Date' as a column.\n",
        "    \"\"\"\n",
        "    # Concatenate with inner join to align dates\n",
        "    df_combined = pd.concat(series_dict.values(), axis=1, join=\"inner\")\n",
        "\n",
        "    # Sort by date\n",
        "    df_combined = df_combined.sort_index()\n",
        "\n",
        "    # Reset index to make Date a column\n",
        "    df_final = df_combined.reset_index()\n",
        "\n",
        "    return df_final\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_market_series(cleaned_data: Any) -> MarketSeriesData:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction and combination of market time series.\n",
        "\n",
        "    Executes:\n",
        "    1. Extraction of BTC/ETH prices.\n",
        "    2. Extraction of USDT/USDC volumes.\n",
        "    3. Combination into a unified daily DataFrame.\n",
        "\n",
        "    Args:\n",
        "        cleaned_data (CleanedMarketData): Result object from Task 10.\n",
        "\n",
        "    Returns:\n",
        "        MarketSeriesData: Result object containing the series and combined DataFrame.\n",
        "    \"\"\"\n",
        "    df_input = cleaned_data.cleaned_df\n",
        "    print(\"Task 11: Extracting asset-specific series...\")\n",
        "\n",
        "    # Step 1: Prices\n",
        "    btc_series = extract_price_series(df_input, \"BTC/USD\", \"Close_BTC_USD\")\n",
        "    eth_series = extract_price_series(df_input, \"ETH/USD\", \"Close_ETH_USD\")\n",
        "\n",
        "    # Step 2: Volumes\n",
        "    usdt_series = extract_volume_series(df_input, \"USDT/USD\", \"Volume_USDT_USD\")\n",
        "    usdc_series = extract_volume_series(df_input, \"USDC/USD\", \"Volume_USDC_USD\")\n",
        "\n",
        "    # Step 3: Combine\n",
        "    series_map = {\n",
        "        \"Close_BTC_USD\": btc_series,\n",
        "        \"Close_ETH_USD\": eth_series,\n",
        "        \"Volume_USDT_USD\": usdt_series,\n",
        "        \"Volume_USDC_USD\": usdc_series\n",
        "    }\n",
        "\n",
        "    df_combined = combine_market_series(series_map)\n",
        "\n",
        "    count = len(df_combined)\n",
        "    print(f\"Task 11 Complete. Combined {count} common trading days.\")\n",
        "    print(f\"Columns: {list(df_combined.columns)}\")\n",
        "\n",
        "    return MarketSeriesData(\n",
        "        market_daily_df=df_combined,\n",
        "        btc_price_series=btc_series,\n",
        "        eth_price_series=eth_series,\n",
        "        usdt_volume_series=usdt_series,\n",
        "        usdc_volume_series=usdc_series,\n",
        "        common_dates_count=count\n",
        "    )\n"
      ],
      "metadata": {
        "id": "bksT25pr3owv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Merge On-Chain and Market Data\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Merge On-Chain and Market Data\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class MergedPanelData:\n",
        "    \"\"\"\n",
        "    Container for the final merged daily panel dataset.\n",
        "\n",
        "    Attributes:\n",
        "        df_panel (pd.DataFrame): The merged DataFrame containing aligned on-chain and market series.\n",
        "        chain_rows (int): Number of rows in the on-chain input.\n",
        "        market_rows (int): Number of rows in the market input.\n",
        "        merged_rows (int): Number of rows after inner join.\n",
        "        start_date (pd.Timestamp): Earliest date in the panel.\n",
        "        end_date (pd.Timestamp): Latest date in the panel.\n",
        "    \"\"\"\n",
        "    df_panel: pd.DataFrame\n",
        "    chain_rows: int\n",
        "    market_rows: int\n",
        "    merged_rows: int\n",
        "    start_date: pd.Timestamp\n",
        "    end_date: pd.Timestamp\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Perform Inner Join on Date\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def join_datasets(df_chain: pd.DataFrame, df_market: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs an inner join between on-chain and market data on the 'Date' column.\n",
        "\n",
        "    Args:\n",
        "        df_chain (pd.DataFrame): Daily on-chain volumes.\n",
        "        df_market (pd.DataFrame): Daily market prices and volumes.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The merged DataFrame.\n",
        "    \"\"\"\n",
        "    # Ensure Date is a column in both\n",
        "    # Task 8 and 11 ensured Date is a column, but we double check/reset if needed\n",
        "    # to be robust against upstream changes.\n",
        "    # Check dtypes\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df_chain[\"Date\"]):\n",
        "        raise ValueError(\"df_chain 'Date' column is not datetime.\")\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df_market[\"Date\"]):\n",
        "        raise ValueError(\"df_market 'Date' column is not datetime.\")\n",
        "\n",
        "    # Perform Inner Join\n",
        "    df_merged = pd.merge(\n",
        "        df_chain,\n",
        "        df_market,\n",
        "        on=\"Date\",\n",
        "        how=\"inner\",\n",
        "        validate=\"1:1\"  # Ensure uniqueness on both sides\n",
        "    )\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Verify Merged DataFrame Completeness\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_panel_completeness(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Verifies that the merged DataFrame contains all required columns and no NaNs.\n",
        "\n",
        "    Required Columns:\n",
        "    - Date\n",
        "    - V_EOA_EOA_USDT, V_EOA_EOA_USDC\n",
        "    - V_SC_SC_USDT, V_SC_SC_USDC\n",
        "    - Volume_USDT_USD, Volume_USDC_USD\n",
        "    - Close_BTC_USD, Close_ETH_USD\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The merged DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The verified DataFrame (rows with NaNs dropped).\n",
        "    \"\"\"\n",
        "    required_cols = [\n",
        "        \"Date\",\n",
        "        \"V_EOA_EOA_USDT\", \"V_EOA_EOA_USDC\",\n",
        "        \"V_SC_SC_USDT\", \"V_SC_SC_USDC\",\n",
        "        \"Volume_USDT_USD\", \"Volume_USDC_USD\",\n",
        "        \"Close_BTC_USD\", \"Close_ETH_USD\"\n",
        "    ]\n",
        "\n",
        "    # Check column presence\n",
        "    missing = set(required_cols) - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Merged panel is missing columns: {missing}\")\n",
        "\n",
        "    # Check for NaNs\n",
        "    if df[required_cols].isnull().any().any():\n",
        "        nan_count = df[required_cols].isnull().any(axis=1).sum()\n",
        "        print(f\"WARNING: Found {nan_count} rows with NaNs in merged panel. Dropping them.\")\n",
        "        df_clean = df.dropna(subset=required_cols).copy()\n",
        "    else:\n",
        "        df_clean = df.copy()\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Finalize Panel Structure\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def finalize_panel(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Finalizes the panel structure: sorts by Date and resets index.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The verified DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final panel DataFrame.\n",
        "    \"\"\"\n",
        "    # Sort by Date\n",
        "    df_sorted = df.sort_values(\"Date\", ascending=True)\n",
        "\n",
        "    # Reset Index\n",
        "    df_final = df_sorted.reset_index(drop=True)\n",
        "\n",
        "    return df_final\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def merge_data_sources(\n",
        "    chain_data: Any,\n",
        "    market_data: Any\n",
        ") -> MergedPanelData:\n",
        "    \"\"\"\n",
        "    Orchestrates the merging of on-chain and market data into a unified panel.\n",
        "\n",
        "    Executes:\n",
        "    1. Inner join on Date.\n",
        "    2. Completeness verification (dropping NaNs).\n",
        "    3. Final sorting and indexing.\n",
        "\n",
        "    Args:\n",
        "        chain_data (AggregatedChainData): Result object from Task 8.\n",
        "        market_data (MarketSeriesData): Result object from Task 11.\n",
        "\n",
        "    Returns:\n",
        "        MergedPanelData: Result object containing the final panel.\n",
        "    \"\"\"\n",
        "    df_chain = chain_data.daily_df\n",
        "    df_market = market_data.market_daily_df\n",
        "\n",
        "    print(\"Task 12: Merging on-chain and market data...\")\n",
        "\n",
        "    # Step 1: Join\n",
        "    df_merged_raw = join_datasets(df_chain, df_market)\n",
        "\n",
        "    # Step 2: Verify\n",
        "    df_verified = verify_panel_completeness(df_merged_raw)\n",
        "\n",
        "    # Step 3: Finalize\n",
        "    df_panel = finalize_panel(df_verified)\n",
        "\n",
        "    rows = len(df_panel)\n",
        "    start = df_panel[\"Date\"].min()\n",
        "    end = df_panel[\"Date\"].max()\n",
        "\n",
        "    print(f\"Task 12 Complete. Final Panel: {rows} rows ({start.date()} to {end.date()}).\")\n",
        "\n",
        "    return MergedPanelData(\n",
        "        df_panel=df_panel,\n",
        "        chain_rows=len(df_chain),\n",
        "        market_rows=len(df_market),\n",
        "        merged_rows=rows,\n",
        "        start_date=start,\n",
        "        end_date=end\n",
        "    )\n"
      ],
      "metadata": {
        "id": "T2NyEuNe4iDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Construct Log-Transformed Series\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Construct Log-Transformed Series\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class LogTransformedData:\n",
        "    \"\"\"\n",
        "    Container for the panel data with log-transformed series.\n",
        "\n",
        "    Attributes:\n",
        "        df_log (pd.DataFrame): The DataFrame containing original and log-transformed series.\n",
        "        log_columns (List[str]): List of names of the newly created log columns.\n",
        "        transformation_map (Dict[str, str]): Mapping from original column to log column.\n",
        "    \"\"\"\n",
        "    df_log: pd.DataFrame\n",
        "    log_columns: List[str]\n",
        "    transformation_map: Dict[str, str]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Identify Series for Log Transformation\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def identify_target_series() -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Identifies the series requiring log transformation and defines their types.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: Mapping of column name to transformation type ('log1p' for volumes, 'log' for prices).\n",
        "    \"\"\"\n",
        "    # Volume series (can be 0) -> log1p\n",
        "    volumes = [\n",
        "        \"V_EOA_EOA_USDT\", \"V_EOA_EOA_USDC\",\n",
        "        \"V_SC_SC_USDT\", \"V_SC_SC_USDC\",\n",
        "        \"Volume_USDT_USD\", \"Volume_USDC_USD\"\n",
        "    ]\n",
        "\n",
        "    # Price series (strictly positive) -> log\n",
        "    prices = [\n",
        "        \"Close_BTC_USD\", \"Close_ETH_USD\"\n",
        "    ]\n",
        "\n",
        "    targets = {col: \"log1p\" for col in volumes}\n",
        "    targets.update({col: \"log\" for col in prices})\n",
        "\n",
        "    return targets\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 2 & 3: Handle Zero Values and Create Log Columns\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_log_transformations(df: pd.DataFrame, targets: Dict[str, str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies logarithmic transformations to specified columns.\n",
        "\n",
        "    - Uses np.log1p(x) = log(x + 1) for volume series (handling zeros).\n",
        "    - Uses np.log(x) for price series (strictly positive).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The merged panel DataFrame.\n",
        "        targets (Dict[str, str]): Mapping of column names to transformation type.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the DataFrame with new 'log_' columns.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    for col, method in targets.items():\n",
        "        if col not in df_out.columns:\n",
        "            raise ValueError(f\"Column '{col}' missing from DataFrame.\")\n",
        "\n",
        "        new_col = f\"log_{col}\"\n",
        "\n",
        "        if method == \"log1p\":\n",
        "            # Ensure non-negative\n",
        "            if (df_out[col] < 0).any():\n",
        "                raise ValueError(f\"Column '{col}' contains negative values, cannot apply log1p.\")\n",
        "            df_out[new_col] = np.log1p(df_out[col])\n",
        "\n",
        "        elif method == \"log\":\n",
        "            # Ensure strictly positive\n",
        "            if (df_out[col] <= 0).any():\n",
        "                raise ValueError(f\"Column '{col}' contains non-positive values, cannot apply log.\")\n",
        "            df_out[new_col] = np.log(df_out[col])\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_log_series(merged_data: Any) -> LogTransformedData:\n",
        "    \"\"\"\n",
        "    Orchestrates the creation of log-transformed series.\n",
        "\n",
        "    Executes:\n",
        "    1. Identification of target columns.\n",
        "    2. Application of log/log1p transformations.\n",
        "\n",
        "    Args:\n",
        "        merged_data (MergedPanelData): Result object from Task 12.\n",
        "\n",
        "    Returns:\n",
        "        LogTransformedData: Result object containing the augmented DataFrame.\n",
        "    \"\"\"\n",
        "    df_input = merged_data.df_panel\n",
        "    print(\"Task 13: Constructing log-transformed series...\")\n",
        "\n",
        "    # Step 1: Identify\n",
        "    targets = identify_target_series()\n",
        "\n",
        "    # Step 2 & 3: Transform\n",
        "    df_log = apply_log_transformations(df_input, targets)\n",
        "\n",
        "    # Metadata\n",
        "    log_cols = [f\"log_{c}\" for c in targets.keys()]\n",
        "    trans_map = {c: f\"log_{c}\" for c in targets.keys()}\n",
        "\n",
        "    print(f\"Task 13 Complete. Created {len(log_cols)} log columns.\")\n",
        "\n",
        "    return LogTransformedData(\n",
        "        df_log=df_log,\n",
        "        log_columns=log_cols,\n",
        "        transformation_map=trans_map\n",
        "    )\n"
      ],
      "metadata": {
        "id": "vouqjWCh5sSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Perform ADF Stationarity Tests\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Perform ADF Stationarity Tests\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ADFResult:\n",
        "    \"\"\"\n",
        "    Container for a single Augmented Dickey-Fuller test result.\n",
        "\n",
        "    Attributes:\n",
        "        series_name (str): Name of the time series tested.\n",
        "        test_statistic (float): The computed ADF statistic.\n",
        "        p_value (float): The p-value associated with the test statistic.\n",
        "        used_lag (int): The number of lags used in the regression.\n",
        "        n_obs (int): The number of observations used for the ADF regression.\n",
        "        critical_values (Dict[str, float]): Critical values for 1%, 5%, and 10%.\n",
        "        is_stationary (bool): True if null hypothesis (unit root) is rejected at 5%.\n",
        "        integration_order (str): 'I(0)' if stationary, 'I(1)' if non-stationary (provisional).\n",
        "    \"\"\"\n",
        "    series_name: str\n",
        "    test_statistic: float\n",
        "    p_value: float\n",
        "    used_lag: int\n",
        "    n_obs: int\n",
        "    critical_values: Dict[str, float]\n",
        "    is_stationary: bool\n",
        "    integration_order: str\n",
        "\n",
        "@dataclass\n",
        "class StationarityTestResults:\n",
        "    \"\"\"\n",
        "    Container for the results of stationarity tests across all series.\n",
        "\n",
        "    Attributes:\n",
        "        results (List[ADFResult]): List of individual test results.\n",
        "        summary_df (pd.DataFrame): Summary table of statistics and decisions.\n",
        "        i1_series (List[str]): List of series names identified as I(1) (non-stationary).\n",
        "    \"\"\"\n",
        "    results: List[ADFResult]\n",
        "    summary_df: pd.DataFrame\n",
        "    i1_series: List[str]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Specify ADF Test Configuration\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_adf_config(preprocessing_config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extracts and formats the ADF test configuration.\n",
        "\n",
        "    Args:\n",
        "        preprocessing_config (Dict[str, Any]): The 'preprocessing' section of STUDY_CONFIG.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Configuration dictionary for adfuller.\n",
        "    \"\"\"\n",
        "    stat_config = preprocessing_config.get(\"stationarity_test\", {})\n",
        "\n",
        "    return {\n",
        "        \"regression\": stat_config.get(\"regression\", \"ct\"),  # Constant + Trend\n",
        "        \"autolag\": stat_config.get(\"autolag\", \"AIC\")        # Akaike Information Criterion\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 2: Execute ADF Test on Each Log-Level Series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_adf_test(series: pd.Series, config: Dict[str, Any]) -> ADFResult:\n",
        "    \"\"\"\n",
        "    Executes the Augmented Dickey-Fuller test on a single time series.\n",
        "\n",
        "    Hypotheses:\n",
        "    H0: The series has a unit root (Non-Stationary).\n",
        "    H1: The series is stationary.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The time series to test.\n",
        "        config (Dict[str, Any]): Configuration for adfuller (regression, autolag).\n",
        "\n",
        "    Returns:\n",
        "        ADFResult: Structured test results.\n",
        "    \"\"\"\n",
        "    # Drop NaNs (e.g., from differencing or missing data) before testing\n",
        "    clean_series = series.dropna()\n",
        "\n",
        "    # Run ADF\n",
        "    # adfuller returns: (adf_stat, pvalue, usedlag, nobs, critical_values, icbest)\n",
        "    result = adfuller(\n",
        "        clean_series,\n",
        "        regression=config[\"regression\"],\n",
        "        autolag=config[\"autolag\"]\n",
        "    )\n",
        "\n",
        "    adf_stat = result[0]\n",
        "    p_value = result[1]\n",
        "    used_lag = result[2]\n",
        "    n_obs = result[3]\n",
        "    crit_values = result[4]\n",
        "\n",
        "    # Decision Rule: Reject H0 if test_stat < critical_value_5%\n",
        "    # Or simply if p_value < 0.05\n",
        "    is_stationary = p_value < 0.05\n",
        "    integration_order = \"I(0)\" if is_stationary else \"I(1)\"\n",
        "\n",
        "    return ADFResult(\n",
        "        series_name=series.name,\n",
        "        test_statistic=adf_stat,\n",
        "        p_value=p_value,\n",
        "        used_lag=used_lag,\n",
        "        n_obs=n_obs,\n",
        "        critical_values=crit_values,\n",
        "        is_stationary=is_stationary,\n",
        "        integration_order=integration_order\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Document Integration Order\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def summarize_stationarity(results: List[ADFResult]) -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Summarizes ADF test results into a DataFrame and identifies I(1) series.\n",
        "\n",
        "    Args:\n",
        "        results (List[ADFResult]): List of ADF test results.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, List[str]]: Summary DataFrame and list of I(1) series names.\n",
        "    \"\"\"\n",
        "    summary_data = []\n",
        "    i1_series = []\n",
        "\n",
        "    # Iterate through 'results' object and extract key outputs\n",
        "    for res in results:\n",
        "        summary_data.append({\n",
        "            \"Series\": res.series_name,\n",
        "            \"ADF Statistic\": res.test_statistic,\n",
        "            \"p-value\": res.p_value,\n",
        "            \"Critical Value (5%)\": res.critical_values[\"5%\"],\n",
        "            \"Result\": \"Stationary\" if res.is_stationary else \"Non-Stationary\",\n",
        "            \"Order\": res.integration_order\n",
        "        })\n",
        "\n",
        "        if not res.is_stationary:\n",
        "            i1_series.append(res.series_name)\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    return summary_df, i1_series\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def perform_stationarity_tests(\n",
        "    log_data: Any,\n",
        "    preprocessing_config: Dict[str, Any]\n",
        ") -> StationarityTestResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the ADF stationarity testing for all log-transformed series.\n",
        "\n",
        "    Executes:\n",
        "    1. Configuration extraction.\n",
        "    2. ADF testing for each series.\n",
        "    3. Summarization and identification of non-stationary series.\n",
        "\n",
        "    Args:\n",
        "        log_data (LogTransformedData): Result object from Task 13.\n",
        "        preprocessing_config (Dict[str, Any]): Preprocessing configuration.\n",
        "\n",
        "    Returns:\n",
        "        StationarityTestResults: Result object containing detailed and summary results.\n",
        "    \"\"\"\n",
        "    df = log_data.df_log\n",
        "    log_cols = log_data.log_columns\n",
        "\n",
        "    print(f\"Task 14: Running ADF tests on {len(log_cols)} series...\")\n",
        "\n",
        "    # Step 1: Config\n",
        "    adf_config = get_adf_config(preprocessing_config)\n",
        "\n",
        "    # Step 2: Execute Tests\n",
        "    results = []\n",
        "    for col in log_cols:\n",
        "        series = df[col]\n",
        "        res = run_adf_test(series, adf_config)\n",
        "        results.append(res)\n",
        "\n",
        "    # Step 3: Summarize\n",
        "    summary_df, i1_series = summarize_stationarity(results)\n",
        "\n",
        "    print(\"Task 14 Complete. Summary:\")\n",
        "    print(summary_df[[\"Series\", \"p-value\", \"Order\"]])\n",
        "    print(f\"Identified {len(i1_series)} non-stationary (I(1)) series.\")\n",
        "\n",
        "    return StationarityTestResults(\n",
        "        results=results,\n",
        "        summary_df=summary_df,\n",
        "        i1_series=i1_series\n",
        "    )\n"
      ],
      "metadata": {
        "id": "M2_V6Zos6mkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Construct Differenced Series and Finalize Integration Order\n",
        "\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Construct Differenced Series and Finalize Integration Order\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class AnalysisSeriesMapping:\n",
        "    \"\"\"\n",
        "    Container mapping specific time series to their designated analysis methods.\n",
        "\n",
        "    Attributes:\n",
        "        bai_perron_series (List[str]): Series for structural break detection (Log-Levels).\n",
        "        hht_series (List[str]): Series for Hilbert-Huang Transform (Log-Levels).\n",
        "        svar_series (List[str]): Series for SVAR (Differenced/Stationary).\n",
        "    \"\"\"\n",
        "    bai_perron_series: List[str]\n",
        "    hht_series: List[str]\n",
        "    svar_series: List[str]\n",
        "\n",
        "@dataclass\n",
        "class FinalizedSeriesData:\n",
        "    \"\"\"\n",
        "    Container for the final dataset ready for econometric analysis.\n",
        "\n",
        "    Attributes:\n",
        "        df_final (pd.DataFrame): DataFrame containing raw, log, and differenced series.\n",
        "        diff_columns (List[str]): List of newly created differenced columns.\n",
        "        stationarity_summary (pd.DataFrame): Summary of ADF tests on differenced series.\n",
        "        series_mapping (AnalysisSeriesMapping): Mapping of series to methods.\n",
        "    \"\"\"\n",
        "    df_final: pd.DataFrame\n",
        "    diff_columns: List[str]\n",
        "    stationarity_summary: pd.DataFrame\n",
        "    series_mapping: AnalysisSeriesMapping\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Compute First Differences\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_differences(df: pd.DataFrame, i1_series: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the first difference for series identified as I(1).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with log-transformed series.\n",
        "        i1_series (List[str]): List of column names identified as non-stationary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A copy of the DataFrame with new 'diff_' columns.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    for col in i1_series:\n",
        "        new_col = f\"diff_{col}\"\n",
        "        # Compute difference\n",
        "        df_out[new_col] = df_out[col].diff()\n",
        "\n",
        "    return df_out\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 2: Run ADF on Differenced Series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_diff_stationarity(\n",
        "    df: pd.DataFrame,\n",
        "    diff_cols: List[str],\n",
        "    adf_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs ADF tests on the differenced series to confirm stationarity.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with differenced series.\n",
        "        diff_cols (List[str]): List of differenced column names.\n",
        "        adf_config (Dict[str, Any]): Configuration for adfuller.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Summary of ADF results for differenced series.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Reuse the run_adf_test logic from Task 14 context (re-implemented here for standalone completeness)\n",
        "    for col in diff_cols:\n",
        "        series = df[col].dropna()\n",
        "\n",
        "        # Run ADF\n",
        "        res = adfuller(series, regression=adf_config[\"regression\"], autolag=adf_config[\"autolag\"])\n",
        "\n",
        "        adf_stat = res[0]\n",
        "        p_value = res[1]\n",
        "        crit_5 = res[4][\"5%\"]\n",
        "        is_stationary = p_value < 0.05\n",
        "\n",
        "        results.append({\n",
        "            \"Series\": col,\n",
        "            \"ADF Statistic\": adf_stat,\n",
        "            \"p-value\": p_value,\n",
        "            \"Critical Value (5%)\": crit_5,\n",
        "            \"Result\": \"Stationary (I(0))\" if is_stationary else \"Non-Stationary (I(1))\"\n",
        "        })\n",
        "\n",
        "        if not is_stationary:\n",
        "            print(f\"WARNING: Series '{col}' is still non-stationary after differencing (p={p_value:.4f}).\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Assign Series to Analysis Methods\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def map_series_to_methods(\n",
        "    log_cols: List[str],\n",
        "    diff_cols: List[str]\n",
        ") -> AnalysisSeriesMapping:\n",
        "    \"\"\"\n",
        "    Maps the available series to the specific analysis methods based on the study design.\n",
        "\n",
        "    - Bai-Perron: Uses Log-Levels (mean-shift model).\n",
        "    - HHT: Uses Log-Levels (BTC/ETH prices).\n",
        "    - SVAR: Uses Differenced Log-Levels (Stationary volumes).\n",
        "\n",
        "    Args:\n",
        "        log_cols (List[str]): List of log-level columns.\n",
        "        diff_cols (List[str]): List of differenced columns.\n",
        "\n",
        "    Returns:\n",
        "        AnalysisSeriesMapping: Structured mapping.\n",
        "    \"\"\"\n",
        "    # 1. Bai-Perron: All log-level series (Volumes + Prices)\n",
        "    # The paper applies structural break tests to the levels (regime shift in mean volume/price level)\n",
        "    bp_series = log_cols\n",
        "\n",
        "    # 2. HHT: Only BTC and ETH prices (Log-Levels)\n",
        "    # We look for columns containing \"Close\"\n",
        "    hht_series = [c for c in log_cols if \"Close\" in c]\n",
        "\n",
        "    # 3. SVAR: Only Stablecoin Volumes (Differenced)\n",
        "    # We look for columns containing \"V_\" or \"Volume_\" in the differenced list\n",
        "    # The SVAR focuses on flow dynamics, so we use stationary inputs.\n",
        "    svar_series = [c for c in diff_cols if \"V_\" in c or \"Volume_\" in c]\n",
        "\n",
        "    return AnalysisSeriesMapping(\n",
        "        bai_perron_series=bp_series,\n",
        "        hht_series=hht_series,\n",
        "        svar_series=svar_series\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def finalize_integration_order(\n",
        "    log_data: Any,\n",
        "    stationarity_results: Any,\n",
        "    preprocessing_config: Dict[str, Any]\n",
        ") -> FinalizedSeriesData:\n",
        "    \"\"\"\n",
        "    Orchestrates the creation of differenced series and finalizes the dataset for analysis.\n",
        "\n",
        "    Executes:\n",
        "    1. Differencing of I(1) series.\n",
        "    2. Verification of stationarity for differenced series.\n",
        "    3. Mapping of series to analysis methods.\n",
        "\n",
        "    Args:\n",
        "        log_data (LogTransformedData): Result object from Task 13.\n",
        "        stationarity_results (StationarityTestResults): Result object from Task 14.\n",
        "        preprocessing_config (Dict[str, Any]): Preprocessing configuration.\n",
        "\n",
        "    Returns:\n",
        "        FinalizedSeriesData: The final dataset and metadata.\n",
        "    \"\"\"\n",
        "    df_input = log_data.df_log\n",
        "    i1_series = stationarity_results.i1_series\n",
        "\n",
        "    print(f\"Task 15: Differencing {len(i1_series)} non-stationary series...\")\n",
        "\n",
        "    # Step 1: Difference\n",
        "    df_diff = compute_differences(df_input, i1_series)\n",
        "    diff_cols = [f\"diff_{c}\" for c in i1_series]\n",
        "\n",
        "    # Step 2: Verify Stationarity\n",
        "    # Reconstruct ADF config\n",
        "    adf_config = {\n",
        "        \"regression\": preprocessing_config.get(\"stationarity_test\", {}).get(\"regression\", \"ct\"),\n",
        "        \"autolag\": preprocessing_config.get(\"stationarity_test\", {}).get(\"autolag\", \"AIC\")\n",
        "    }\n",
        "\n",
        "    print(\"Task 15: Verifying stationarity of differenced series...\")\n",
        "    summary_df = verify_diff_stationarity(df_diff, diff_cols, adf_config)\n",
        "\n",
        "    # Step 3: Map\n",
        "    mapping = map_series_to_methods(log_data.log_columns, diff_cols)\n",
        "\n",
        "    print(\"Task 15 Complete. Series mapped to methods:\")\n",
        "    print(f\"  - Bai-Perron: {len(mapping.bai_perron_series)} series\")\n",
        "    print(f\"  - HHT: {len(mapping.hht_series)} series\")\n",
        "    print(f\"  - SVAR: {len(mapping.svar_series)} series\")\n",
        "\n",
        "    return FinalizedSeriesData(\n",
        "        df_final=df_diff,\n",
        "        diff_columns=diff_cols,\n",
        "        stationarity_summary=summary_df,\n",
        "        series_mapping=mapping\n",
        "    )\n"
      ],
      "metadata": {
        "id": "FK3e0Oxm8AUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Apply Bai–Perron to EOA–EOA Blockchain Series\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Apply Bai–Perron to EOA–EOA Blockchain Series\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class BreakPointResult:\n",
        "    \"\"\"\n",
        "    Container for the results of a structural break test on a single series.\n",
        "\n",
        "    Attributes:\n",
        "        series_name (str): Name of the series.\n",
        "        optimal_k (int): The optimal number of breaks selected by BIC.\n",
        "        break_indices (List[int]): Indices of the detected break points (0-based).\n",
        "        break_dates (List[pd.Timestamp]): Calendar dates corresponding to the break indices.\n",
        "        global_ssr (float): Sum of Squared Residuals for the optimal partition.\n",
        "        bic (float): Bayesian Information Criterion for the optimal model.\n",
        "        sup_f_stat (float): The F-statistic testing 0 breaks vs optimal_k breaks.\n",
        "        regime_means (List[float]): The mean values for each regime.\n",
        "    \"\"\"\n",
        "    series_name: str\n",
        "    optimal_k: int\n",
        "    break_indices: List[int]\n",
        "    break_dates: List[pd.Timestamp]\n",
        "    global_ssr: float\n",
        "    bic: float\n",
        "    sup_f_stat: float\n",
        "    regime_means: List[float]\n",
        "\n",
        "@dataclass\n",
        "class BaiPerronResults:\n",
        "    \"\"\"\n",
        "    Container for the results of Bai-Perron tests on EOA-EOA series.\n",
        "\n",
        "    Attributes:\n",
        "        usdt_result (BreakPointResult): Results for USDT EOA-EOA volume.\n",
        "        usdc_result (BreakPointResult): Results for USDC EOA-EOA volume.\n",
        "    \"\"\"\n",
        "    usdt_result: BreakPointResult\n",
        "    usdc_result: BreakPointResult\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Specify Mean-Shift Model (Dynamic Programming Implementation)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_ssr_matrix(y: np.ndarray, min_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Precomputes the Sum of Squared Residuals (SSR) for all admissible segments.\n",
        "\n",
        "    This function calculates the SSR for every possible contiguous segment y[i:j]\n",
        "    where the segment length (j-i) is at least `min_size`. It uses vectorized\n",
        "    cumulative sums to achieve O(T^2) computational complexity, which is essential\n",
        "    for the dynamic programming step in structural break detection.\n",
        "\n",
        "    The SSR for a segment is calculated as:\n",
        "    SSR(i, j) = sum(y[i:j]^2) - (sum(y[i:j])^2) / (j - i)\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): The time series array of shape (T,).\n",
        "        min_size (int): Minimum segment length (h).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A (T+1, T+1) matrix where entry [i, j] is the SSR for segment y[i:j].\n",
        "                    Entries where segment length < min_size are infinity.\n",
        "    \"\"\"\n",
        "    T = len(y)\n",
        "    # Initialize matrix with infinity to represent invalid segments\n",
        "    ssr_mat = np.full((T + 1, T + 1), np.inf)\n",
        "\n",
        "    # Precompute cumulative sums of y and y^2 for O(1) segment sum retrieval\n",
        "    # cumsum[k] is sum(y[0:k])\n",
        "    # We prepend 0.0 to make indexing easier: cumsum[j] - cumsum[i] = sum(y[i:j])\n",
        "    cum_y = np.concatenate(([0.0], np.cumsum(y)))\n",
        "    cum_y2 = np.concatenate(([0.0], np.cumsum(y ** 2)))\n",
        "\n",
        "    # Iterate over all possible start points i\n",
        "    for i in range(T):\n",
        "        # Iterate over all possible end points j\n",
        "        # Segment must be at least min_size\n",
        "        # Vectorized calculation for all j > i + min_size\n",
        "\n",
        "        # Valid j range: i + min_size to T (inclusive)\n",
        "        j_indices = np.arange(i + min_size, T + 1)\n",
        "\n",
        "        if len(j_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate sums for segments y[i:j] using precomputed cumulative sums\n",
        "        sum_y = cum_y[j_indices] - cum_y[i]\n",
        "        sum_y2 = cum_y2[j_indices] - cum_y2[i]\n",
        "        n = j_indices - i\n",
        "\n",
        "        # Calculate SSR using the variance formula: sum(x^2) - (sum(x)^2)/n\n",
        "        # SSR = sum(y^2) - (sum(y)^2) / n\n",
        "        ssr_vals = sum_y2 - (sum_y ** 2) / n\n",
        "\n",
        "        # Store in matrix\n",
        "        # Note: ssr_vals corresponds to j_indices\n",
        "        ssr_mat[i, j_indices] = ssr_vals\n",
        "\n",
        "    return ssr_mat\n",
        "\n",
        "def bai_perron_dp(\n",
        "    y: np.ndarray,\n",
        "    max_breaks: int,\n",
        "    min_size: int\n",
        ") -> Dict[int, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes the Bai-Perron dynamic programming algorithm to find optimal partitions.\n",
        "\n",
        "    This function implements the core dynamic programming recursion described in\n",
        "    Bai & Perron (2003). It finds the global minimum Sum of Squared Residuals (SSR)\n",
        "    for every number of breaks k from 0 to `max_breaks`.\n",
        "\n",
        "    The recursion is:\n",
        "    SSR(T_{k,T}) = min_{m*h <= j <= T-h} [ SSR(T_{k-1, j}) + SSR(j, T) ]\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): The time series array of shape (T,).\n",
        "        max_breaks (int): Maximum number of breaks allowed (m).\n",
        "        min_size (int): Minimum segment size (h).\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, Dict]: A dictionary mapping k (number of breaks) to results:\n",
        "            {\n",
        "                k: {\n",
        "                    'ssr': float,       # The global minimum SSR for k breaks\n",
        "                    'breaks': List[int] # The list of break indices (0-based)\n",
        "                }\n",
        "            }\n",
        "    \"\"\"\n",
        "    T = len(y)\n",
        "    # Precompute the SSR matrix for all segments\n",
        "    ssr_mat = compute_ssr_matrix(y, min_size)\n",
        "\n",
        "    # dp[k][t] stores the min SSR for partitioning y[0:t] with k breaks\n",
        "    # partition_map[k][t] stores the index of the *last* break for backtracking\n",
        "    dp = np.full((max_breaks + 1, T + 1), np.inf)\n",
        "    partition_map = np.zeros((max_breaks + 1, T + 1), dtype=int)\n",
        "\n",
        "    # Base case: 0 breaks (1 segment)\n",
        "    # dp[0][t] is simply SSR(0, t) for the single segment y[0:t]\n",
        "    for t in range(min_size, T + 1):\n",
        "        dp[0][t] = ssr_mat[0, t]\n",
        "\n",
        "    # Recursive step: 1 to max_breaks\n",
        "    for k in range(1, max_breaks + 1):\n",
        "        # t must be at least (k+1)*min_size to accommodate k+1 segments\n",
        "        for t in range((k + 1) * min_size, T + 1):\n",
        "\n",
        "            # We want to find split point j such that:\n",
        "            # 1. Previous partition y[0:j] has k-1 breaks (valid if j >= k*min_size)\n",
        "            # 2. Last segment y[j:t] has length >= min_size (valid if j <= t - min_size)\n",
        "\n",
        "            # Search range for j (the location of the k-th break)\n",
        "            j_start = k * min_size\n",
        "            j_end = t - min_size # inclusive\n",
        "\n",
        "            if j_start > j_end:\n",
        "                continue\n",
        "\n",
        "            # Vectorized search for optimal j\n",
        "            j_candidates = np.arange(j_start, j_end + 1)\n",
        "\n",
        "            # Cost = SSR of previous partition (k-1 breaks up to j) + SSR of new segment (j to t)\n",
        "            costs = dp[k-1, j_candidates] + ssr_mat[j_candidates, t]\n",
        "\n",
        "            # Find the j that minimizes the total SSR\n",
        "            min_idx = np.argmin(costs)\n",
        "            min_cost = costs[min_idx]\n",
        "            best_j = j_candidates[min_idx]\n",
        "\n",
        "            dp[k][t] = min_cost\n",
        "            partition_map[k][t] = best_j\n",
        "\n",
        "    # Reconstruct optimal partitions for each k by backtracking\n",
        "    results = {}\n",
        "    for k in range(max_breaks + 1):\n",
        "        ssr = dp[k][T]\n",
        "        if np.isinf(ssr):\n",
        "            continue\n",
        "\n",
        "        breaks = []\n",
        "        if k > 0:\n",
        "            curr = T\n",
        "            for _ in range(k):\n",
        "                # Find where the last break occurred for the current end point\n",
        "                prev = partition_map[k - len(breaks)][curr]\n",
        "                breaks.append(prev)\n",
        "                curr = prev\n",
        "            breaks.reverse()\n",
        "\n",
        "        results[k] = {\n",
        "            'ssr': ssr,\n",
        "            'breaks': breaks\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "def select_optimal_model(\n",
        "    y: np.ndarray,\n",
        "    dp_results: Dict[int, Dict[str, Any]]\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Selects the optimal number of breaks using the Bayesian Information Criterion (BIC).\n",
        "\n",
        "    The BIC penalizes model complexity to prevent overfitting. For a structural change\n",
        "    model with k breaks (k+1 regimes), the number of parameters is p*(k+1) + k,\n",
        "    where p is the number of coefficients per regime. For a pure mean-shift model, p=1.\n",
        "\n",
        "    BIC(k) = T * ln(SSR(k)/T) + (k + 1)(q + 1) * ln(T)\n",
        "    Here q=0 (only intercept changes), so parameters = k+1 means + k break dates?\n",
        "    Bai-Perron (2003) suggest penalty term: p*ln(T) where p is number of parameters.\n",
        "    For pure mean shift: p = (k+1) means + k break dates = 2k + 1.\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): The time series array.\n",
        "        dp_results (Dict): Results from the DP algorithm containing SSRs for each k.\n",
        "\n",
        "    Returns:\n",
        "        int: The optimal number of breaks (k) that minimizes the BIC.\n",
        "    \"\"\"\n",
        "    T = len(y)\n",
        "    best_k = 0\n",
        "    min_bic = np.inf\n",
        "\n",
        "    for k, res in dp_results.items():\n",
        "        ssr = res['ssr']\n",
        "        if ssr <= 1e-9: # Avoid log(0) or negative SSR due to precision\n",
        "            continue\n",
        "\n",
        "        # Number of parameters: (k+1) means + k break dates\n",
        "        n_params = (k + 1) + k\n",
        "\n",
        "        # BIC formula: T * ln(MSE) + penalty\n",
        "        bic = T * np.log(ssr / T) + n_params * np.log(T)\n",
        "\n",
        "        if bic < min_bic:\n",
        "            min_bic = bic\n",
        "            best_k = k\n",
        "\n",
        "    return best_k\n",
        "\n",
        "def compute_f_statistic(\n",
        "    y: np.ndarray,\n",
        "    ssr_restricted: float,\n",
        "    ssr_unrestricted: float,\n",
        "    k: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the F-statistic for testing the null hypothesis of 0 breaks against\n",
        "    the alternative of k breaks.\n",
        "\n",
        "    The statistic is defined as:\n",
        "    F = ((SSR_0 - SSR_k) / k) / (SSR_k / (T - (k+1) - k))\n",
        "\n",
        "    Degrees of freedom:\n",
        "    - Numerator: k (number of additional breaks * parameters changing per break).\n",
        "      For mean shift, q=1 parameter changes per break. So numerator df = k.\n",
        "    - Denominator: T - n_params_unrestricted = T - (2k + 1)\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): The time series array.\n",
        "        ssr_restricted (float): SSR for the restricted model (0 breaks).\n",
        "        ssr_unrestricted (float): SSR for the unrestricted model (k breaks).\n",
        "        k (int): Number of breaks in the alternative hypothesis.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed F-statistic. Returns 0.0 if k=0 or denominator is 0.\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "\n",
        "    T = len(y)\n",
        "    # Number of parameters in unrestricted model: (k+1) means + k break dates\n",
        "    n_params = 2 * k + 1\n",
        "    df_resid = T - n_params\n",
        "\n",
        "    # F-statistic calculation\n",
        "    numerator = (ssr_restricted - ssr_unrestricted) / k # q=1\n",
        "    denominator = ssr_unrestricted / df_resid\n",
        "\n",
        "    if denominator == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return numerator / denominator\n",
        "\n",
        "def compute_regime_means(y: np.ndarray, breaks: List[int]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Computes the mean value for each regime defined by the break indices.\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): The time series array.\n",
        "        breaks (List[int]): List of break indices (start of new regimes).\n",
        "\n",
        "    Returns:\n",
        "        List[float]: List of mean values for each segment [break_i, break_{i+1}).\n",
        "    \"\"\"\n",
        "    means = []\n",
        "    start = 0\n",
        "    # Add T to breaks to handle the last segment uniformly\n",
        "    boundaries = breaks + [len(y)]\n",
        "\n",
        "    for end in boundaries:\n",
        "        segment = y[start:end]\n",
        "        if len(segment) > 0:\n",
        "            means.append(np.mean(segment))\n",
        "        else:\n",
        "            means.append(0.0) # Should not happen with min_size constraint\n",
        "        start = end\n",
        "\n",
        "    return means\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 2 & 3: Apply to EOA-EOA Series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_series_breaks_robust(\n",
        "    series: pd.Series,\n",
        "    config: Any\n",
        ") -> BreakPointResult:\n",
        "    \"\"\"\n",
        "    Runs the robust Bai-Perron analysis on a single pandas Series.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The time series (with DatetimeIndex or Date column).\n",
        "        config (ValidatedAnalysisConfig): Analysis configuration.\n",
        "\n",
        "    Returns:\n",
        "        BreakPointResult: The detected breaks and statistics.\n",
        "    \"\"\"\n",
        "    y = series.values\n",
        "    T = len(y)\n",
        "\n",
        "    # Parameters\n",
        "    max_breaks = config.bp_max_breaks\n",
        "    epsilon = config.bp_trimming\n",
        "    min_size = int(np.floor(epsilon * T))\n",
        "\n",
        "    # 1. Run Dynamic Programming\n",
        "    dp_results = bai_perron_dp(y, max_breaks, min_size)\n",
        "\n",
        "    # 2. Select Optimal Model\n",
        "    best_k = select_optimal_model(y, dp_results)\n",
        "\n",
        "    # 3. Extract Results for Optimal k\n",
        "    best_res = dp_results[best_k]\n",
        "    breaks = best_res['breaks']\n",
        "    ssr_k = best_res['ssr']\n",
        "\n",
        "    # 4. Compute Statistics\n",
        "    ssr_0 = dp_results[0]['ssr']\n",
        "\n",
        "    # If best_k is 0, we still want to report the SupF for 1 break to show if any break was significant\n",
        "    # But strictly, SupF usually refers to the test statistic for the selected model vs null.\n",
        "    # If selected model is 0 breaks, F is 0.\n",
        "    # However, to align with the paper which reports SupF for the detected break,\n",
        "    # if best_k > 0, we compute F(0 vs k).\n",
        "    # If best_k == 0, we report 0.\n",
        "\n",
        "    f_stat = compute_f_statistic(y, ssr_0, ssr_k, best_k)\n",
        "\n",
        "    # 5. Compute Regime Means\n",
        "    means = compute_regime_means(y, breaks)\n",
        "\n",
        "    # 6. Map Dates\n",
        "    # Break index 'b' means the break happens *at* index b (start of new regime).\n",
        "    # The date corresponding to index b is the start date of the new regime.\n",
        "    # The \"break date\" is usually the last date of the old regime or first of new.\n",
        "    # We will return the date at index b (first date of new regime).\n",
        "    break_dates = [series.index[b] for b in breaks]\n",
        "\n",
        "    # Recalculate BIC for reporting\n",
        "    n_params = 2 * best_k + 1\n",
        "    bic = T * np.log(ssr_k / T) + n_params * np.log(T)\n",
        "\n",
        "    return BreakPointResult(\n",
        "        series_name=series.name,\n",
        "        optimal_k=best_k,\n",
        "        break_indices=breaks,\n",
        "        break_dates=break_dates,\n",
        "        global_ssr=ssr_k,\n",
        "        bic=bic,\n",
        "        sup_f_stat=f_stat,\n",
        "        regime_means=means\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_bai_perron_eoa(\n",
        "    finalized_data: Any,\n",
        "    analysis_config: Any\n",
        ") -> BaiPerronResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the Bai-Perron structural break detection for EOA-EOA series.\n",
        "\n",
        "    Executes:\n",
        "    1. Extraction of USDT and USDC EOA-EOA log-volume series.\n",
        "    2. Application of the mean-shift break detection algorithm.\n",
        "    3. Computation of SupF statistics.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset from Task 15.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        BaiPerronResults: Container with results for both tokens.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "\n",
        "    # Ensure Date is index for easy mapping\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    print(\"Task 16: Running Bai-Perron on EOA-EOA series...\")\n",
        "\n",
        "    # 1. USDT\n",
        "    col_usdt = \"log_V_EOA_EOA_USDT\"\n",
        "    print(f\"  - Analyzing {col_usdt}...\")\n",
        "    res_usdt = analyze_series_breaks_robust(df_indexed[col_usdt].dropna(), analysis_config)\n",
        "    print(f\"    -> Optimal k: {res_usdt.optimal_k}\")\n",
        "    print(f\"    -> Breaks at: {[d.date() for d in res_usdt.break_dates]}\")\n",
        "    print(f\"    -> SupF: {res_usdt.sup_f_stat:.4f}\")\n",
        "\n",
        "    # 2. USDC\n",
        "    col_usdc = \"log_V_EOA_EOA_USDC\"\n",
        "    print(f\"  - Analyzing {col_usdc}...\")\n",
        "    res_usdc = analyze_series_breaks_robust(df_indexed[col_usdc].dropna(), analysis_config)\n",
        "    print(f\"    -> Optimal k: {res_usdc.optimal_k}\")\n",
        "    print(f\"    -> Breaks at: {[d.date() for d in res_usdc.break_dates]}\")\n",
        "    print(f\"    -> SupF: {res_usdc.sup_f_stat:.4f}\")\n",
        "\n",
        "    return BaiPerronResults(\n",
        "        usdt_result=res_usdt,\n",
        "        usdc_result=res_usdc\n",
        "    )\n"
      ],
      "metadata": {
        "id": "jA-9wa6y-azq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Apply Bai–Perron to SC–SC Blockchain Series\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Apply Bai–Perron to SC–SC Blockchain Series\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SCBaiPerronResults:\n",
        "    \"\"\"\n",
        "    Container for the results of Bai-Perron tests on SC-SC series.\n",
        "\n",
        "    Attributes:\n",
        "        usdt_result (BreakPointResult): Results for USDT SC-SC volume.\n",
        "        usdc_result (BreakPointResult): Results for USDC SC-SC volume.\n",
        "    \"\"\"\n",
        "    usdt_result: Any # Type: BreakPointResult\n",
        "    usdc_result: Any # Type: BreakPointResult\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1 & 2: Apply to SC-SC Series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# We reuse the analyze_series_breaks_robust function from Task 16.\n",
        "# No new low-level logic is needed, just the application to the new series.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_bai_perron_sc(\n",
        "    finalized_data: Any,\n",
        "    analysis_config: Any\n",
        ") -> SCBaiPerronResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the Bai-Perron structural break detection for SC-SC series.\n",
        "\n",
        "    Executes:\n",
        "    1. Extraction of USDT and USDC SC-SC log-volume series.\n",
        "    2. Application of the mean-shift break detection algorithm.\n",
        "    3. Computation of SupF statistics.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset from Task 15.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        SCBaiPerronResults: Container with results for both tokens.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "\n",
        "    # Ensure Date is index for easy mapping\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    print(\"Task 17: Running Bai-Perron on SC-SC series...\")\n",
        "\n",
        "    # 1. USDT\n",
        "    col_usdt = \"log_V_SC_SC_USDT\"\n",
        "    print(f\"  - Analyzing {col_usdt}...\")\n",
        "    # Reuse the robust analysis function from Task 16\n",
        "    res_usdt = analyze_series_breaks_robust(df_indexed[col_usdt].dropna(), analysis_config)\n",
        "    print(f\"    -> Optimal k: {res_usdt.optimal_k}\")\n",
        "    print(f\"    -> Breaks at: {[d.date() for d in res_usdt.break_dates]}\")\n",
        "    print(f\"    -> SupF: {res_usdt.sup_f_stat:.4f}\")\n",
        "\n",
        "    # 2. USDC\n",
        "    col_usdc = \"log_V_SC_SC_USDC\"\n",
        "    print(f\"  - Analyzing {col_usdc}...\")\n",
        "    res_usdc = analyze_series_breaks_robust(df_indexed[col_usdc].dropna(), analysis_config)\n",
        "    print(f\"    -> Optimal k: {res_usdc.optimal_k}\")\n",
        "    print(f\"    -> Breaks at: {[d.date() for d in res_usdc.break_dates]}\")\n",
        "    print(f\"    -> SupF: {res_usdc.sup_f_stat:.4f}\")\n",
        "\n",
        "    return SCBaiPerronResults(\n",
        "        usdt_result=res_usdt,\n",
        "        usdc_result=res_usdc\n",
        "    )\n"
      ],
      "metadata": {
        "id": "M-W8BrVn-qVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Apply Bai–Perron to Exchange Volumes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Apply Bai–Perron to Exchange Volumes\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ExchangeBaiPerronResults:\n",
        "    \"\"\"\n",
        "    Container for the results of Bai-Perron tests on Exchange Volume series.\n",
        "\n",
        "    Attributes:\n",
        "        usdt_result (BreakPointResult): Results for USDT Exchange volume.\n",
        "        usdc_result (BreakPointResult): Results for USDC Exchange volume.\n",
        "    \"\"\"\n",
        "    usdt_result: Any # Type: BreakPointResult\n",
        "    usdc_result: Any # Type: BreakPointResult\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1 & 2: Apply to Exchange Series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# We reuse the analyze_series_breaks_robust function from Task 16.\n",
        "# No new low-level logic is needed, just the application to the new series.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_bai_perron_exchange(\n",
        "    finalized_data: Any,\n",
        "    analysis_config: Any\n",
        ") -> ExchangeBaiPerronResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the Bai-Perron structural break detection for Exchange Volume series.\n",
        "\n",
        "    Executes:\n",
        "    1. Extraction of USDT and USDC Exchange log-volume series.\n",
        "    2. Application of the mean-shift break detection algorithm.\n",
        "    3. Computation of SupF statistics.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset from Task 15.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        ExchangeBaiPerronResults: Container with results for both tokens.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "\n",
        "    # Ensure Date is index for easy mapping\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    print(\"Task 18: Running Bai-Perron on Exchange Volume series...\")\n",
        "\n",
        "    # 1. USDT\n",
        "    col_usdt = \"log_Volume_USDT_USD\"\n",
        "    print(f\"  - Analyzing {col_usdt}...\")\n",
        "    # Reuse the robust analysis function from Task 16\n",
        "    res_usdt = analyze_series_breaks_robust(df_indexed[col_usdt].dropna(), analysis_config)\n",
        "    print(f\"    -> Optimal k: {res_usdt.optimal_k}\")\n",
        "    print(f\"    -> Breaks at: {[d.date() for d in res_usdt.break_dates]}\")\n",
        "    print(f\"    -> SupF: {res_usdt.sup_f_stat:.4f}\")\n",
        "\n",
        "    # 2. USDC\n",
        "    col_usdc = \"log_Volume_USDC_USD\"\n",
        "    print(f\"  - Analyzing {col_usdc}...\")\n",
        "    res_usdc = analyze_series_breaks_robust(df_indexed[col_usdc].dropna(), analysis_config)\n",
        "    print(f\"    -> Optimal k: {res_usdc.optimal_k}\")\n",
        "    print(f\"    -> Breaks at: {[d.date() for d in res_usdc.break_dates]}\")\n",
        "    print(f\"    -> SupF: {res_usdc.sup_f_stat:.4f}\")\n",
        "\n",
        "    return ExchangeBaiPerronResults(\n",
        "        usdt_result=res_usdt,\n",
        "        usdc_result=res_usdc\n",
        "    )\n"
      ],
      "metadata": {
        "id": "i6qNeOh8C_ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Apply Bai–Perron to BTC and ETH Prices\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Apply Bai–Perron to BTC and ETH Prices\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PriceBaiPerronResults:\n",
        "    \"\"\"\n",
        "    Container for the results of Bai-Perron tests on Cryptocurrency Price series.\n",
        "\n",
        "    Attributes:\n",
        "        btc_result (BreakPointResult): Results for BTC Price.\n",
        "        eth_result (BreakPointResult): Results for ETH Price.\n",
        "    \"\"\"\n",
        "    btc_result: Any # Type: BreakPointResult\n",
        "    eth_result: Any # Type: BreakPointResult\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1 & 2: Apply to Price Series\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# We reuse the analyze_series_breaks_robust function from Task 16.\n",
        "# No new low-level logic is needed, just the application to the new series.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_bai_perron_prices(\n",
        "    finalized_data: Any,\n",
        "    analysis_config: Any\n",
        ") -> PriceBaiPerronResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the Bai-Perron structural break detection for BTC and ETH Price series.\n",
        "\n",
        "    Executes:\n",
        "    1. Extraction of BTC and ETH log-price series.\n",
        "    2. Application of the mean-shift break detection algorithm.\n",
        "    3. Computation of SupF statistics.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset from Task 15.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        PriceBaiPerronResults: Container with results for both assets.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "\n",
        "    # Ensure Date is index for easy mapping\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    print(\"Task 19: Running Bai-Perron on Price series...\")\n",
        "\n",
        "    # 1. BTC\n",
        "    col_btc = \"log_Close_BTC_USD\"\n",
        "    print(f\"  - Analyzing {col_btc}...\")\n",
        "    # Reuse the robust analysis function from Task 16\n",
        "    res_btc = analyze_series_breaks_robust(df_indexed[col_btc].dropna(), analysis_config)\n",
        "    print(f\"    -> Optimal k: {res_btc.optimal_k}\")\n",
        "    print(f\"    -> Breaks at: {[d.date() for d in res_btc.break_dates]}\")\n",
        "    print(f\"    -> SupF: {res_btc.sup_f_stat:.4f}\")\n",
        "\n",
        "    # 2. ETH\n",
        "    col_eth = \"log_Close_ETH_USD\"\n",
        "    print(f\"  - Analyzing {col_eth}...\")\n",
        "    res_eth = analyze_series_breaks_robust(df_indexed[col_eth].dropna(), analysis_config)\n",
        "    print(f\"    -> Optimal k: {res_eth.optimal_k}\")\n",
        "    print(f\"    -> Breaks at: {[d.date() for d in res_eth.break_dates]}\")\n",
        "    print(f\"    -> SupF: {res_eth.sup_f_stat:.4f}\")\n",
        "\n",
        "    return PriceBaiPerronResults(\n",
        "        btc_result=res_btc,\n",
        "        eth_result=res_eth\n",
        "    )\n"
      ],
      "metadata": {
        "id": "aAHmvp25EDz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 — Design Structural Break Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Design Structural Break Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class StructuralBreakOrchestratorResult:\n",
        "    \"\"\"\n",
        "    Container for the results of structural break detection across all series.\n",
        "\n",
        "    Attributes:\n",
        "        results (Dict[str, BreakPointResult]): Dictionary mapping series name to break results.\n",
        "        failed_series (Dict[str, str]): Dictionary mapping failed series names to error messages.\n",
        "    \"\"\"\n",
        "    results: Dict[str, Any] # Type: BreakPointResult\n",
        "    failed_series: Dict[str, str]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 1 & 2: Define Orchestrator Inputs and Internal Callables\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def callable_bai_perron(series: pd.Series, config: Any) -> Any:\n",
        "    \"\"\"\n",
        "    Internal callable to execute Bai-Perron analysis on a single series.\n",
        "\n",
        "    Wraps the robust analysis function from Task 16.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The time series.\n",
        "        config (ValidatedAnalysisConfig): Analysis configuration.\n",
        "\n",
        "    Returns:\n",
        "        BreakPointResult: The analysis result.\n",
        "    \"\"\"\n",
        "    # Reuse the robust implementation from Task 16\n",
        "    # We assume analyze_series_breaks_robust is available in the environment\n",
        "    return analyze_series_breaks_robust(series, config)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 3: Define Orchestrator Control Flow\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def orchestrate_structural_breaks(\n",
        "    finalized_data: Any,\n",
        "    analysis_config: Any\n",
        ") -> StructuralBreakOrchestratorResult:\n",
        "    \"\"\"\n",
        "    Orchestrates the structural break detection for all configured series.\n",
        "\n",
        "    Iterates through the series mapped for Bai-Perron analysis in Task 15,\n",
        "    executes the detection algorithm, and aggregates results.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset from Task 15.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        StructuralBreakOrchestratorResult: Aggregated results.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "\n",
        "    # Ensure Date index\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    # Get target series from mapping\n",
        "    target_series = finalized_data.series_mapping.bai_perron_series\n",
        "\n",
        "    results = {}\n",
        "    failures = {}\n",
        "\n",
        "    print(f\"Task 20: Orchestrating structural breaks for {len(target_series)} series...\")\n",
        "\n",
        "    for col in target_series:\n",
        "        if col not in df_indexed.columns:\n",
        "            failures[col] = \"Column missing from DataFrame\"\n",
        "            print(f\"  - WARNING: Series '{col}' missing.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(f\"  - Processing {col}...\")\n",
        "            series = df_indexed[col].dropna()\n",
        "\n",
        "            if len(series) == 0:\n",
        "                failures[col] = \"Series is empty after dropping NaNs\"\n",
        "                continue\n",
        "\n",
        "            # Execute Analysis\n",
        "            res = callable_bai_perron(series, analysis_config)\n",
        "            results[col] = res\n",
        "\n",
        "            # Log brief result\n",
        "            breaks_str = \", \".join([str(d.date()) for d in res.break_dates])\n",
        "            print(f\"    -> Found {len(res.break_dates)} breaks: {breaks_str}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failures[col] = str(e)\n",
        "            print(f\"    -> FAILED: {e}\")\n",
        "\n",
        "    print(f\"Task 20 Complete. Successful: {len(results)}, Failed: {len(failures)}\")\n",
        "\n",
        "    return StructuralBreakOrchestratorResult(\n",
        "        results=results,\n",
        "        failed_series=failures\n",
        "    )\n"
      ],
      "metadata": {
        "id": "-H-Zctb1FMzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 — Execute AAFT Robustness for Structural Breaks\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Execute AAFT Robustness for Structural Breaks\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class RobustnessResult:\n",
        "    \"\"\"\n",
        "    Container for the results of AAFT surrogate testing on a single series.\n",
        "\n",
        "    Attributes:\n",
        "        series_name (str): Name of the series tested.\n",
        "        observed_sup_f (float): The SupF statistic from the original series.\n",
        "        surrogate_sup_f_stats (List[float]): List of SupF statistics from surrogates.\n",
        "        p_value (float): Empirical p-value.\n",
        "        n_surrogates (int): Number of surrogates generated.\n",
        "    \"\"\"\n",
        "    series_name: str\n",
        "    observed_sup_f: float\n",
        "    surrogate_sup_f_stats: List[float]\n",
        "    p_value: float\n",
        "    n_surrogates: int\n",
        "\n",
        "@dataclass\n",
        "class AAFTRobustnessResults:\n",
        "    \"\"\"\n",
        "    Container for the results of AAFT robustness testing across all series.\n",
        "\n",
        "    Attributes:\n",
        "        results (Dict[str, RobustnessResult]): Dictionary mapping series name to robustness results.\n",
        "    \"\"\"\n",
        "    results: Dict[str, RobustnessResult]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Define AAFT Surrogate Generation Algorithm\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_aaft_surrogates(series: np.ndarray, n_surrogates: int, seed: Optional[int] = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates Amplitude Adjusted Fourier Transform (AAFT) surrogates.\n",
        "\n",
        "    Algorithm:\n",
        "    1. Create a Gaussian time series y(t) with the same rank structure as the original series x(t).\n",
        "    2. Compute the FFT of y(t).\n",
        "    3. Randomize the phases of the FFT coefficients while preserving the power spectrum.\n",
        "    4. Perform inverse FFT to get y'(t).\n",
        "    5. Remap the values of x(t) to the ranks of y'(t) to preserve the amplitude distribution.\n",
        "\n",
        "    Args:\n",
        "        series (np.ndarray): The original time series (1D array).\n",
        "        n_surrogates (int): Number of surrogates to generate.\n",
        "        seed (Optional[int]): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D array of shape (n_surrogates, len(series)) containing the surrogates.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    n = len(series)\n",
        "    surrogates = np.zeros((n_surrogates, n))\n",
        "\n",
        "    # Sort original series for remapping\n",
        "    sorted_original = np.sort(series)\n",
        "\n",
        "    # Rank ordering of original series (not strictly needed for AAFT step 1,\n",
        "    # but we need the sorted values for step 5)\n",
        "\n",
        "    for i in range(n_surrogates):\n",
        "        # Step 1: Gaussianization\n",
        "        # Generate Gaussian noise\n",
        "        white_noise = np.random.randn(n)\n",
        "\n",
        "        # Reorder Gaussian noise to match the rank order of the original series\n",
        "        # This creates a Gaussian series with the same temporal correlation structure (roughly)\n",
        "        # Actually, standard AAFT starts with random Gaussian noise, sorts it to match original ranks.\n",
        "        # Let's follow the standard Schreiber & Schmitz (1996) algorithm more precisely if needed,\n",
        "        # or the simple AAFT (Theiler et al. 1992).\n",
        "        # Theiler's AAFT:\n",
        "        # 1. Gaussianize: r(t) = sort(gaussian)[rank(x(t))]\n",
        "        # 2. Phase randomize r(t) -> s(t)\n",
        "        # 3. Remap: x'(t) = sort(x)[rank(s(t))]\n",
        "\n",
        "        # 1. Gaussianize\n",
        "        ranks_original = np.argsort(np.argsort(series)) # Ranks 0..n-1\n",
        "        gaussian_sorted = np.sort(np.random.randn(n))\n",
        "        y_t = gaussian_sorted[ranks_original]\n",
        "\n",
        "        # 2. Phase Randomization of y_t\n",
        "        # Use rfft for efficiency and symmetry handling\n",
        "        rfft_coeffs = np.fft.rfft(y_t)\n",
        "\n",
        "        # Random phases in [0, 2pi]\n",
        "        # We only randomize phases for positive frequencies (indices 1 to n//2)\n",
        "        # DC component (0) and Nyquist (if n is even) must remain real.\n",
        "        random_phases = np.random.uniform(0, 2*np.pi, len(rfft_coeffs))\n",
        "        random_phases[0] = 0 # DC component phase is 0\n",
        "        if n % 2 == 0:\n",
        "            random_phases[-1] = 0 # Nyquist phase is 0/pi (keep real)\n",
        "\n",
        "        # Apply phase shift: z' = |z| * exp(i * (angle(z) + phi))\n",
        "        magnitudes = np.abs(rfft_coeffs)\n",
        "        new_coeffs = magnitudes * np.exp(1j * random_phases)\n",
        "\n",
        "        # Inverse FFT\n",
        "        s_t = np.fft.irfft(new_coeffs, n=n)\n",
        "\n",
        "        # 3. Remap to original amplitude distribution\n",
        "        ranks_s = np.argsort(np.argsort(s_t))\n",
        "        surrogate = sorted_original[ranks_s]\n",
        "\n",
        "        surrogates[i, :] = surrogate\n",
        "\n",
        "    return surrogates\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 2 & 3: Generate Surrogates and Compute Statistics\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_surrogate_stats(\n",
        "    series: pd.Series,\n",
        "    observed_sup_f: float,\n",
        "    config: Any,\n",
        "    n_surrogates: int\n",
        ") -> RobustnessResult:\n",
        "    \"\"\"\n",
        "    Generates surrogates and computes the empirical p-value for the SupF statistic.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The original time series.\n",
        "        observed_sup_f (float): The observed SupF statistic.\n",
        "        config (ValidatedAnalysisConfig): Analysis configuration (for Bai-Perron params).\n",
        "        n_surrogates (int): Number of surrogates to generate.\n",
        "\n",
        "    Returns:\n",
        "        RobustnessResult: The robustness test results.\n",
        "    \"\"\"\n",
        "    y = series.values\n",
        "    T = len(y)\n",
        "\n",
        "    # Parameters for Bai-Perron\n",
        "    max_breaks = config.bp_max_breaks\n",
        "    epsilon = config.bp_trimming\n",
        "    min_size = int(np.floor(epsilon * T))\n",
        "\n",
        "    # Generate Surrogates\n",
        "    surrogates = generate_aaft_surrogates(y, n_surrogates, seed=42)\n",
        "\n",
        "    surrogate_stats = []\n",
        "\n",
        "    # We need to import the Bai-Perron logic.\n",
        "    # Assuming bai_perron_dp and compute_f_statistic are available from Task 16 context.\n",
        "    for i in range(n_surrogates):\n",
        "        y_surr = surrogates[i]\n",
        "\n",
        "        # Run DP\n",
        "        dp_res = bai_perron_dp(y_surr, max_breaks, min_size)\n",
        "\n",
        "        # Select optimal k (using same BIC logic)\n",
        "        best_k = select_optimal_model(y_surr, dp_res)\n",
        "\n",
        "        # Compute SupF (F-stat for 0 vs best_k)\n",
        "        ssr_0 = dp_res[0]['ssr']\n",
        "        ssr_k = dp_res[best_k]['ssr']\n",
        "\n",
        "        f_stat = compute_f_statistic(y_surr, ssr_0, ssr_k, best_k)\n",
        "        surrogate_stats.append(f_stat)\n",
        "\n",
        "    # Compute p-value\n",
        "    # p = count(surr >= obs) / N\n",
        "    count_exceed = sum(1 for s in surrogate_stats if s >= observed_sup_f)\n",
        "    p_value = count_exceed / n_surrogates\n",
        "\n",
        "    return RobustnessResult(\n",
        "        series_name=series.name,\n",
        "        observed_sup_f=observed_sup_f,\n",
        "        surrogate_sup_f_stats=surrogate_stats,\n",
        "        p_value=p_value,\n",
        "        n_surrogates=n_surrogates\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_aaft_robustness(\n",
        "    finalized_data: Any,\n",
        "    break_results: Any,\n",
        "    analysis_config: Any\n",
        ") -> AAFTRobustnessResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the AAFT robustness testing for all series with detected breaks.\n",
        "\n",
        "    Iterates through the results from Task 20 (StructuralBreakOrchestratorResult),\n",
        "    and for each series, runs the surrogate analysis.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset.\n",
        "        break_results (StructuralBreakOrchestratorResult): Results from Task 20.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration.\n",
        "\n",
        "    Returns:\n",
        "        AAFTRobustnessResults: Container with robustness results.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    results = {}\n",
        "    n_surrogates = 1000 # As per config/paper\n",
        "\n",
        "    print(f\"Task 21: Executing AAFT robustness tests ({n_surrogates} iterations)...\")\n",
        "\n",
        "    for series_name, res in break_results.results.items():\n",
        "        print(f\"  - Testing {series_name} (Observed SupF: {res.sup_f_stat:.4f})...\")\n",
        "\n",
        "        series = df_indexed[series_name].dropna()\n",
        "\n",
        "        # Execute AAFT\n",
        "        robustness = compute_surrogate_stats(\n",
        "            series,\n",
        "            res.sup_f_stat,\n",
        "            analysis_config,\n",
        "            n_surrogates\n",
        "        )\n",
        "\n",
        "        results[series_name] = robustness\n",
        "        print(f\"    -> p-value: {robustness.p_value:.4f}\")\n",
        "\n",
        "    return AAFTRobustnessResults(results=results)\n"
      ],
      "metadata": {
        "id": "xupqBCSEGU_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 — Apply EMD to Decompose BTC and ETH Log Prices\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Apply EMD to Decompose BTC and ETH Log Prices\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class EMDResult:\n",
        "    \"\"\"\n",
        "    Container for the results of Empirical Mode Decomposition on a single series.\n",
        "\n",
        "    Attributes:\n",
        "        series_name (str): Name of the decomposed series.\n",
        "        imfs (List[np.ndarray]): List of Intrinsic Mode Functions (arrays).\n",
        "        residue (np.ndarray): The final monotonic residue.\n",
        "        n_imfs (int): Number of IMFs extracted.\n",
        "        reconstruction_error (float): Max absolute error between original and sum(IMFs)+residue.\n",
        "    \"\"\"\n",
        "    series_name: str\n",
        "    imfs: List[np.ndarray]\n",
        "    residue: np.ndarray\n",
        "    n_imfs: int\n",
        "    reconstruction_error: float\n",
        "\n",
        "@dataclass\n",
        "class EMDDecompositionResults:\n",
        "    \"\"\"\n",
        "    Container for the EMD results of BTC and ETH prices.\n",
        "\n",
        "    Attributes:\n",
        "        btc_result (EMDResult): EMD results for BTC.\n",
        "        eth_result (EMDResult): EMD results for ETH.\n",
        "    \"\"\"\n",
        "    btc_result: EMDResult\n",
        "    eth_result: EMDResult\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Specify EMD Algorithm (Self-Contained Implementation)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_envelopes(y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs upper and lower envelopes using cubic spline interpolation of extrema.\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): The input signal.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: (upper_envelope, lower_envelope).\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "    x = np.arange(n)\n",
        "\n",
        "    # Find local maxima and minima\n",
        "    # argrelextrema returns indices\n",
        "    max_idx = argrelextrema(y, np.greater)[0]\n",
        "    min_idx = argrelextrema(y, np.less)[0]\n",
        "\n",
        "    # Add endpoints to ensure envelopes cover the full range\n",
        "    # We assume endpoints are extrema to bound the spline\n",
        "    if len(max_idx) == 0 or max_idx[0] != 0:\n",
        "        max_idx = np.r_[0, max_idx]\n",
        "    if len(max_idx) == 0 or max_idx[-1] != n - 1:\n",
        "        max_idx = np.r_[max_idx, n - 1]\n",
        "\n",
        "    if len(min_idx) == 0 or min_idx[0] != 0:\n",
        "        min_idx = np.r_[0, min_idx]\n",
        "    if len(min_idx) == 0 or min_idx[-1] != n - 1:\n",
        "        min_idx = np.r_[min_idx, n - 1]\n",
        "\n",
        "    # Interpolate\n",
        "    # CubicSpline requires strictly increasing x\n",
        "    upper_spline = CubicSpline(max_idx, y[max_idx], bc_type='natural')\n",
        "    lower_spline = CubicSpline(min_idx, y[min_idx], bc_type='natural')\n",
        "\n",
        "    return upper_spline(x), lower_spline(x)\n",
        "\n",
        "def is_monotonic(y: np.ndarray) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if a signal is monotonic (no local extrema).\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): The signal.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if monotonic.\n",
        "    \"\"\"\n",
        "    max_idx = argrelextrema(y, np.greater)[0]\n",
        "    min_idx = argrelextrema(y, np.less)[0]\n",
        "    return (len(max_idx) + len(min_idx)) == 0\n",
        "\n",
        "def sift_one_imf(\n",
        "    signal: np.ndarray,\n",
        "    sd_threshold: float = 0.2,\n",
        "    max_sifts: int = 100\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Performs the sifting process to extract one IMF.\n",
        "\n",
        "    Args:\n",
        "        signal (np.ndarray): The current residual signal.\n",
        "        sd_threshold (float): Standard deviation threshold for stopping sifting.\n",
        "        max_sifts (int): Maximum number of sifts allowed.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: (extracted_imf, remaining_signal).\n",
        "    \"\"\"\n",
        "    h = signal.copy()\n",
        "\n",
        "    for _ in range(max_sifts):\n",
        "        # Check if h is monotonic (residue)\n",
        "        if is_monotonic(h):\n",
        "            break\n",
        "\n",
        "        # Get envelopes\n",
        "        upper, lower = get_envelopes(h)\n",
        "        mean_env = (upper + lower) / 2.0\n",
        "\n",
        "        # Update\n",
        "        h_prev = h.copy()\n",
        "        h = h - mean_env\n",
        "\n",
        "        # Check convergence (Cauchy-type convergence)\n",
        "        # SD = sum((h_prev - h)^2) / sum(h_prev^2)\n",
        "        # Avoid division by zero\n",
        "        denom = np.sum(h_prev ** 2)\n",
        "        if denom == 0:\n",
        "            break\n",
        "\n",
        "        sd = np.sum((h_prev - h) ** 2) / denom\n",
        "\n",
        "        if sd < sd_threshold:\n",
        "            break\n",
        "\n",
        "    return h, signal - h\n",
        "\n",
        "def empirical_mode_decomposition(\n",
        "    series: np.ndarray,\n",
        "    max_imfs: int,\n",
        "    stop_sd: float\n",
        ") -> Tuple[List[np.ndarray], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Executes the Empirical Mode Decomposition algorithm.\n",
        "\n",
        "    Args:\n",
        "        series (np.ndarray): The input time series.\n",
        "        max_imfs (int): Maximum number of IMFs to extract.\n",
        "        stop_sd (float): Sifting stopping criterion.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], np.ndarray]: (List of IMFs, Residue).\n",
        "    \"\"\"\n",
        "    residue = series.copy()\n",
        "    imfs = []\n",
        "\n",
        "    for _ in range(max_imfs):\n",
        "        if is_monotonic(residue):\n",
        "            break\n",
        "\n",
        "        # Sift\n",
        "        imf, residue = sift_one_imf(residue, sd_threshold=stop_sd)\n",
        "\n",
        "        # Check if IMF is trivial (too small or constant)\n",
        "        if np.allclose(imf, 0):\n",
        "            break\n",
        "\n",
        "        imfs.append(imf)\n",
        "\n",
        "    return imfs, residue\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 2 & 3: Apply EMD to BTC and ETH\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def decompose_series(\n",
        "    series: pd.Series,\n",
        "    config: Any\n",
        ") -> EMDResult:\n",
        "    \"\"\"\n",
        "    Wraps the EMD algorithm for a specific pandas Series.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The log-price series.\n",
        "        config (ValidatedAnalysisConfig): Analysis configuration.\n",
        "\n",
        "    Returns:\n",
        "        EMDResult: The decomposition results.\n",
        "    \"\"\"\n",
        "    y = series.values\n",
        "\n",
        "    # Parameters\n",
        "    max_imfs = config.hht_max_imfs\n",
        "    # We assume a standard stop_sd if not in config, or hardcode 0.2 as per paper\n",
        "    stop_sd = 0.2\n",
        "\n",
        "    # Run EMD\n",
        "    imfs, residue = empirical_mode_decomposition(y, max_imfs, stop_sd)\n",
        "\n",
        "    # Verify reconstruction\n",
        "    reconstructed = sum(imfs) + residue\n",
        "    error = np.max(np.abs(y - reconstructed))\n",
        "\n",
        "    return EMDResult(\n",
        "        series_name=series.name,\n",
        "        imfs=imfs,\n",
        "        residue=residue,\n",
        "        n_imfs=len(imfs),\n",
        "        reconstruction_error=error\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def decompose_prices_emd(\n",
        "    finalized_data: Any,\n",
        "    analysis_config: Any\n",
        ") -> EMDDecompositionResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the EMD decomposition for BTC and ETH log-price series.\n",
        "\n",
        "    Executes:\n",
        "    1. Extraction of log-price series.\n",
        "    2. Application of EMD algorithm.\n",
        "    3. Validation of reconstruction.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset from Task 15.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        EMDDecompositionResults: Container with results for both assets.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "\n",
        "    # Ensure Date index\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    print(\"Task 22: Running EMD decomposition on Price series...\")\n",
        "\n",
        "    # 1. BTC\n",
        "    col_btc = \"log_Close_BTC_USD\"\n",
        "    print(f\"  - Decomposing {col_btc}...\")\n",
        "    res_btc = decompose_series(df_indexed[col_btc].dropna(), analysis_config)\n",
        "    print(f\"    -> Extracted {res_btc.n_imfs} IMFs. Reconstruction Error: {res_btc.reconstruction_error:.2e}\")\n",
        "\n",
        "    # 2. ETH\n",
        "    col_eth = \"log_Close_ETH_USD\"\n",
        "    print(f\"  - Decomposing {col_eth}...\")\n",
        "    res_eth = decompose_series(df_indexed[col_eth].dropna(), analysis_config)\n",
        "    print(f\"    -> Extracted {res_eth.n_imfs} IMFs. Reconstruction Error: {res_eth.reconstruction_error:.2e}\")\n",
        "\n",
        "    return EMDDecompositionResults(\n",
        "        btc_result=res_btc,\n",
        "        eth_result=res_eth\n",
        "    )\n"
      ],
      "metadata": {
        "id": "RWdwCZXOOL8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23 — Compute Hilbert Transform and Spectrum\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Compute Hilbert Transform and Spectrum\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class IMFAnalyticData:\n",
        "    \"\"\"\n",
        "    Container for the analytic signal properties of a single IMF.\n",
        "\n",
        "    Attributes:\n",
        "        imf_index (int): Index of the IMF (0-based).\n",
        "        analytic_signal (np.ndarray): The complex analytic signal z(t).\n",
        "        instantaneous_phase (np.ndarray): Unwrapped phase phi(t).\n",
        "        instantaneous_frequency (np.ndarray): Angular frequency omega(t) = dphi/dt.\n",
        "        instantaneous_amplitude (np.ndarray): Envelope A(t) = |z(t)|.\n",
        "    \"\"\"\n",
        "    imf_index: int\n",
        "    analytic_signal: np.ndarray\n",
        "    instantaneous_phase: np.ndarray\n",
        "    instantaneous_frequency: np.ndarray\n",
        "    instantaneous_amplitude: np.ndarray\n",
        "\n",
        "@dataclass\n",
        "class HilbertSpectrumResult:\n",
        "    \"\"\"\n",
        "    Container for the Hilbert Spectrum of a single series.\n",
        "\n",
        "    Attributes:\n",
        "        series_name (str): Name of the series.\n",
        "        imf_data (List[IMFAnalyticData]): Analytic properties for each IMF.\n",
        "        hilbert_spectrum (np.ndarray): 2D array H(t, omega) representing amplitude.\n",
        "        time_axis (np.ndarray): Time indices.\n",
        "        frequency_axis (np.ndarray): Frequency bin centers.\n",
        "    \"\"\"\n",
        "    series_name: str\n",
        "    imf_data: List[IMFAnalyticData]\n",
        "    hilbert_spectrum: np.ndarray\n",
        "    time_axis: np.ndarray\n",
        "    frequency_axis: np.ndarray\n",
        "\n",
        "@dataclass\n",
        "class HilbertTransformResults:\n",
        "    \"\"\"\n",
        "    Container for Hilbert Transform results for BTC and ETH.\n",
        "\n",
        "    Attributes:\n",
        "        btc_spectrum (HilbertSpectrumResult): Results for BTC.\n",
        "        eth_spectrum (HilbertSpectrumResult): Results for ETH.\n",
        "    \"\"\"\n",
        "    btc_spectrum: HilbertSpectrumResult\n",
        "    eth_spectrum: HilbertSpectrumResult\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 1 & 2: Apply Hilbert Transform and Compute Instantaneous Attributes\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_analytic_properties(imf: np.ndarray, idx: int) -> IMFAnalyticData:\n",
        "    \"\"\"\n",
        "    Computes the analytic signal, phase, frequency, and amplitude for a single IMF.\n",
        "\n",
        "    Uses scipy.signal.hilbert for the analytic signal.\n",
        "    Frequency is computed as the gradient of the unwrapped phase.\n",
        "\n",
        "    Args:\n",
        "        imf (np.ndarray): The Intrinsic Mode Function array.\n",
        "        idx (int): The index of the IMF.\n",
        "\n",
        "    Returns:\n",
        "        IMFAnalyticData: Container with computed properties.\n",
        "    \"\"\"\n",
        "    # 1. Analytic Signal\n",
        "    # scipy.signal.hilbert returns the analytic signal x(t) + iH(x(t))\n",
        "    analytic = hilbert(imf)\n",
        "\n",
        "    # 2. Instantaneous Amplitude\n",
        "    amplitude = np.abs(analytic)\n",
        "\n",
        "    # 3. Instantaneous Phase\n",
        "    # angle returns values in [-pi, pi]\n",
        "    phase = np.angle(analytic)\n",
        "    # Unwrap to avoid discontinuities\n",
        "    unwrapped_phase = np.unwrap(phase)\n",
        "\n",
        "    # 4. Instantaneous Frequency\n",
        "    # omega = d(phase)/dt\n",
        "    # We use central difference for gradient\n",
        "    frequency = np.gradient(unwrapped_phase)\n",
        "\n",
        "    # Normalize frequency?\n",
        "    # The derivative is in radians per sample (day).\n",
        "    # We keep it in radians/sample.\n",
        "    return IMFAnalyticData(\n",
        "        imf_index=idx,\n",
        "        analytic_signal=analytic,\n",
        "        instantaneous_phase=unwrapped_phase,\n",
        "        instantaneous_frequency=frequency,\n",
        "        instantaneous_amplitude=amplitude\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 3: Construct Hilbert Spectrum\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_spectrum(\n",
        "    imf_data_list: List[IMFAnalyticData],\n",
        "    n_time_points: int,\n",
        "    n_freq_bins: int = 100,\n",
        "    max_freq: float = np.pi\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs the Hilbert Spectrum H(t, omega).\n",
        "\n",
        "    Aggregates the amplitude of all IMFs into time-frequency bins.\n",
        "\n",
        "    Args:\n",
        "        imf_data_list (List[IMFAnalyticData]): List of analytic data for all IMFs.\n",
        "        n_time_points (int): Length of the time series.\n",
        "        n_freq_bins (int): Number of frequency bins.\n",
        "        max_freq (float): Maximum frequency to consider (default Nyquist = pi).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "            (spectrum_matrix, time_axis, freq_axis)\n",
        "            spectrum_matrix shape is (n_freq_bins, n_time_points).\n",
        "    \"\"\"\n",
        "    # Define frequency bins\n",
        "    freq_edges = np.linspace(0, max_freq, n_freq_bins + 1)\n",
        "    freq_centers = (freq_edges[:-1] + freq_edges[1:]) / 2\n",
        "\n",
        "    # Initialize spectrum (Frequency x Time)\n",
        "    # We sum amplitudes (or energy? Task says \"Hilbert spectrum... represents time-frequency distribution\")\n",
        "    # Usually H(t, w) is amplitude. Energy is H^2.\n",
        "    # We accumulate Amplitude here.\n",
        "    spectrum = np.zeros((n_freq_bins, n_time_points))\n",
        "\n",
        "    for t in range(n_time_points):\n",
        "        for imf_data in imf_data_list:\n",
        "            freq = imf_data.instantaneous_frequency[t]\n",
        "            amp = imf_data.instantaneous_amplitude[t]\n",
        "\n",
        "            # Find bin index\n",
        "            # We only care about positive frequencies up to Nyquist\n",
        "            if 0 <= freq < max_freq:\n",
        "                bin_idx = int(freq / max_freq * n_freq_bins)\n",
        "                if bin_idx < n_freq_bins:\n",
        "                    spectrum[bin_idx, t] += amp\n",
        "\n",
        "    return spectrum, np.arange(n_time_points), freq_centers\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_hilbert_spectrum(\n",
        "    emd_results: Any\n",
        ") -> HilbertTransformResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of Hilbert Transforms and Spectra for BTC and ETH.\n",
        "\n",
        "    Executes:\n",
        "    1. Analytic signal computation for each IMF.\n",
        "    2. Instantaneous phase/frequency/amplitude derivation.\n",
        "    3. Aggregation into Hilbert Spectrum.\n",
        "\n",
        "    Args:\n",
        "        emd_results (EMDDecompositionResults): Results from Task 22.\n",
        "\n",
        "    Returns:\n",
        "        HilbertTransformResults: Container with spectra for both assets.\n",
        "    \"\"\"\n",
        "    print(\"Task 23: Computing Hilbert Spectra...\")\n",
        "\n",
        "    def process_asset(emd_res: Any) -> HilbertSpectrumResult:\n",
        "        imfs = emd_res.imfs\n",
        "        n_samples = len(imfs[0]) if imfs else 0\n",
        "\n",
        "        # 1. Compute properties for each IMF\n",
        "        imf_data_list = []\n",
        "        for i, imf in enumerate(imfs):\n",
        "            data = compute_analytic_properties(imf, i)\n",
        "            imf_data_list.append(data)\n",
        "\n",
        "        # 2. Construct Spectrum\n",
        "        spectrum, t_axis, f_axis = construct_spectrum(imf_data_list, n_samples)\n",
        "\n",
        "        return HilbertSpectrumResult(\n",
        "            series_name=emd_res.series_name,\n",
        "            imf_data=imf_data_list,\n",
        "            hilbert_spectrum=spectrum,\n",
        "            time_axis=t_axis,\n",
        "            frequency_axis=f_axis\n",
        "        )\n",
        "\n",
        "    # Process BTC\n",
        "    print(\"  - Processing BTC...\")\n",
        "    btc_spec = process_asset(emd_results.btc_result)\n",
        "\n",
        "    # Process ETH\n",
        "    print(\"  - Processing ETH...\")\n",
        "    eth_spec = process_asset(emd_results.eth_result)\n",
        "\n",
        "    return HilbertTransformResults(\n",
        "        btc_spectrum=btc_spec,\n",
        "        eth_spectrum=eth_spec\n",
        "    )\n"
      ],
      "metadata": {
        "id": "hvhDkgt6P9zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24 — Compute Instantaneous Energy and Detect Extreme Events\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Compute Instantaneous Energy and Detect Extreme Events\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ExtremeEventDetectionResult:\n",
        "    \"\"\"\n",
        "    Container for the results of extreme event detection on a single series.\n",
        "\n",
        "    Attributes:\n",
        "        series_name (str): Name of the series.\n",
        "        instantaneous_energy (np.ndarray): The unnormalized energy IE(t).\n",
        "        normalized_energy (np.ndarray): The normalized energy IE_N(t).\n",
        "        mean_energy (float): E_mu.\n",
        "        std_energy (float): sigma.\n",
        "        threshold (float): E_th = E_mu + B * sigma.\n",
        "        extreme_event_dates (List[pd.Timestamp]): Dates where IE(t) > E_th.\n",
        "        extreme_event_indices (List[int]): Indices where IE(t) > E_th.\n",
        "    \"\"\"\n",
        "    series_name: str\n",
        "    instantaneous_energy: np.ndarray\n",
        "    normalized_energy: np.ndarray\n",
        "    mean_energy: float\n",
        "    std_energy: float\n",
        "    threshold: float\n",
        "    extreme_event_dates: List[pd.Timestamp]\n",
        "    extreme_event_indices: List[int]\n",
        "\n",
        "@dataclass\n",
        "class ExtremeEventResults:\n",
        "    \"\"\"\n",
        "    Container for extreme event results for BTC and ETH.\n",
        "\n",
        "    Attributes:\n",
        "        btc_events (ExtremeEventDetectionResult): Results for BTC.\n",
        "        eth_events (ExtremeEventDetectionResult): Results for ETH.\n",
        "    \"\"\"\n",
        "    btc_events: ExtremeEventDetectionResult\n",
        "    eth_events: ExtremeEventDetectionResult\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 1: Compute Instantaneous Energy\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_instantaneous_energy(\n",
        "    spectrum: np.ndarray,\n",
        "    freq_axis: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the instantaneous energy IE(t) from the Hilbert Spectrum.\n",
        "\n",
        "    IE(t) = sum(H(t, w)^2 * dw)\n",
        "\n",
        "    Args:\n",
        "        spectrum (np.ndarray): The Hilbert Spectrum H(t, w) (Freq x Time).\n",
        "        freq_axis (np.ndarray): The frequency bin centers.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The instantaneous energy array (1D, length T).\n",
        "    \"\"\"\n",
        "    # Calculate bin width (assuming uniform spacing)\n",
        "    if len(freq_axis) > 1:\n",
        "        dw = freq_axis[1] - freq_axis[0]\n",
        "    else:\n",
        "        dw = 1.0 # Fallback if single bin\n",
        "\n",
        "    # Square the amplitude spectrum to get energy density\n",
        "    energy_density = spectrum ** 2\n",
        "\n",
        "    # Integrate over frequency (sum * dw)\n",
        "    # spectrum shape is (n_freq, n_time)\n",
        "    # Sum along axis 0 (frequency)\n",
        "    ie_t = np.sum(energy_density, axis=0) * dw\n",
        "\n",
        "    return ie_t\n",
        "\n",
        "def normalize_energy(ie_t: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Normalizes the instantaneous energy by its maximum value.\n",
        "\n",
        "    IE_N(t) = IE(t) / max(IE(t))\n",
        "\n",
        "    Args:\n",
        "        ie_t (np.ndarray): Instantaneous energy.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Normalized energy.\n",
        "    \"\"\"\n",
        "    max_val = np.max(ie_t)\n",
        "    if max_val == 0:\n",
        "        return np.zeros_like(ie_t)\n",
        "    return ie_t / max_val\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 2: Compute Extreme Event Threshold\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_threshold(ie_t: np.ndarray, b_param: float) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Computes the statistical threshold for extreme events.\n",
        "\n",
        "    E_th = E_mu + B * sigma\n",
        "\n",
        "    Args:\n",
        "        ie_t (np.ndarray): Instantaneous energy.\n",
        "        b_param (float): The multiplier B (e.g., 4).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float, float]: (mean, std, threshold).\n",
        "    \"\"\"\n",
        "    mean_val = np.mean(ie_t)\n",
        "    std_val = np.std(ie_t, ddof=1) # Sample standard deviation\n",
        "    threshold = mean_val + b_param * std_val\n",
        "\n",
        "    return mean_val, std_val, threshold\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 3: Identify Extreme Events\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def find_extreme_events(\n",
        "    ie_t: np.ndarray,\n",
        "    threshold: float,\n",
        "    date_index: pd.Index\n",
        ") -> Tuple[List[int], List[pd.Timestamp]]:\n",
        "    \"\"\"\n",
        "    Identifies indices and dates where energy exceeds the threshold.\n",
        "\n",
        "    Args:\n",
        "        ie_t (np.ndarray): Instantaneous energy.\n",
        "        threshold (float): The energy threshold.\n",
        "        date_index (pd.Index): The DatetimeIndex corresponding to the time axis.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[int], List[pd.Timestamp]]: (indices, dates).\n",
        "    \"\"\"\n",
        "    # Find indices\n",
        "    indices = np.where(ie_t > threshold)[0].tolist()\n",
        "\n",
        "    # Map to dates\n",
        "    dates = [date_index[i] for i in indices]\n",
        "\n",
        "    return indices, dates\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def detect_extreme_events(\n",
        "    hilbert_results: Any,\n",
        "    analysis_config: Any,\n",
        "    date_index: pd.Index\n",
        ") -> ExtremeEventResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the detection of extreme events for BTC and ETH.\n",
        "\n",
        "    Executes:\n",
        "    1. Energy computation.\n",
        "    2. Threshold calculation.\n",
        "    3. Event identification.\n",
        "\n",
        "    Args:\n",
        "        hilbert_results (HilbertTransformResults): Results from Task 23.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration (for B param).\n",
        "        date_index (pd.Index): The DatetimeIndex for mapping.\n",
        "\n",
        "    Returns:\n",
        "        ExtremeEventResults: Container with results for both assets.\n",
        "    \"\"\"\n",
        "    print(\"Task 24: Detecting Extreme Events...\")\n",
        "\n",
        "    b_param = analysis_config.hht_threshold_b\n",
        "\n",
        "    def process_asset(spectrum_res: Any) -> ExtremeEventDetectionResult:\n",
        "        spectrum = spectrum_res.hilbert_spectrum\n",
        "        freq_axis = spectrum_res.frequency_axis\n",
        "\n",
        "        # 1. Compute Energy\n",
        "        ie_t = compute_instantaneous_energy(spectrum, freq_axis)\n",
        "        ie_n = normalize_energy(ie_t)\n",
        "\n",
        "        # 2. Compute Threshold\n",
        "        mean_val, std_val, threshold = compute_threshold(ie_t, b_param)\n",
        "\n",
        "        # 3. Detect Events\n",
        "        indices, dates = find_extreme_events(ie_t, threshold, date_index)\n",
        "\n",
        "        return ExtremeEventDetectionResult(\n",
        "            series_name=spectrum_res.series_name,\n",
        "            instantaneous_energy=ie_t,\n",
        "            normalized_energy=ie_n,\n",
        "            mean_energy=mean_val,\n",
        "            std_energy=std_val,\n",
        "            threshold=threshold,\n",
        "            extreme_event_dates=dates,\n",
        "            extreme_event_indices=indices\n",
        "        )\n",
        "\n",
        "    # Process BTC\n",
        "    print(\"  - Processing BTC...\")\n",
        "    btc_res = process_asset(hilbert_results.btc_spectrum)\n",
        "    print(f\"    -> Threshold: {btc_res.threshold:.4f}\")\n",
        "    print(f\"    -> Found {len(btc_res.extreme_event_dates)} events: {[d.date() for d in btc_res.extreme_event_dates]}\")\n",
        "\n",
        "    # Process ETH\n",
        "    print(\"  - Processing ETH...\")\n",
        "    eth_res = process_asset(hilbert_results.eth_spectrum)\n",
        "    print(f\"    -> Threshold: {eth_res.threshold:.4f}\")\n",
        "    print(f\"    -> Found {len(eth_res.extreme_event_dates)} events: {[d.date() for d in eth_res.extreme_event_dates]}\")\n",
        "\n",
        "    return ExtremeEventResults(\n",
        "        btc_events=btc_res,\n",
        "        eth_events=eth_res\n",
        "    )\n"
      ],
      "metadata": {
        "id": "kVNv_DthRofs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25 — Design HHT Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Design HHT Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class HHTOrchestratorResult:\n",
        "    \"\"\"\n",
        "    Container for the results of HHT analysis across all series.\n",
        "\n",
        "    Attributes:\n",
        "        results (Dict[str, Dict[str, Any]]): Dictionary mapping series name to HHT results.\n",
        "        failed_series (Dict[str, str]): Dictionary mapping failed series names to error messages.\n",
        "    \"\"\"\n",
        "    results: Dict[str, Any] # Type: Dict containing EMD, Hilbert, and EE results\n",
        "    failed_series: Dict[str, str]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 1 & 2: Define Orchestrator Inputs and Internal Callables\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def callable_emd(series: pd.Series, config: Any) -> Any:\n",
        "    \"\"\"\n",
        "    Internal callable to execute EMD decomposition on a single series.\n",
        "\n",
        "    Wraps the decomposition function from Task 22.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The time series.\n",
        "        config (ValidatedAnalysisConfig): Analysis configuration.\n",
        "\n",
        "    Returns:\n",
        "        EMDResult: The decomposition result.\n",
        "    \"\"\"\n",
        "    # Reuse the implementation from Task 22\n",
        "    return decompose_series(series, config)\n",
        "\n",
        "def callable_hilbert_spectrum(emd_res: Any) -> Any:\n",
        "    \"\"\"\n",
        "    Internal callable to compute Hilbert Spectrum for a single series.\n",
        "\n",
        "    Wraps the spectrum construction logic from Task 23.\n",
        "\n",
        "    Args:\n",
        "        emd_res (EMDResult): The decomposition result.\n",
        "\n",
        "    Returns:\n",
        "        HilbertSpectrumResult: The spectrum result.\n",
        "    \"\"\"\n",
        "    # Reuse logic from Task 23\n",
        "    # We need to adapt compute_hilbert_spectrum which processes both assets at once\n",
        "    imfs = emd_res.imfs\n",
        "    n_samples = len(imfs[0]) if imfs else 0\n",
        "\n",
        "    # 1. Compute properties for each IMF\n",
        "    imf_data_list = []\n",
        "    for i, imf in enumerate(imfs):\n",
        "        data = compute_analytic_properties(imf, i)\n",
        "        imf_data_list.append(data)\n",
        "\n",
        "    # 2. Construct Spectrum\n",
        "    spectrum, t_axis, f_axis = construct_spectrum(imf_data_list, n_samples)\n",
        "\n",
        "    return HilbertSpectrumResult(\n",
        "        series_name=emd_res.series_name,\n",
        "        imf_data=imf_data_list,\n",
        "        hilbert_spectrum=spectrum,\n",
        "        time_axis=t_axis,\n",
        "        frequency_axis=f_axis\n",
        "    )\n",
        "\n",
        "def callable_extreme_events(spectrum_res: Any, config: Any, date_index: pd.Index) -> Any:\n",
        "    \"\"\"\n",
        "    Internal callable to detect extreme events for a single series.\n",
        "\n",
        "    Wraps the detection logic from Task 24.\n",
        "\n",
        "    Args:\n",
        "        spectrum_res (HilbertSpectrumResult): The spectrum result.\n",
        "        config (ValidatedAnalysisConfig): Analysis configuration.\n",
        "        date_index (pd.Index): The DatetimeIndex.\n",
        "\n",
        "    Returns:\n",
        "        ExtremeEventDetectionResult: The detection result.\n",
        "    \"\"\"\n",
        "    # Reuse logic from Task 24\n",
        "    b_param = config.hht_threshold_b\n",
        "\n",
        "    spectrum = spectrum_res.hilbert_spectrum\n",
        "    freq_axis = spectrum_res.frequency_axis\n",
        "\n",
        "    # 1. Compute Energy\n",
        "    ie_t = compute_instantaneous_energy(spectrum, freq_axis)\n",
        "    ie_n = normalize_energy(ie_t)\n",
        "\n",
        "    # 2. Compute Threshold\n",
        "    mean_val, std_val, threshold = compute_threshold(ie_t, b_param)\n",
        "\n",
        "    # 3. Detect Events\n",
        "    indices, dates = find_extreme_events(ie_t, threshold, date_index)\n",
        "\n",
        "    return ExtremeEventDetectionResult(\n",
        "        series_name=spectrum_res.series_name,\n",
        "        instantaneous_energy=ie_t,\n",
        "        normalized_energy=ie_n,\n",
        "        mean_energy=mean_val,\n",
        "        std_energy=std_val,\n",
        "        threshold=threshold,\n",
        "        extreme_event_dates=dates,\n",
        "        extreme_event_indices=indices\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 3: Define Orchestrator Control Flow\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def orchestrate_hht(\n",
        "    finalized_data: Any,\n",
        "    analysis_config: Any\n",
        ") -> HHTOrchestratorResult:\n",
        "    \"\"\"\n",
        "    Orchestrates the HHT analysis for all configured series.\n",
        "\n",
        "    Iterates through the series mapped for HHT analysis in Task 15,\n",
        "    executes EMD, Hilbert Transform, and Extreme Event Detection, and aggregates results.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset from Task 15.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration from Task 1.\n",
        "\n",
        "    Returns:\n",
        "        HHTOrchestratorResult: Aggregated results.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "\n",
        "    # Ensure Date index\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    # Get target series from mapping\n",
        "    target_series = finalized_data.series_mapping.hht_series\n",
        "\n",
        "    results = {}\n",
        "    failures = {}\n",
        "\n",
        "    print(f\"Task 25: Orchestrating HHT for {len(target_series)} series...\")\n",
        "\n",
        "    for col in target_series:\n",
        "        if col not in df_indexed.columns:\n",
        "            failures[col] = \"Column missing from DataFrame\"\n",
        "            print(f\"  - WARNING: Series '{col}' missing.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(f\"  - Processing {col}...\")\n",
        "            series = df_indexed[col].dropna()\n",
        "\n",
        "            if len(series) == 0:\n",
        "                failures[col] = \"Series is empty after dropping NaNs\"\n",
        "                continue\n",
        "\n",
        "            # 1. EMD\n",
        "            emd_res = callable_emd(series, analysis_config)\n",
        "\n",
        "            # 2. Hilbert Spectrum\n",
        "            hs_res = callable_hilbert_spectrum(emd_res)\n",
        "\n",
        "            # 3. Extreme Events\n",
        "            ee_res = callable_extreme_events(hs_res, analysis_config, series.index)\n",
        "\n",
        "            results[col] = {\n",
        "                \"emd\": emd_res,\n",
        "                \"spectrum\": hs_res,\n",
        "                \"events\": ee_res\n",
        "            }\n",
        "\n",
        "            # Log brief result\n",
        "            print(f\"    -> Found {len(ee_res.extreme_event_dates)} extreme events.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failures[col] = str(e)\n",
        "            print(f\"    -> FAILED: {e}\")\n",
        "\n",
        "    print(f\"Task 25 Complete. Successful: {len(results)}, Failed: {len(failures)}\")\n",
        "\n",
        "    return HHTOrchestratorResult(\n",
        "        results=results,\n",
        "        failed_series=failures\n",
        "    )\n"
      ],
      "metadata": {
        "id": "SjpY1mQoTGhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26 — Execute AAFT Robustness for HHT Extreme Events\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Execute AAFT Robustness for HHT Extreme Events\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class HHTRobustnessResult:\n",
        "    \"\"\"\n",
        "    Container for the results of AAFT robustness testing on HHT extreme events.\n",
        "\n",
        "    Attributes:\n",
        "        series_name (str): Name of the series tested.\n",
        "        observed_max_energy (float): The maximum instantaneous energy from the original series.\n",
        "        surrogate_max_energies (List[float]): List of max energies from surrogates.\n",
        "        p_value (float): Empirical p-value for the maximum energy peak.\n",
        "        n_surrogates (int): Number of surrogates generated.\n",
        "    \"\"\"\n",
        "    series_name: str\n",
        "    observed_max_energy: float\n",
        "    surrogate_max_energies: List[float]\n",
        "    p_value: float\n",
        "    n_surrogates: int\n",
        "\n",
        "@dataclass\n",
        "class HHTRobustnessResults:\n",
        "    \"\"\"\n",
        "    Container for HHT robustness results for BTC and ETH.\n",
        "\n",
        "    Attributes:\n",
        "        btc_robustness (HHTRobustnessResult): Results for BTC.\n",
        "        eth_robustness (HHTRobustnessResult): Results for ETH.\n",
        "    \"\"\"\n",
        "    btc_robustness: HHTRobustnessResult\n",
        "    eth_robustness: HHTRobustnessResult\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 1 & 2: Generate Surrogates and Compute Energy Maxima\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_surrogate_maxima(\n",
        "    series: pd.Series,\n",
        "    config: Any,\n",
        "    n_surrogates: int\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Generates AAFT surrogates and computes the maximum instantaneous energy for each.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The original time series.\n",
        "        config (ValidatedAnalysisConfig): Analysis configuration.\n",
        "        n_surrogates (int): Number of surrogates.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: List of maximum energy values from surrogates.\n",
        "    \"\"\"\n",
        "    y = series.values\n",
        "\n",
        "    # Generate Surrogates (Reusing Task 21 function)\n",
        "    # We assume generate_aaft_surrogates is available in the environment\n",
        "    surrogates = generate_aaft_surrogates(y, n_surrogates, seed=42)\n",
        "\n",
        "    max_energies = []\n",
        "\n",
        "    # EMD Parameters\n",
        "    max_imfs = config.hht_max_imfs\n",
        "    stop_sd = 0.2\n",
        "\n",
        "    for i in range(n_surrogates):\n",
        "        y_surr = surrogates[i]\n",
        "\n",
        "        # 1. EMD\n",
        "        # Reusing empirical_mode_decomposition from Task 22\n",
        "        imfs, _ = empirical_mode_decomposition(y_surr, max_imfs, stop_sd)\n",
        "\n",
        "        if not imfs:\n",
        "            max_energies.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # 2. Hilbert Transform & Energy\n",
        "        # We need to compute energy without storing the full spectrum for efficiency\n",
        "        # IE(t) = sum(A_j(t)^2 * w_j(t)?) No, IE(t) = sum(H(t, w)^2 dw)\n",
        "        n_samples = len(y_surr)\n",
        "        n_freq_bins = 100\n",
        "        max_freq = np.pi\n",
        "        spectrum = np.zeros((n_freq_bins, n_samples))\n",
        "\n",
        "        freq_edges = np.linspace(0, max_freq, n_freq_bins + 1)\n",
        "        dw = freq_edges[1] - freq_edges[0]\n",
        "\n",
        "        for j, imf in enumerate(imfs):\n",
        "            # Analytic signal (Task 23 logic)\n",
        "            # Reusing compute_analytic_properties logic inline for speed\n",
        "            analytic = hilbert(imf)\n",
        "            amp = np.abs(analytic)\n",
        "            phase = np.unwrap(np.angle(analytic))\n",
        "            freq = np.gradient(phase)\n",
        "\n",
        "            # Binning\n",
        "            # Vectorized binning\n",
        "            valid_mask = (freq >= 0) & (freq < max_freq)\n",
        "            bin_indices = (freq[valid_mask] / max_freq * n_freq_bins).astype(int)\n",
        "            # Clip to be safe\n",
        "            bin_indices = np.clip(bin_indices, 0, n_freq_bins - 1)\n",
        "\n",
        "            # Accumulate\n",
        "            # We can't easily vectorize the accumulation into 2D array with numpy without ufunc.at\n",
        "            # np.add.at(spectrum, (bin_indices, np.where(valid_mask)[0]), amp[valid_mask])\n",
        "            np.add.at(spectrum, (bin_indices, np.where(valid_mask)[0]), amp[valid_mask])\n",
        "\n",
        "        # 3. Compute Energy (Task 24 logic)\n",
        "        energy_density = spectrum ** 2\n",
        "        ie_t = np.sum(energy_density, axis=0) * dw\n",
        "\n",
        "        # 4. Max Energy\n",
        "        max_energies.append(np.max(ie_t))\n",
        "\n",
        "    return max_energies\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 3: Compute Empirical p-Values\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_hht_p_value(\n",
        "    observed_max: float,\n",
        "    surrogate_maxima: List[float]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the empirical p-value for the observed maximum energy.\n",
        "\n",
        "    Args:\n",
        "        observed_max (float): Observed max energy.\n",
        "        surrogate_maxima (List[float]): List of surrogate max energies.\n",
        "\n",
        "    Returns:\n",
        "        float: p-value.\n",
        "    \"\"\"\n",
        "    n = len(surrogate_maxima)\n",
        "    if n == 0:\n",
        "        return 1.0\n",
        "\n",
        "    count_exceed = sum(1 for m in surrogate_maxima if m >= observed_max)\n",
        "    return count_exceed / n\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_hht_robustness(\n",
        "    finalized_data: Any,\n",
        "    hht_results: Any,\n",
        "    analysis_config: Any\n",
        ") -> HHTRobustnessResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the AAFT robustness testing for HHT extreme events.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset.\n",
        "        hht_results (HHTOrchestratorResult): Results from Task 25 (containing EE results).\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration.\n",
        "\n",
        "    Returns:\n",
        "        HHTRobustnessResults: Container with robustness results.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    n_surrogates = 1000 # As per config\n",
        "\n",
        "    print(f\"Task 26: Executing HHT robustness tests ({n_surrogates} iterations)...\")\n",
        "\n",
        "    def process_asset(series_name: str, observed_max: float) -> HHTRobustnessResult:\n",
        "        print(f\"  - Testing {series_name} (Observed Max Energy: {observed_max:.4f})...\")\n",
        "\n",
        "        series = df_indexed[series_name].dropna()\n",
        "\n",
        "        # Generate and Compute\n",
        "        surrogate_maxima = compute_surrogate_maxima(series, analysis_config, n_surrogates)\n",
        "\n",
        "        # Compute p-value\n",
        "        p_val = calculate_hht_p_value(observed_max, surrogate_maxima)\n",
        "\n",
        "        print(f\"    -> p-value: {p_val:.4f}\")\n",
        "\n",
        "        return HHTRobustnessResult(\n",
        "            series_name=series_name,\n",
        "            observed_max_energy=observed_max,\n",
        "            surrogate_max_energies=surrogate_maxima,\n",
        "            p_value=p_val,\n",
        "            n_surrogates=n_surrogates\n",
        "        )\n",
        "\n",
        "    # Extract observed max from HHT results\n",
        "    # We need to access the nested structure from Task 25\n",
        "    # results['log_Close_BTC_USD']['events'] -> ExtremeEventDetectionResult\n",
        "    # BTC\n",
        "    btc_name = \"log_Close_BTC_USD\"\n",
        "    btc_events = hht_results.results[btc_name][\"events\"]\n",
        "    btc_max = np.max(btc_events.instantaneous_energy)\n",
        "    btc_res = process_asset(btc_name, btc_max)\n",
        "\n",
        "    # ETH\n",
        "    eth_name = \"log_Close_ETH_USD\"\n",
        "    eth_events = hht_results.results[eth_name][\"events\"]\n",
        "    eth_max = np.max(eth_events.instantaneous_energy)\n",
        "    eth_res = process_asset(eth_name, eth_max)\n",
        "\n",
        "    return HHTRobustnessResults(\n",
        "        btc_robustness=btc_res,\n",
        "        eth_robustness=eth_res\n",
        "    )\n"
      ],
      "metadata": {
        "id": "3I9fHaupUfKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27 — Prepare Stationary SVAR Input Series\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Prepare Stationary SVAR Input Series\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SVARInputData:\n",
        "    \"\"\"\n",
        "    Container for the stationary time series used in SVAR analysis.\n",
        "\n",
        "    Attributes:\n",
        "        df_full (pd.DataFrame): Full sample of stationary series (differenced log volumes).\n",
        "        df_pre (pd.DataFrame): Pre-election subsample.\n",
        "        df_post (pd.DataFrame): Post-election subsample.\n",
        "        variables (List[str]): List of variable names in the vector Y_t.\n",
        "        break_date (pd.Timestamp): The date used to split the sample.\n",
        "    \"\"\"\n",
        "    df_full: pd.DataFrame\n",
        "    df_pre: pd.DataFrame\n",
        "    df_post: pd.DataFrame\n",
        "    variables: List[str]\n",
        "    break_date: pd.Timestamp\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 1: Select Differenced Series for SVAR\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def select_svar_series(\n",
        "    df: pd.DataFrame,\n",
        "    target_cols: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Selects and cleans the differenced series for SVAR analysis.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing differenced series.\n",
        "        target_cols (List[str]): List of column names for the SVAR (e.g., diff_log_V_EOA_EOA_USDC).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A clean DataFrame with no NaNs, containing only target columns.\n",
        "    \"\"\"\n",
        "    # Select columns\n",
        "    subset = df[target_cols].copy()\n",
        "\n",
        "    # Drop NaNs (e.g., first row from differencing)\n",
        "    clean_subset = subset.dropna()\n",
        "\n",
        "    # Ensure Date is preserved in index if it was in index, or set it\n",
        "    # Assuming df has Date index from previous steps\n",
        "\n",
        "    return clean_subset\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 2 & 3: Define Regime Boundaries and Construct Vectors\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def split_regimes(\n",
        "    df: pd.DataFrame,\n",
        "    break_date: pd.Timestamp\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits the dataset into pre- and post-election regimes.\n",
        "\n",
        "    Pre-election: Date < break_date\n",
        "    Post-election: Date >= break_date\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The clean SVAR DataFrame (index must be DatetimeIndex).\n",
        "        break_date (pd.Timestamp): The regime shift date.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: Dictionary with 'pre' and 'post' DataFrames.\n",
        "    \"\"\"\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise ValueError(\"DataFrame index must be DatetimeIndex for splitting.\")\n",
        "\n",
        "    # Split\n",
        "    mask_pre = df.index < break_date\n",
        "    mask_post = df.index >= break_date\n",
        "\n",
        "    df_pre = df.loc[mask_pre].copy()\n",
        "    df_post = df.loc[mask_post].copy()\n",
        "\n",
        "    return {\n",
        "        \"pre\": df_pre,\n",
        "        \"post\": df_post\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_svar_inputs(\n",
        "    finalized_data: Any,\n",
        "    analysis_config: Any\n",
        ") -> SVARInputData:\n",
        "    \"\"\"\n",
        "    Orchestrates the preparation of stationary inputs for SVAR.\n",
        "\n",
        "    Executes:\n",
        "    1. Selection of differenced EOA-EOA volume series.\n",
        "    2. Cleaning of missing values.\n",
        "    3. Splitting into pre- and post-election samples.\n",
        "\n",
        "    Args:\n",
        "        finalized_data (FinalizedSeriesData): The dataset from Task 15.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration (for break date).\n",
        "\n",
        "    Returns:\n",
        "        SVARInputData: Container with full, pre, and post datasets.\n",
        "    \"\"\"\n",
        "    df = finalized_data.df_final\n",
        "\n",
        "    # Ensure Date index\n",
        "    if \"Date\" in df.columns:\n",
        "        df_indexed = df.set_index(\"Date\")\n",
        "    else:\n",
        "        df_indexed = df\n",
        "\n",
        "    # Identify target variables\n",
        "    # Let's look for \"diff_log_V_EOA_EOA_USDC\" and \"diff_log_V_EOA_EOA_USDT\".\n",
        "    target_vars = [\n",
        "        \"diff_log_V_EOA_EOA_USDC\",\n",
        "        \"diff_log_V_EOA_EOA_USDT\"\n",
        "    ]\n",
        "\n",
        "    # Verify existence\n",
        "    missing = [c for c in target_vars if c not in df_indexed.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing SVAR target variables: {missing}\")\n",
        "\n",
        "    print(f\"Task 27: Preparing SVAR inputs for {target_vars}...\")\n",
        "\n",
        "    # 1. Select and Clean\n",
        "    df_clean = select_svar_series(df_indexed, target_vars)\n",
        "\n",
        "    # 2. Split\n",
        "    break_date = analysis_config.svar_break_date\n",
        "    splits = split_regimes(df_clean, break_date)\n",
        "\n",
        "    print(f\"  - Full Sample: {len(df_clean)} rows\")\n",
        "    print(f\"  - Pre-Election (< {break_date.date()}): {len(splits['pre'])} rows\")\n",
        "    print(f\"  - Post-Election (>= {break_date.date()}): {len(splits['post'])} rows\")\n",
        "\n",
        "    return SVARInputData(\n",
        "        df_full=df_clean,\n",
        "        df_pre=splits[\"pre\"],\n",
        "        df_post=splits[\"post\"],\n",
        "        variables=target_vars,\n",
        "        break_date=break_date\n",
        "    )\n"
      ],
      "metadata": {
        "id": "goGYVD86WLZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28 — Estimate Reduced-Form VAR with Lag Selection\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 28: Estimate Reduced-Form VAR with Lag Selection\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class VARModelResults:\n",
        "    \"\"\"\n",
        "    Container for the estimated reduced-form VAR model.\n",
        "\n",
        "    Attributes:\n",
        "        selected_lag (int): The optimal lag order p* selected via AIC.\n",
        "        full_model_results (VARResultsWrapper): The fitted model object from statsmodels.\n",
        "        aic_value (float): The AIC of the selected model.\n",
        "        resid_cov (np.ndarray): The residual covariance matrix (Sigma_u).\n",
        "        coefficients (pd.DataFrame): Estimated coefficients (Phi matrices and intercept).\n",
        "    \"\"\"\n",
        "    selected_lag: int\n",
        "    full_model_results: VARResultsWrapper\n",
        "    aic_value: float\n",
        "    resid_cov: np.ndarray\n",
        "    coefficients: pd.DataFrame\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 1: Specify VAR Model\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def initialize_var_model(df: pd.DataFrame) -> VAR:\n",
        "    \"\"\"\n",
        "    Initializes the VAR model structure using statsmodels.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The stationary multivariate time series.\n",
        "\n",
        "    Returns:\n",
        "        VAR: The initialized VAR model instance.\n",
        "    \"\"\"\n",
        "    # Ensure no NaNs (Task 27 should have handled this, but defensive check)\n",
        "    if df.isnull().any().any():\n",
        "        raise ValueError(\"Input data for VAR contains NaNs.\")\n",
        "\n",
        "    model = VAR(df)\n",
        "    return model\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 2: Select Lag Order via AIC\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def select_optimal_lag(model: VAR, max_lags: int) -> int:\n",
        "    \"\"\"\n",
        "    Selects the optimal lag order based on the Akaike Information Criterion (AIC).\n",
        "\n",
        "    Args:\n",
        "        model (VAR): The initialized VAR model.\n",
        "        max_lags (int): Maximum number of lags to check.\n",
        "\n",
        "    Returns:\n",
        "        int: The optimal lag order p*.\n",
        "    \"\"\"\n",
        "    # select_order returns a LagOrderResults object\n",
        "    lag_results = model.select_order(maxlags=max_lags)\n",
        "\n",
        "    # Extract the selected lag for AIC\n",
        "    selected_lag = lag_results.aic\n",
        "\n",
        "    return selected_lag\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Step 3: Estimate Full-Sample VAR\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def fit_var_model(model: VAR, lag_order: int) -> VARResultsWrapper:\n",
        "    \"\"\"\n",
        "    Estimates the VAR model parameters using OLS given the lag order.\n",
        "\n",
        "    Args:\n",
        "        model (VAR): The initialized VAR model.\n",
        "        lag_order (int): The lag order to use.\n",
        "\n",
        "    Returns:\n",
        "        VARResultsWrapper: The fitted model results.\n",
        "    \"\"\"\n",
        "    results = model.fit(lag_order)\n",
        "    return results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 28, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_reduced_var(\n",
        "    svar_input: Any,\n",
        "    analysis_config: Any\n",
        ") -> VARModelResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of the reduced-form VAR model.\n",
        "\n",
        "    Executes:\n",
        "    1. Model initialization.\n",
        "    2. Lag selection via AIC.\n",
        "    3. Estimation on the full sample.\n",
        "\n",
        "    Args:\n",
        "        svar_input (SVARInputData): The input data from Task 27.\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration (for max_lags).\n",
        "\n",
        "    Returns:\n",
        "        VARModelResults: Container with fitted model and stats.\n",
        "    \"\"\"\n",
        "    df = svar_input.df_full\n",
        "    max_lags = analysis_config.svar_max_lags\n",
        "\n",
        "    print(f\"Task 28: Estimating VAR on full sample ({len(df)} obs)...\")\n",
        "\n",
        "    # Step 1: Initialize\n",
        "    model = initialize_var_model(df)\n",
        "\n",
        "    # Step 2: Select Lag\n",
        "    selected_lag = select_optimal_lag(model, max_lags)\n",
        "    print(f\"  - Selected Lag (AIC): {selected_lag}\")\n",
        "\n",
        "    # Step 3: Fit\n",
        "    results = fit_var_model(model, selected_lag)\n",
        "\n",
        "    # Extract key metrics\n",
        "    aic = results.aic\n",
        "    sigma_u = results.sigma_u.values\n",
        "    coeffs = results.params\n",
        "\n",
        "    print(f\"  - Model AIC: {aic:.4f}\")\n",
        "\n",
        "    return VARModelResults(\n",
        "        selected_lag=selected_lag,\n",
        "        full_model_results=results,\n",
        "        aic_value=aic,\n",
        "        resid_cov=sigma_u,\n",
        "        coefficients=coeffs\n",
        "    )\n"
      ],
      "metadata": {
        "id": "SU-oQKGRXf_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 29 — Estimate VAR on Pre- and Post-Election Subsamples\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 29: Estimate VAR on Pre- and Post-Election Subsamples\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class RegimeVARResult:\n",
        "    \"\"\"\n",
        "    Container for VAR estimation results on a specific regime (subsample).\n",
        "\n",
        "    Attributes:\n",
        "        regime_name (str): 'Pre-Election' or 'Post-Election'.\n",
        "        n_obs (int): Number of observations in the subsample.\n",
        "        coefficients (pd.DataFrame): Estimated coefficient matrix (K x Kp+1).\n",
        "        resid_cov (np.ndarray): Residual covariance matrix (Sigma_u).\n",
        "        theta_vector (np.ndarray): Vectorized coefficients (aligned with covariance).\n",
        "        theta_cov (np.ndarray): Covariance matrix of the coefficients.\n",
        "    \"\"\"\n",
        "    regime_name: str\n",
        "    n_obs: int\n",
        "    coefficients: pd.DataFrame\n",
        "    resid_cov: np.ndarray\n",
        "    theta_vector: np.ndarray\n",
        "    theta_cov: np.ndarray\n",
        "\n",
        "@dataclass\n",
        "class RegimeComparisonData:\n",
        "    \"\"\"\n",
        "    Container for the comparative analysis of pre- and post-election regimes.\n",
        "\n",
        "    Attributes:\n",
        "        pre_election_results (RegimeVARResult): Results for the pre-election period.\n",
        "        post_election_results (RegimeVARResult): Results for the post-election period.\n",
        "        lag_order (int): The lag order used for both regimes.\n",
        "    \"\"\"\n",
        "    pre_election_results: RegimeVARResult\n",
        "    post_election_results: RegimeVARResult\n",
        "    lag_order: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Step 1 & 2: Estimate VAR on Subsamples\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_regime_var(\n",
        "    df: pd.DataFrame,\n",
        "    lag_order: int,\n",
        "    regime_name: str\n",
        ") -> RegimeVARResult:\n",
        "    \"\"\"\n",
        "    Estimates a VAR model on a specific subsample.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The subsample data.\n",
        "        lag_order (int): The lag order p*.\n",
        "        regime_name (str): Label for the regime.\n",
        "\n",
        "    Returns:\n",
        "        RegimeVARResult: The estimation results.\n",
        "    \"\"\"\n",
        "    # Initialize and fit\n",
        "    model = VAR(df)\n",
        "    results = model.fit(lag_order)\n",
        "\n",
        "    # Extract coefficients\n",
        "    # params is (Kp + 1) x K DataFrame (constants + lags)\n",
        "    coeffs = results.params\n",
        "\n",
        "    # Extract residual covariance\n",
        "    sigma_u = results.sigma_u.values\n",
        "\n",
        "    # Extract vectorized parameters and their covariance\n",
        "    # statsmodels cov_params() returns the covariance of the flattened parameters\n",
        "    # The default flattening order in statsmodels for VAR is column-major (F-order),\n",
        "    # meaning equation by equation.\n",
        "    # We must flatten params in the same order to align theta with theta_cov.\n",
        "    theta_vector = coeffs.values.flatten(order='F')\n",
        "    theta_cov = results.cov_params().values\n",
        "\n",
        "    return RegimeVARResult(\n",
        "        regime_name=regime_name,\n",
        "        n_obs=len(df),\n",
        "        coefficients=coeffs,\n",
        "        resid_cov=sigma_u,\n",
        "        theta_vector=theta_vector,\n",
        "        theta_cov=theta_cov\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Step 3: Vectorize Coefficients for Comparison\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# This step is integrated into estimate_regime_var via flattening.\n",
        "# We ensure consistency by using order='F' which matches statsmodels cov_params structure.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 29, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_regime_vars(\n",
        "    svar_input: Any,\n",
        "    full_model_results: Any\n",
        ") -> RegimeComparisonData:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of VAR models for pre- and post-election regimes.\n",
        "\n",
        "    Executes:\n",
        "    1. Estimation on pre-election sample.\n",
        "    2. Estimation on post-election sample.\n",
        "    3. Packaging of vectorized coefficients for Wald testing.\n",
        "\n",
        "    Args:\n",
        "        svar_input (SVARInputData): The input data from Task 27.\n",
        "        full_model_results (VARModelResults): The full model results from Task 28 (for lag order).\n",
        "\n",
        "    Returns:\n",
        "        RegimeComparisonData: Container with both regime results.\n",
        "    \"\"\"\n",
        "    lag_order = full_model_results.selected_lag\n",
        "    print(f\"Task 29: Estimating Regime VARs with lag p={lag_order}...\")\n",
        "\n",
        "    # 1. Pre-Election\n",
        "    print(\"  - Estimating Pre-Election VAR...\")\n",
        "    pre_res = estimate_regime_var(svar_input.df_pre, lag_order, \"Pre-Election\")\n",
        "\n",
        "    # 2. Post-Election\n",
        "    print(\"  - Estimating Post-Election VAR...\")\n",
        "    post_res = estimate_regime_var(svar_input.df_post, lag_order, \"Post-Election\")\n",
        "\n",
        "    # Verify dimensions\n",
        "    if pre_res.theta_vector.shape != post_res.theta_vector.shape:\n",
        "        raise ValueError(\"Parameter vector dimensions mismatch between regimes.\")\n",
        "\n",
        "    return RegimeComparisonData(\n",
        "        pre_election_results=pre_res,\n",
        "        post_election_results=post_res,\n",
        "        lag_order=lag_order\n",
        "    )\n"
      ],
      "metadata": {
        "id": "IXLhZQxOZ3n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 30 — Apply Cholesky Identification to Obtain SVAR\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 30: Apply Cholesky Identification to Obtain SVAR\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class StructuralImpactMatrix:\n",
        "    \"\"\"\n",
        "    Container for the structural impact matrix (Cholesky factor) for a specific regime and ordering.\n",
        "\n",
        "    Attributes:\n",
        "        regime_name (str): 'Pre-Election' or 'Post-Election'.\n",
        "        ordering (List[str]): The variable ordering used for identification.\n",
        "        impact_matrix (pd.DataFrame): The lower triangular impact matrix P (A0_inv).\n",
        "                                      Rows: Response variables. Cols: Structural shocks.\n",
        "        covariance_matrix (pd.DataFrame): The permuted residual covariance matrix used for decomposition.\n",
        "        is_positive_definite (bool): True if Cholesky decomposition succeeded.\n",
        "    \"\"\"\n",
        "    regime_name: str\n",
        "    ordering: List[str]\n",
        "    impact_matrix: pd.DataFrame\n",
        "    covariance_matrix: pd.DataFrame\n",
        "    is_positive_definite: bool\n",
        "\n",
        "@dataclass\n",
        "class SVARIdentificationResults:\n",
        "    \"\"\"\n",
        "    Container for SVAR identification results across regimes and orderings.\n",
        "\n",
        "    Attributes:\n",
        "        results (List[StructuralImpactMatrix]): List of impact matrices for all combinations.\n",
        "    \"\"\"\n",
        "    results: List[StructuralImpactMatrix]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Step 1 & 2: Apply Cholesky Decomposition\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_cholesky_impact(\n",
        "    resid_cov: np.ndarray,\n",
        "    variables: List[str],\n",
        "    ordering: List[str],\n",
        "    regime_name: str\n",
        ") -> StructuralImpactMatrix:\n",
        "    \"\"\"\n",
        "    Computes the structural impact matrix via Cholesky decomposition for a given ordering.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Maps the desired identification ordering to the indices of the reduced-form covariance matrix.\n",
        "    2. Permutes the covariance matrix to match the identification ordering.\n",
        "    3. Performs Cholesky decomposition (Sigma = P * P') to obtain the lower triangular impact matrix P.\n",
        "    4. Wraps the result in a labeled DataFrame.\n",
        "\n",
        "    Args:\n",
        "        resid_cov (np.ndarray): The residual covariance matrix (in original variable order).\n",
        "        variables (List[str]): The list of variables corresponding to resid_cov indices.\n",
        "        ordering (List[str]): The desired recursive ordering for identification.\n",
        "        regime_name (str): Name of the regime (e.g., \"Pre-Election\").\n",
        "\n",
        "    Returns:\n",
        "        StructuralImpactMatrix: The identified structural model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If ordering variables do not match model variables.\n",
        "    \"\"\"\n",
        "    # 1. Reorder Covariance Matrix\n",
        "    # Map variable names to indices in the original covariance matrix\n",
        "    var_to_idx = {v: i for i, v in enumerate(variables)}\n",
        "\n",
        "    # Get permutation indices\n",
        "    try:\n",
        "        perm_indices = [var_to_idx[v] for v in ordering]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Ordering variable not found in model variables: {e}\")\n",
        "\n",
        "    # Permute rows and columns\n",
        "    # Sigma_perm = P_mat * Sigma * P_mat.T\n",
        "    # We use numpy indexing for efficient permutation\n",
        "    # resid_cov[np.ix_(rows, cols)] selects the submatrix\n",
        "    cov_perm = resid_cov[np.ix_(perm_indices, perm_indices)]\n",
        "\n",
        "    # 2. Cholesky Decomposition\n",
        "    # Sigma = L L.T\n",
        "    # L is lower triangular\n",
        "    try:\n",
        "        L = np.linalg.cholesky(cov_perm)\n",
        "        is_pd = True\n",
        "    except np.linalg.LinAlgError:\n",
        "        # Handle non-positive definite matrices gracefully (though unlikely for well-specified VAR)\n",
        "        # We return a matrix of NaNs to indicate failure without crashing the pipeline\n",
        "        L = np.full_like(cov_perm, np.nan)\n",
        "        is_pd = False\n",
        "        print(f\"WARNING: Covariance matrix for {regime_name} with ordering {ordering} is not positive definite.\")\n",
        "\n",
        "    # 3. Package Result\n",
        "    # Create DataFrame for clarity\n",
        "    # Rows: Response Variables (in ordering)\n",
        "    # Cols: Structural Shocks (corresponding to variables in ordering)\n",
        "    impact_df = pd.DataFrame(\n",
        "        L,\n",
        "        index=ordering,\n",
        "        columns=[f\"Shock_{v}\" for v in ordering]\n",
        "    )\n",
        "\n",
        "    cov_df = pd.DataFrame(\n",
        "        cov_perm,\n",
        "        index=ordering,\n",
        "        columns=ordering\n",
        "    )\n",
        "\n",
        "    return StructuralImpactMatrix(\n",
        "        regime_name=regime_name,\n",
        "        ordering=ordering,\n",
        "        impact_matrix=impact_df,\n",
        "        covariance_matrix=cov_df,\n",
        "        is_positive_definite=is_pd\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Step 3: Extract Impact Matrices (Integrated in Step 2)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# The extraction logic is part of the compute_cholesky_impact function which returns the full matrix.\n",
        "# Analysis of specific elements (diagonal vs off-diagonal) is done in the orchestrator or downstream reporting.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 30, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def identify_structural_shocks(\n",
        "    regime_data: Any,\n",
        "    svar_input: Any,\n",
        "    analysis_config: Any\n",
        ") -> SVARIdentificationResults:\n",
        "    \"\"\"\n",
        "    Orchestrates the SVAR identification via Cholesky decomposition.\n",
        "\n",
        "    Iterates through:\n",
        "    1. Regimes (Pre-Election, Post-Election).\n",
        "    2. Identification Orderings (from config).\n",
        "\n",
        "    Args:\n",
        "        regime_data (RegimeComparisonData): Results from Task 29.\n",
        "        svar_input (SVARInputData): Input data (for variable names).\n",
        "        analysis_config (ValidatedAnalysisConfig): Configuration (for orderings).\n",
        "        # Note: ValidatedAnalysisConfig doesn't store orderings directly in the dataclass\n",
        "        # defined in Task 1 (it stores scalar params). We need to access the raw config\n",
        "        # or assume standard orderings. The prompt's Task 1 ValidatedAnalysisConfig\n",
        "        # didn't include orderings list. I will access the raw STUDY_CONFIG passed\n",
        "        # or define standard orderings here if not available.\n",
        "        # Correction: Task 1 ValidatedAnalysisConfig indeed missed the list.\n",
        "        # I will accept the raw config dictionary or a list of orderings as an argument.\n",
        "        # Let's assume we pass the raw 'svar' config dict or list of lists.\n",
        "\n",
        "    Returns:\n",
        "        SVARIdentificationResults: Container with all impact matrices.\n",
        "    \"\"\"\n",
        "    # Define orderings based on variables\n",
        "    vars_full = svar_input.variables\n",
        "\n",
        "    # Heuristic mapping: find which full var contains the short name\n",
        "    # This assumes distinct names\n",
        "    name_map = {}\n",
        "    for short in [\"USDC\", \"USDT\"]:\n",
        "        matches = [v for v in vars_full if short in v]\n",
        "        if len(matches) == 1:\n",
        "            name_map[short] = matches[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Ambiguous or missing variable mapping for {short}: {matches}\")\n",
        "\n",
        "    # Define standard orderings if not provided\n",
        "    # Ordering 1: USDC -> USDT\n",
        "    ord1 = [name_map[\"USDC\"], name_map[\"USDT\"]]\n",
        "    # Ordering 2: USDT -> USDC\n",
        "    ord2 = [name_map[\"USDT\"], name_map[\"USDC\"]]\n",
        "\n",
        "    orderings = [ord1, ord2]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(\"Task 30: Identifying Structural Shocks...\")\n",
        "\n",
        "    # Process Regimes\n",
        "    regimes = [\n",
        "        regime_data.pre_election_results,\n",
        "        regime_data.post_election_results\n",
        "    ]\n",
        "\n",
        "    for regime in regimes:\n",
        "        print(f\"  - Processing {regime.regime_name}...\")\n",
        "\n",
        "        for i, ordering in enumerate(orderings):\n",
        "            # Compute Cholesky\n",
        "            impact = compute_cholesky_impact(\n",
        "                regime.resid_cov,\n",
        "                vars_full,\n",
        "                ordering,\n",
        "                regime.regime_name\n",
        "            )\n",
        "            results.append(impact)\n",
        "\n",
        "            if impact.is_positive_definite:\n",
        "                # Log diagonal elements (Own shocks)\n",
        "                diag = np.diag(impact.impact_matrix.values)\n",
        "\n",
        "                print(f\"    -> Ordering {i+1}: Own-Shock Impacts = {diag}\")\n",
        "            else:\n",
        "                print(f\"    -> Ordering {i+1}: Identification Failed (Non-PD)\")\n",
        "\n",
        "    return SVARIdentificationResults(results=results)\n"
      ],
      "metadata": {
        "id": "mrsvQL8SbWiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 31 — Compute Wald Statistic for Regime Comparison\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 31: Compute Wald Statistic for Regime Comparison\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class WaldTestResult:\n",
        "    \"\"\"\n",
        "    Container for the results of the Wald test for structural change.\n",
        "\n",
        "    Attributes:\n",
        "        wald_statistic (float): The computed Wald statistic W.\n",
        "        degrees_of_freedom (int): The number of parameters tested (k).\n",
        "        p_value (float): The p-value from the Chi-squared distribution.\n",
        "        is_significant (bool): True if p-value < 0.05.\n",
        "        parameter_difference (np.ndarray): The vector difference (theta_post - theta_pre).\n",
        "    \"\"\"\n",
        "    wald_statistic: float\n",
        "    degrees_of_freedom: int\n",
        "    p_value: float\n",
        "    is_significant: bool\n",
        "    parameter_difference: np.ndarray\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 31, Step 1 & 2: Compute Wald Statistic\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_wald_stat(\n",
        "    theta_pre: np.ndarray,\n",
        "    cov_pre: np.ndarray,\n",
        "    theta_post: np.ndarray,\n",
        "    cov_post: np.ndarray\n",
        ") -> Tuple[float, np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Computes the Wald statistic for testing the equality of parameters across regimes.\n",
        "\n",
        "    W = (theta_post - theta_pre)' (Cov_pre + Cov_post)^(-1) (theta_post - theta_pre)\n",
        "\n",
        "    Args:\n",
        "        theta_pre (np.ndarray): Vectorized parameters for pre-election regime.\n",
        "        cov_pre (np.ndarray): Covariance matrix for pre-election parameters.\n",
        "        theta_post (np.ndarray): Vectorized parameters for post-election regime.\n",
        "        cov_post (np.ndarray): Covariance matrix for post-election parameters.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, np.ndarray, int]: (Wald statistic, difference vector, degrees of freedom).\n",
        "    \"\"\"\n",
        "    # 1. Compute Difference\n",
        "    delta_theta = theta_post - theta_pre\n",
        "\n",
        "    # 2. Compute Combined Covariance\n",
        "    # Under the null of no change, and assuming independence of samples (pre/post),\n",
        "    # Var(delta_theta) = Var(theta_post) + Var(theta_pre)\n",
        "    V = cov_pre + cov_post\n",
        "\n",
        "    # 3. Compute Wald Statistic\n",
        "    # W = delta' V^-1 delta\n",
        "    # Use solve for stability: x = V^-1 delta => V x = delta\n",
        "    try:\n",
        "        x = np.linalg.solve(V, delta_theta)\n",
        "        wald_stat = np.dot(delta_theta, x)\n",
        "    except np.linalg.LinAlgError:\n",
        "        # Fallback to pseudo-inverse if V is singular (unlikely for full rank VAR)\n",
        "        print(\"WARNING: Covariance matrix singular, using pseudo-inverse for Wald test.\")\n",
        "        V_inv = np.linalg.pinv(V)\n",
        "        wald_stat = delta_theta.T @ V_inv @ delta_theta\n",
        "\n",
        "    k = len(delta_theta)\n",
        "\n",
        "    return wald_stat, delta_theta, k\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 31, Step 3: Compute p-Value and Interpret\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_chi2_p_value(wald_stat: float, df: int) -> float:\n",
        "    \"\"\"\n",
        "    Computes the p-value from the Chi-squared distribution.\n",
        "\n",
        "    Args:\n",
        "        wald_stat (float): The Wald statistic.\n",
        "        df (int): Degrees of freedom.\n",
        "\n",
        "    Returns:\n",
        "        float: The p-value (survival function).\n",
        "    \"\"\"\n",
        "    # sf = 1 - cdf\n",
        "    return chi2.sf(wald_stat, df)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 31, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def perform_wald_test(\n",
        "    regime_data: Any\n",
        ") -> WaldTestResult:\n",
        "    \"\"\"\n",
        "    Orchestrates the Wald test for structural change between pre- and post-election regimes.\n",
        "\n",
        "    Executes:\n",
        "    1. Extraction of parameter vectors and covariances.\n",
        "    2. Computation of Wald statistic.\n",
        "    3. Computation of p-value.\n",
        "\n",
        "    Args:\n",
        "        regime_data (RegimeComparisonData): Results from Task 29.\n",
        "\n",
        "    Returns:\n",
        "        WaldTestResult: The test results.\n",
        "    \"\"\"\n",
        "    print(\"Task 31: Performing Wald Test for Regime Change...\")\n",
        "\n",
        "    pre = regime_data.pre_election_results\n",
        "    post = regime_data.post_election_results\n",
        "\n",
        "    # 1. Compute Statistic\n",
        "    wald_stat, delta, k = calculate_wald_stat(\n",
        "        pre.theta_vector,\n",
        "        pre.theta_cov,\n",
        "        post.theta_vector,\n",
        "        post.theta_cov\n",
        "    )\n",
        "\n",
        "    # 2. Compute p-value\n",
        "    p_val = compute_chi2_p_value(wald_stat, k)\n",
        "\n",
        "    is_sig = p_val < 0.05\n",
        "\n",
        "    print(f\"  - Wald Statistic: {wald_stat:.4f}\")\n",
        "    print(f\"  - Degrees of Freedom: {k}\")\n",
        "    print(f\"  - p-value: {p_val:.4e}\")\n",
        "    print(f\"  - Significant Change: {is_sig}\")\n",
        "\n",
        "    return WaldTestResult(\n",
        "        wald_statistic=wald_stat,\n",
        "        degrees_of_freedom=k,\n",
        "        p_value=p_val,\n",
        "        is_significant=is_sig,\n",
        "        parameter_difference=delta\n",
        "    )\n"
      ],
      "metadata": {
        "id": "y4pNMQeDdiT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 32 — Synthesize Results and Validate Against Expected Findings\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 32: Synthesize Results and Validate Against Expected Findings\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ValidationItem:\n",
        "    \"\"\"\n",
        "    Container for a single validation check.\n",
        "\n",
        "    Attributes:\n",
        "        metric_name (str): Name of the metric/event being validated.\n",
        "        observed_value (Any): The value computed by the pipeline.\n",
        "        expected_value (Any): The value expected from the paper.\n",
        "        status (str): 'PASS', 'FAIL', or 'APPROX'.\n",
        "        comment (str): Additional context.\n",
        "    \"\"\"\n",
        "    metric_name: str\n",
        "    observed_value: Any\n",
        "    expected_value: Any\n",
        "    status: str\n",
        "    comment: str\n",
        "\n",
        "@dataclass\n",
        "class FinalStudyReport:\n",
        "    \"\"\"\n",
        "    Master container for the study's final validation report.\n",
        "\n",
        "    Attributes:\n",
        "        structural_breaks (List[ValidationItem]): Validation of break dates and significance.\n",
        "        extreme_events (List[ValidationItem]): Validation of HHT events.\n",
        "        svar_regime_shift (List[ValidationItem]): Validation of Wald test and impact matrices.\n",
        "        overall_conclusion (str): Summary of the replication success.\n",
        "    \"\"\"\n",
        "    structural_breaks: List[ValidationItem]\n",
        "    extreme_events: List[ValidationItem]\n",
        "    svar_regime_shift: List[ValidationItem]\n",
        "    overall_conclusion: str\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Step 1: Verify Structural Break Timeline\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_breaks(\n",
        "    break_results: Any,\n",
        "    aaft_results: Any\n",
        ") -> List[ValidationItem]:\n",
        "    \"\"\"\n",
        "    Validates the detected structural breaks against expected dates.\n",
        "\n",
        "    Expected:\n",
        "    - EOA-EOA: 2024-11-03\n",
        "    - Exchange: 2024-11-05\n",
        "    - ETH: 2024-11-06\n",
        "    - BTC: 2024-11-09\n",
        "    - SC-SC USDC: 2025-01-02\n",
        "    - SC-SC USDT: 2025-01-16\n",
        "\n",
        "    Args:\n",
        "        break_results (StructuralBreakOrchestratorResult): Detection results.\n",
        "        aaft_results (AAFTRobustnessResults): Robustness results.\n",
        "\n",
        "    Returns:\n",
        "        List[ValidationItem]: List of validation checks.\n",
        "    \"\"\"\n",
        "    items = []\n",
        "\n",
        "    # Define expectations\n",
        "    expectations = {\n",
        "        \"log_V_EOA_EOA_USDT\": \"2024-11-03\",\n",
        "        \"log_V_EOA_EOA_USDC\": \"2024-11-03\",\n",
        "        \"log_Volume_USDT_USD\": \"2024-11-05\",\n",
        "        \"log_Volume_USDC_USD\": \"2024-11-05\",\n",
        "        \"log_Close_ETH_USD\": \"2024-11-06\",\n",
        "        \"log_Close_BTC_USD\": \"2024-11-09\",\n",
        "        \"log_V_SC_SC_USDC\": \"2025-01-02\",\n",
        "        \"log_V_SC_SC_USDT\": \"2025-01-16\"\n",
        "    }\n",
        "\n",
        "    for series, exp_date_str in expectations.items():\n",
        "        if series not in break_results.results:\n",
        "            items.append(ValidationItem(series, \"Missing\", exp_date_str, \"FAIL\", \"Series not processed\"))\n",
        "            continue\n",
        "\n",
        "        res = break_results.results[series]\n",
        "        dates = [d.date() for d in res.break_dates]\n",
        "\n",
        "        # Check if expected date is in detected dates (allowing +/- 2 days)\n",
        "        exp_date = pd.Timestamp(exp_date_str).date()\n",
        "        match_found = False\n",
        "        obs_date = None\n",
        "\n",
        "        for d in dates:\n",
        "            delta = abs((d - exp_date).days)\n",
        "            if delta <= 2:\n",
        "                match_found = True\n",
        "                obs_date = d\n",
        "                break\n",
        "\n",
        "        # Check significance\n",
        "        p_val = aaft_results.results[series].p_value\n",
        "        sig_status = \"Significant\" if p_val < 0.001 else f\"Not Significant (p={p_val})\"\n",
        "\n",
        "        status = \"PASS\" if match_found and p_val < 0.001 else \"FAIL\"\n",
        "        comment = f\"Observed: {dates}. {sig_status}.\"\n",
        "\n",
        "        items.append(ValidationItem(\n",
        "            metric_name=f\"Break Date: {series}\",\n",
        "            observed_value=str(obs_date) if obs_date else \"None\",\n",
        "            expected_value=exp_date_str,\n",
        "            status=status,\n",
        "            comment=comment\n",
        "        ))\n",
        "\n",
        "    return items\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Step 2: Verify HHT Extreme Events\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_hht(\n",
        "    hht_results: Any,\n",
        "    hht_robust_res: Any\n",
        ") -> List[ValidationItem]:\n",
        "    \"\"\"\n",
        "    Validates HHT extreme event detection.\n",
        "\n",
        "    Expected:\n",
        "    - ETH: ~2024-11-07\n",
        "    - BTC: ~2024-11-10\n",
        "    - Significance: p < 0.001\n",
        "\n",
        "    Args:\n",
        "        hht_results (HHTOrchestratorResult): Detection results.\n",
        "        hht_robust_res (HHTRobustnessResults): Robustness results.\n",
        "\n",
        "    Returns:\n",
        "        List[ValidationItem]: Validation checks.\n",
        "    \"\"\"\n",
        "    items = []\n",
        "\n",
        "    # BTC\n",
        "    btc_name = \"log_Close_BTC_USD\"\n",
        "    if btc_name in hht_results.results:\n",
        "        events = hht_results.results[btc_name][\"events\"]\n",
        "        dates = [d.date() for d in events.extreme_event_dates]\n",
        "        p_val = hht_robust_res.btc_robustness.p_value\n",
        "\n",
        "        # Check for date near Nov 10\n",
        "        exp_date = pd.Timestamp(\"2024-11-10\").date()\n",
        "        match = any(abs((d - exp_date).days) <= 3 for d in dates)\n",
        "\n",
        "        status = \"PASS\" if match and p_val < 0.001 else \"FAIL\"\n",
        "        items.append(ValidationItem(\n",
        "            metric_name=\"HHT Event: BTC\",\n",
        "            observed_value=str(dates),\n",
        "            expected_value=\"~2024-11-10\",\n",
        "            status=status,\n",
        "            comment=f\"p-value: {p_val}\"\n",
        "        ))\n",
        "\n",
        "    # ETH\n",
        "    eth_name = \"log_Close_ETH_USD\"\n",
        "    if eth_name in hht_results.results:\n",
        "        events = hht_results.results[eth_name][\"events\"]\n",
        "        dates = [d.date() for d in events.extreme_event_dates]\n",
        "        p_val = hht_robust_res.eth_robustness.p_value\n",
        "\n",
        "        # Check for date near Nov 7\n",
        "        exp_date = pd.Timestamp(\"2024-11-07\").date()\n",
        "        match = any(abs((d - exp_date).days) <= 3 for d in dates)\n",
        "\n",
        "        status = \"PASS\" if match and p_val < 0.001 else \"FAIL\"\n",
        "        items.append(ValidationItem(\n",
        "            metric_name=\"HHT Event: ETH\",\n",
        "            observed_value=str(dates),\n",
        "            expected_value=\"~2024-11-07\",\n",
        "            status=status,\n",
        "            comment=f\"p-value: {p_val}\"\n",
        "        ))\n",
        "\n",
        "    return items\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Step 3: Verify SVAR Regime Shift\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_svar(\n",
        "    wald_res: Any,\n",
        "    svar_id_res: Any\n",
        ") -> List[ValidationItem]:\n",
        "    \"\"\"\n",
        "    Validates SVAR regime shift findings.\n",
        "\n",
        "    Expected:\n",
        "    - Wald p-value < 0.0001.\n",
        "    - Impact matrix diagonal elements increase post-election.\n",
        "\n",
        "    Args:\n",
        "        wald_res (WaldTestResult): Wald test results.\n",
        "        svar_id_res (SVARIdentificationResults): Impact matrices.\n",
        "\n",
        "    Returns:\n",
        "        List[ValidationItem]: Validation checks.\n",
        "    \"\"\"\n",
        "    items = []\n",
        "\n",
        "    # 1. Wald Test\n",
        "    p_val = wald_res.p_value\n",
        "    status = \"PASS\" if p_val < 0.0001 else \"FAIL\"\n",
        "    items.append(ValidationItem(\n",
        "        metric_name=\"SVAR Wald Test\",\n",
        "        observed_value=f\"{p_val:.4e}\",\n",
        "        expected_value=\"< 0.0001\",\n",
        "        status=status,\n",
        "        comment=\"Tests equality of pre/post parameters.\"\n",
        "    ))\n",
        "\n",
        "    # 2. Impact Matrix Increase\n",
        "    pre_mats = [r for r in svar_id_res.results if r.regime_name == \"Pre-Election\"]\n",
        "    post_mats = [r for r in svar_id_res.results if r.regime_name == \"Post-Election\"]\n",
        "\n",
        "    if pre_mats and post_mats:\n",
        "        # Compare first ordering\n",
        "        pre_imp = pre_mats[0].impact_matrix\n",
        "        post_imp = post_mats[0].impact_matrix\n",
        "\n",
        "        # Calculate % change in diagonal (Own Shock)\n",
        "        pre_diag = np.diag(pre_imp.values)\n",
        "        post_diag = np.diag(post_imp.values)\n",
        "\n",
        "        # Avoid div by zero\n",
        "        pct_change = (post_diag - pre_diag) / pre_diag * 100\n",
        "\n",
        "        # Expect increase\n",
        "        avg_increase = np.mean(pct_change)\n",
        "        status = \"PASS\" if avg_increase > 0 else \"FAIL\"\n",
        "\n",
        "        items.append(ValidationItem(\n",
        "            metric_name=\"Volatility Spillover Increase\",\n",
        "            observed_value=f\"{avg_increase:.2f}%\",\n",
        "            expected_value=\"> 0% (approx 28-45%)\",\n",
        "            status=status,\n",
        "            comment=f\"Diagonal changes: {pct_change}\"\n",
        "        ))\n",
        "\n",
        "    return items\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 32, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def synthesize_study_results(\n",
        "    break_res: Any,\n",
        "    aaft_res: Any,\n",
        "    hht_res: Any,\n",
        "    hht_robust_res: Any,\n",
        "    wald_res: Any,\n",
        "    svar_id_res: Any\n",
        ") -> FinalStudyReport:\n",
        "    \"\"\"\n",
        "    Orchestrates the final validation and synthesis of the study.\n",
        "\n",
        "    Aggregates checks from all components to confirm replication success.\n",
        "\n",
        "    Args:\n",
        "        break_res: StructuralBreakOrchestratorResult\n",
        "        aaft_res: AAFTRobustnessResults\n",
        "        hht_res: HHTOrchestratorResult\n",
        "        hht_robust_res: HHTRobustnessResults\n",
        "        wald_res: WaldTestResult\n",
        "        svar_id_res: SVARIdentificationResults\n",
        "\n",
        "    Returns:\n",
        "        FinalStudyReport: The comprehensive report.\n",
        "    \"\"\"\n",
        "    print(\"Task 32: Synthesizing Final Results...\")\n",
        "\n",
        "    # 1. Breaks\n",
        "    break_items = verify_breaks(break_res, aaft_res)\n",
        "\n",
        "    # 2. HHT\n",
        "    hht_items = verify_hht(hht_res, hht_robust_res)\n",
        "\n",
        "    # 3. SVAR\n",
        "    svar_items = verify_svar(wald_res, svar_id_res)\n",
        "\n",
        "    # Conclusion\n",
        "    all_items = break_items + hht_items + svar_items\n",
        "    failures = [i for i in all_items if i.status == \"FAIL\"]\n",
        "\n",
        "    if not failures:\n",
        "        conclusion = \"SUCCESS: All empirical findings replicated within tolerance.\"\n",
        "    else:\n",
        "        conclusion = f\"PARTIAL SUCCESS: {len(failures)} checks failed. Review logs.\"\n",
        "\n",
        "    print(f\"  - {conclusion}\")\n",
        "    for item in all_items:\n",
        "        print(f\"    [{item.status}] {item.metric_name}: Obs={item.observed_value}, Exp={item.expected_value}\")\n",
        "\n",
        "    return FinalStudyReport(\n",
        "        structural_breaks=break_items,\n",
        "        extreme_events=hht_items,\n",
        "        svar_regime_shift=svar_items,\n",
        "        overall_conclusion=conclusion\n",
        "    )\n"
      ],
      "metadata": {
        "id": "dtSEmVJFh56P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Top-Level Orchestrator: On-Chain Behavioral Pre-Emption System (OBPS)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class OBPSPipelineResult:\n",
        "    \"\"\"\n",
        "    Master container for the entire OBPS research pipeline results.\n",
        "\n",
        "    Attributes:\n",
        "        config (ValidatedStudyConfig): Validated configuration.\n",
        "        chain_validation (ChainValidationResult): Chain data validation stats.\n",
        "        market_validation (MarketValidationResult): Market data validation stats.\n",
        "        final_panel (FinalizedSeriesData): The processed time series panel.\n",
        "        stationarity_tests (StationarityTestResults): ADF test results.\n",
        "        structural_breaks (StructuralBreakOrchestratorResult): Detected breaks.\n",
        "        break_robustness (AAFTRobustnessResults): AAFT robustness for breaks.\n",
        "        hht_analysis (HHTOrchestratorResult): Hilbert-Huang Transform results.\n",
        "        hht_robustness (HHTRobustnessResults): AAFT robustness for HHT.\n",
        "        svar_model (VARModelResults): Full-sample VAR model.\n",
        "        svar_regimes (RegimeComparisonData): Pre/Post election VAR models.\n",
        "        svar_identification (SVARIdentificationResults): Structural impact matrices.\n",
        "        wald_test (WaldTestResult): Regime shift significance test.\n",
        "        final_report (FinalStudyReport): Synthesis and validation against expected findings.\n",
        "    \"\"\"\n",
        "    config: Any # ValidatedStudyConfig\n",
        "    chain_validation: Any # ChainValidationResult\n",
        "    market_validation: Any # MarketValidationResult\n",
        "    final_panel: Any # FinalizedSeriesData\n",
        "    stationarity_tests: Any # StationarityTestResults\n",
        "    structural_breaks: Any # StructuralBreakOrchestratorResult\n",
        "    break_robustness: Any # AAFTRobustnessResults\n",
        "    hht_analysis: Any # HHTOrchestratorResult\n",
        "    hht_robustness: Any # HHTRobustnessResults\n",
        "    svar_model: Any # VARModelResults\n",
        "    svar_regimes: Any # RegimeComparisonData\n",
        "    svar_identification: Any # SVARIdentificationResults\n",
        "    wald_test: Any # WaldTestResult\n",
        "    final_report: Any # FinalStudyReport\n",
        "\n",
        "def run_obps_pipeline(\n",
        "    df_chain_raw: pd.DataFrame,\n",
        "    df_market_raw: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> OBPSPipelineResult:\n",
        "    \"\"\"\n",
        "    Executes the end-to-end On-Chain Behavioral Pre-Emption System (OBPS) research pipeline.\n",
        "\n",
        "    This orchestrator sequentially executes Tasks 1 through 32, transforming raw blockchain\n",
        "    and market data into rigorous econometric insights regarding political risk transmission.\n",
        "\n",
        "    Pipeline Stages:\n",
        "    1. Configuration Validation\n",
        "    2. On-Chain Data Processing (Validation, Filtering, Normalization, Classification, Aggregation)\n",
        "    3. Market Data Processing (Validation, Cleansing, Extraction)\n",
        "    4. Panel Construction (Merging, Transformation, Stationarity Testing)\n",
        "    5. Structural Break Analysis (Bai-Perron, AAFT Robustness)\n",
        "    6. Non-Linear Signal Processing (HHT, Extreme Event Detection, Robustness)\n",
        "    7. Structural VAR Analysis (Estimation, Regime Comparison, Identification, Wald Test)\n",
        "    8. Synthesis and Reporting\n",
        "\n",
        "    Args:\n",
        "        df_chain_raw (pd.DataFrame): Raw ERC-20 transfer logs.\n",
        "        df_market_raw (pd.DataFrame): Raw market OHLCV data.\n",
        "        study_config (Dict[str, Any]): Configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        OBPSPipelineResult: A container holding all intermediate and final artifacts.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"STARTING OBPS PIPELINE EXECUTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 1: Configuration & Validation (Task 1)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[Phase 1] Configuration Validation\")\n",
        "    validated_config = validate_study_config(study_config)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 2: Chain Data Processing (Tasks 2-9)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[Phase 2] On-Chain Data Processing\")\n",
        "\n",
        "    # Task 2: Validate Structure\n",
        "    chain_val_res = validate_df_chain_raw(df_chain_raw, validated_config.schemas)\n",
        "\n",
        "    # Task 4: Filter Time/Status\n",
        "    chain_filtered = filter_chain_data(\n",
        "        chain_val_res.validated_df,\n",
        "        validated_config.meta,\n",
        "        study_config[\"preprocessing\"]\n",
        "    )\n",
        "\n",
        "    # Task 5: Normalize Values\n",
        "    chain_norm = normalize_chain_data(\n",
        "        chain_filtered,\n",
        "        validated_config.schemas,\n",
        "        study_config[\"preprocessing\"]\n",
        "    )\n",
        "\n",
        "    # Task 6: Deduplicate\n",
        "    chain_dedup = deduplicate_chain_data(chain_norm)\n",
        "\n",
        "    # Task 7: Classify Topology\n",
        "    chain_class = classify_chain_topology(chain_dedup)\n",
        "\n",
        "    # Task 8: Aggregate Daily\n",
        "    chain_agg = aggregate_chain_volumes(chain_class, validated_config.meta)\n",
        "\n",
        "    # Task 9: Validate Series\n",
        "    validate_daily_chain_series(chain_agg, validated_config.meta)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 3: Market Data Processing (Tasks 3, 10-11)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[Phase 3] Market Data Processing\")\n",
        "\n",
        "    # Task 3: Validate Structure\n",
        "    market_val_res = validate_df_market_raw(\n",
        "        df_market_raw,\n",
        "        validated_config.schemas,\n",
        "        validated_config.meta\n",
        "    )\n",
        "\n",
        "    # Task 10: Cleanse\n",
        "    market_clean = cleanse_market_data(market_val_res, validated_config.meta)\n",
        "\n",
        "    # Task 11: Extract Series\n",
        "    market_series = extract_market_series(market_clean)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 4: Panel Construction (Tasks 12-15)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[Phase 4] Panel Construction\")\n",
        "\n",
        "    # Task 12: Merge\n",
        "    merged_panel = merge_data_sources(chain_agg, market_series)\n",
        "\n",
        "    # Task 13: Log Transform\n",
        "    log_data = construct_log_series(merged_panel)\n",
        "\n",
        "    # Task 14: Stationarity Tests\n",
        "    adf_results = perform_stationarity_tests(\n",
        "        log_data,\n",
        "        study_config[\"preprocessing\"]\n",
        "    )\n",
        "\n",
        "    # Task 15: Finalize Integration Order\n",
        "    finalized_data = finalize_integration_order(\n",
        "        log_data,\n",
        "        adf_results,\n",
        "        study_config[\"preprocessing\"]\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 5: Structural Break Analysis (Tasks 16-21)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[Phase 5] Structural Break Analysis\")\n",
        "\n",
        "    # Task 20: Orchestrate Detection (covers Tasks 16-19 logic)\n",
        "    break_results = orchestrate_structural_breaks(\n",
        "        finalized_data,\n",
        "        validated_config.analysis\n",
        "    )\n",
        "\n",
        "    # Task 21: Robustness\n",
        "    break_robustness = execute_aaft_robustness(\n",
        "        finalized_data,\n",
        "        break_results,\n",
        "        validated_config.analysis\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 6: HHT Analysis (Tasks 22-26)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[Phase 6] Non-Linear Signal Processing (HHT)\")\n",
        "\n",
        "    # Task 25: Orchestrate HHT (covers Tasks 22-24 logic)\n",
        "    hht_results = orchestrate_hht(\n",
        "        finalized_data,\n",
        "        validated_config.analysis\n",
        "    )\n",
        "\n",
        "    # Task 26: Robustness\n",
        "    hht_robustness = execute_hht_robustness(\n",
        "        finalized_data,\n",
        "        hht_results,\n",
        "        validated_config.analysis\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 7: SVAR Analysis (Tasks 27-31)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[Phase 7] Structural VAR Analysis\")\n",
        "\n",
        "    # Task 27: Prepare Inputs\n",
        "    svar_input = prepare_svar_inputs(\n",
        "        finalized_data,\n",
        "        validated_config.analysis\n",
        "    )\n",
        "\n",
        "    # Task 28: Estimate Full VAR\n",
        "    var_model = estimate_reduced_var(\n",
        "        svar_input,\n",
        "        validated_config.analysis\n",
        "    )\n",
        "\n",
        "    # Task 29: Estimate Regimes\n",
        "    regime_vars = estimate_regime_vars(\n",
        "        svar_input,\n",
        "        var_model\n",
        "    )\n",
        "\n",
        "    # Task 30: Identify Shocks\n",
        "    svar_id = identify_structural_shocks(\n",
        "        regime_vars,\n",
        "        svar_input,\n",
        "        validated_config.analysis\n",
        "    )\n",
        "\n",
        "    # Task 31: Wald Test\n",
        "    wald_res = perform_wald_test(regime_vars)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 8: Synthesis (Task 32)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n[Phase 8] Synthesis and Reporting\")\n",
        "\n",
        "    final_report = synthesize_study_results(\n",
        "        break_results,\n",
        "        break_robustness,\n",
        "        hht_results,\n",
        "        hht_robustness,\n",
        "        wald_res,\n",
        "        svar_id\n",
        "    )\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"OBPS PIPELINE EXECUTION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return OBPSPipelineResult(\n",
        "        config=validated_config,\n",
        "        chain_validation=chain_val_res,\n",
        "        market_validation=market_val_res,\n",
        "        final_panel=finalized_data,\n",
        "        stationarity_tests=adf_results,\n",
        "        structural_breaks=break_results,\n",
        "        break_robustness=break_robustness,\n",
        "        hht_analysis=hht_results,\n",
        "        hht_robustness=hht_robustness,\n",
        "        svar_model=var_model,\n",
        "        svar_regimes=regime_vars,\n",
        "        svar_identification=svar_id,\n",
        "        wald_test=wald_res,\n",
        "        final_report=final_report\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "HdCpE9BvtHTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}